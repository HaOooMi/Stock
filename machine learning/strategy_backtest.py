#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é˜¶æ®µ11ï¼šç­–ç•¥ä¿¡å·ä¸å›æµ‹é›å½¢

åŠ¨ä½œï¼š
1. é€‰è®­ç»ƒé›†æ”¶ç›Šæœ€é«˜çš„ cluster åˆ—è¡¨ï¼ˆtop1 æˆ– top2ï¼‰
2. ç”Ÿæˆæµ‹è¯•é›† signal=1/0
3. è®¡ç®—ç­–ç•¥æ”¶ç›Š vs åŸºå‡†ï¼ˆæŒæœ‰ï¼‰ â†’ ä¿å­˜æƒç›Šæ›²çº¿
4. è¾“å‡º reports/strategy_equity.csv
5. æœ‰éšæœºåŸºå‡†ï¼ˆ100 æ¬¡éšæœºä¿¡å·ï¼‰å¯¹æ¯”

éªŒæ”¶ï¼š
- ç­–ç•¥æ”¶ç›Šä¸å°äºåŸºå‡†ä¸”å›æ’¤ä¸è¿‡åˆ†æ”¾å¤§
- æœ‰éšæœºåŸºå‡†å¯¹æ¯”

æ•°æ®è¯´æ˜ï¼š
- ä½¿ç”¨ InfluxDB å¯¼å…¥æ–°æ•°æ®ä½œä¸ºæµ‹è¯•é›†ï¼ˆ2025å¹´1æœˆ1æ—¥è‡³2025å¹´8æœˆ1æ—¥ï¼‰
- ä½¿ç”¨ pca_state çš„æ•°æ®é¢„å¤„ç†
- ä¸¥æ ¼æŒ‰ç…§åŸæœ‰ä»£ç çš„å‘½åæ ¼å¼å’Œå‚æ•°è®¾å®š

ä½œè€…: Assistant
æ—¥æœŸ: 2025-09-29
"""

import os
import sys
import glob
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import pickle
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# å¯¼å…¥å¿…è¦æ¨¡å—
from sklearn.cluster import KMeans
from sklearn.preprocessing import RobustScaler
from pca_state import run_complete_feature_pipeline, run_complete_target_pipeline, PCAStateGenerator

class StrategyBacktest:
    """
    ç­–ç•¥ä¿¡å·ä¸å›æµ‹é›å½¢
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. ä» cluster_evaluate çš„èšç±»ç»“æœä¸­é€‰æ‹©æ”¶ç›Šæœ€é«˜çš„èšç±»
    2. å¯¹æ–°æµ‹è¯•æ•°æ®ï¼ˆ2025å¹´1æœˆ1æ—¥è‡³2025å¹´8æœˆ1æ—¥ï¼‰ç”Ÿæˆäº¤æ˜“ä¿¡å·
    3. è®¡ç®—ç­–ç•¥æ”¶ç›Šå¹¶ä¸åŸºå‡†å¯¹æ¯”
    4. éšæœºåŸºå‡†å¯¹æ¯”éªŒè¯
    5. ä¿å­˜æƒç›Šæ›²çº¿å’Œåˆ†ææŠ¥å‘Š
    """
    
    def __init__(self, reports_dir: str = "machine learning/ML output/reports"):
        """
        åˆå§‹åŒ–ç­–ç•¥å›æµ‹å™¨
        
        Parameters:
        -----------
        reports_dir : str
            æŠ¥å‘Šä¿å­˜ç›®å½•
        """
        # è®¾ç½®ç›®å½•è·¯å¾„
        self.project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        
        if os.path.isabs(reports_dir):
            self.reports_dir = reports_dir
        else:
            self.reports_dir = os.path.join(self.project_root, reports_dir)
        
        self.ml_output_dir = os.path.join(self.project_root, "machine learning/ML output")
        self.models_dir = os.path.join(self.ml_output_dir, "models")
        self.states_dir = os.path.join(self.ml_output_dir, "states")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.reports_dir, exist_ok=True)
        
        # ç­–ç•¥å‚æ•°
        self.test_start_date = "2025-01-01"
        self.test_end_date = "2025-08-01"
        self.random_simulations = 100
        
        # å­˜å‚¨æ¨¡å‹å’Œæ•°æ®
        self.cluster_models = {}
        
        print(f"ğŸ¯ ç­–ç•¥å›æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ æŠ¥å‘Šç›®å½•: {self.reports_dir}")
        print(f"ğŸ“… æµ‹è¯•æœŸé—´: {self.test_start_date} ~ {self.test_end_date}")

    def load_cluster_evaluation_results(self) -> Dict:
        """
        åŠ è½½ cluster_evaluate çš„èšç±»ç»“æœ
        
        Returns:
        --------
        dict
            èšç±»è¯„ä¼°ç»“æœ
        """
        print("ğŸ“Š åŠ è½½èšç±»è¯„ä¼°ç»“æœ...")
        
        # åŠ è½½èšç±»æ¨¡å‹
        cluster_models_file = os.path.join(self.reports_dir, "cluster_models.pkl")
        if not os.path.exists(cluster_models_file):
            raise FileNotFoundError(f"èšç±»æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {cluster_models_file}")
        
        with open(cluster_models_file, 'rb') as f:
            self.cluster_models = pickle.load(f)
        
        print(f"   âœ… åŠ è½½äº† {len(self.cluster_models)} ä¸ªèšç±»æ¨¡å‹")
        
        # åŠ è½½èšç±»æ¯”è¾ƒç»“æœ
        comparison_file = os.path.join(self.reports_dir, "cluster_comparison.csv")
        if not os.path.exists(comparison_file):
            raise FileNotFoundError(f"èšç±»æ¯”è¾ƒæ–‡ä»¶ä¸å­˜åœ¨: {comparison_file}")
        
        comparison_df = pd.read_csv(comparison_file)
        
        print(f"   ğŸ“‹ èšç±»æ¯”è¾ƒæ•°æ®: {len(comparison_df)} æ¡è®°å½•")
        
        return {
            'cluster_models': self.cluster_models,
            'comparison_df': comparison_df
        }

    def select_best_clusters(self, comparison_df: pd.DataFrame, top_n: int =3) -> Dict:
        """
        é€‰æ‹©å…¨å±€æ’åæœ€é«˜çš„èšç±»ï¼ˆæŒ‰global_rankæ’åºï¼‰
        
        Parameters:
        -----------
        comparison_df : pd.DataFrame
            èšç±»æ¯”è¾ƒæ•°æ®
        top_n : int, default=3
            é€‰æ‹© top N ä¸ªèšç±»
            
        Returns:
        --------
        dict
            æœ€ä½³èšç±»ä¿¡æ¯
        """
        print(f"ğŸ¯ é€‰æ‹©å…¨å±€æ’åæœ€é«˜çš„ top{top_n} èšç±»...")
        
        # åªé€‰æ‹©éªŒè¯é€šè¿‡çš„èšç±»
        valid_clusters = comparison_df[comparison_df['validation_passed'] == True].copy()
        
        if len(valid_clusters) == 0:
            print("   âš ï¸ è­¦å‘Šï¼šæ²¡æœ‰éªŒè¯é€šè¿‡çš„èšç±»ï¼Œä½¿ç”¨æ‰€æœ‰èšç±»")
            valid_clusters = comparison_df.copy()
        
        # æŒ‰global_rankæ’åºï¼ˆä»å°åˆ°å¤§ï¼Œrankè¶Šå°è¶Šå¥½ï¼‰ï¼Œé€‰æ‹© top N
        top_clusters = valid_clusters.nsmallest(top_n, 'global_rank')
        
        selected_clusters = []
        for _, row in top_clusters.iterrows():
            cluster_info = {
                'k_value': int(row['k_value']),
                'cluster_id': int(row['cluster_id']),
                'train_mean_return': row['train_mean_return'],
                'test_mean_return': row['test_mean_return'],
                'train_rank': int(row['train_rank']),
                'test_rank': int(row['test_rank']),
                'global_rank': int(row['global_rank'])
            }
            selected_clusters.append(cluster_info)
            
            print(f"   âœ… é€‰ä¸­: k={cluster_info['k_value']}, cluster_id={cluster_info['cluster_id']} (å…¨å±€æ’å: {cluster_info['global_rank']})")
            print(f"      è®­ç»ƒæ”¶ç›Š: {cluster_info['train_mean_return']:+.6f} (è®­ç»ƒæ’å: {cluster_info['train_rank']})")
            print(f"      æµ‹è¯•æ”¶ç›Š: {cluster_info['test_mean_return']:+.6f} (æµ‹è¯•æ’å: {cluster_info['test_rank']})")
            print(f"      ç»¼åˆæ”¶ç›Š: {cluster_info['train_mean_return'] + cluster_info['test_mean_return']:+.6f}")
        
        return {
            'selected_clusters': selected_clusters,
            'selection_method': f'top_{top_n}_global_rank'
        }

    def prepare_test_data(self, symbol: str = "000001") -> pd.DataFrame:
        """
        å‡†å¤‡æµ‹è¯•æ•°æ®ï¼ˆ2025å¹´1æœˆ1æ—¥è‡³2025å¹´8æœˆ1æ—¥ï¼‰
        ä½¿ç”¨ pca_state çš„å®Œæ•´æ•°æ®é¢„å¤„ç†æµç¨‹
        
        Parameters:
        -----------
        symbol : str
            è‚¡ç¥¨ä»£ç 
            
        Returns:
        --------
        pd.DataFrame
            é¢„å¤„ç†åçš„æµ‹è¯•æ•°æ®ï¼ŒåŒ…å«PCAç‰¹å¾å’Œç›®æ ‡å˜é‡
        """
        print(f"ğŸ”§ å‡†å¤‡æµ‹è¯•æ•°æ®: {symbol} ({self.test_start_date} ~ {self.test_end_date})")
        print("   ä½¿ç”¨ pca_state çš„å®Œæ•´æ•°æ®é¢„å¤„ç†æµç¨‹")
        try:
            # é…ç½®å‚æ•°ï¼ˆä¸ pca_state.main() ç›¸åŒï¼‰
            config = {
                'symbol': symbol,
                'start_date': self.test_start_date,  # 2025-01-01
                'end_date': self.test_end_date,      # 2025-08-01
                'use_auto_features': True,           # ä½¿ç”¨è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆ
                'final_k_features': 15,              # æœ€ç»ˆç‰¹å¾æ•°é‡
                'target_periods': [1, 5, 10],        # ç›®æ ‡æ—¶é—´çª—å£
                'pca_components': 0.9,               # PCAè§£é‡Šæ–¹å·®æ¯”ä¾‹
                'train_ratio': 0.8                   # è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆç”¨äºå†…éƒ¨åˆ‡åˆ†ï¼‰
            }
            
            print("   ğŸ“‹ æ‰§è¡Œé…ç½®:")
            for key, value in config.items():
                print(f"      {key}: {value}")
            print()
            
            # === æ­¥éª¤1: å®Œæ•´ç‰¹å¾å·¥ç¨‹æµç¨‹ ===
            print("   ğŸ”§ æ­¥éª¤1: å®Œæ•´ç‰¹å¾å·¥ç¨‹æµç¨‹")
            feature_results = run_complete_feature_pipeline(
                symbol=config['symbol'],
                start_date=config['start_date'],
                end_date=config['end_date'],
                use_auto_features=config['use_auto_features'],
                final_k_features=config['final_k_features']
            )
            
            if not feature_results.get('success'):
                raise ValueError(f"ç‰¹å¾å·¥ç¨‹å¤±è´¥: {feature_results.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
            print("      âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ")
            
            # === æ­¥éª¤2: å®Œæ•´ç›®æ ‡å˜é‡å·¥ç¨‹æµç¨‹ ===
            print("   ğŸ¯ æ­¥éª¤2: å®Œæ•´ç›®æ ‡å˜é‡å·¥ç¨‹æµç¨‹")
            target_results = run_complete_target_pipeline(
                scaled_features_df=feature_results['scaled_features_df'],
                symbol=config['symbol'],
                target_periods=config['target_periods']
            )
            
            if not target_results.get('success'):
                raise ValueError(f"ç›®æ ‡å˜é‡å·¥ç¨‹å¤±è´¥: {target_results.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
            print("      âœ… ç›®æ ‡å˜é‡å·¥ç¨‹å®Œæˆ")
            
            # === æ­¥éª¤3: PCAçŠ¶æ€ç”Ÿæˆ ===
            print("   ï¿½ æ­¥éª¤3: PCAçŠ¶æ€ç”Ÿæˆ")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡å‡†åŒ–ç‰¹å¾CSVæ–‡ä»¶
            csv_path = feature_results.get('csv_path')
            if not csv_path or not os.path.exists(csv_path):
                raise ValueError("æ ‡å‡†åŒ–ç‰¹å¾CSVæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡ŒPCAé™ç»´")
            
            # åˆå§‹åŒ–PCAçŠ¶æ€ç”Ÿæˆå™¨
            pca_generator = PCAStateGenerator()
            
            # ç”ŸæˆPCAçŠ¶æ€ï¼ˆä½¿ç”¨å®Œæ•´æµç¨‹ï¼‰
            pca_results = pca_generator.generate_pca_states(
                csv_path=csv_path,
                symbol=config['symbol'],
                n_components=config['pca_components'],
                train_ratio=config['train_ratio']
            )
            
            if not pca_results or 'n_components' not in pca_results:
                raise ValueError("PCAçŠ¶æ€ç”Ÿæˆå¤±è´¥")
            
            print("      âœ… PCAçŠ¶æ€ç”Ÿæˆå®Œæˆ")
            
            # === æ­¥éª¤4: æ„å»ºæœ€ç»ˆæµ‹è¯•æ•°æ® ===
            print("   ğŸ”¨ æ­¥éª¤4: æ„å»ºæœ€ç»ˆæµ‹è¯•æ•°æ®")
            
            # è·å–å®Œæ•´æ•°æ®é›†ï¼ˆåŒ…å«ç›®æ ‡å˜é‡ï¼‰
            complete_dataset = target_results['complete_dataset']
            
            # è·å–PCAé™ç»´åçš„ç‰¹å¾æ•°æ®
            # ä½¿ç”¨æ‰€æœ‰æ•°æ®ï¼ˆè®­ç»ƒ+æµ‹è¯•ï¼‰ä½œä¸ºæ–°çš„æµ‹è¯•é›†
            states_all = np.vstack([pca_results['states_train'], pca_results['states_test']])
            
            # åˆ›å»ºPCAç‰¹å¾DataFrame
            pca_columns = [f'PC{i+1}' for i in range(states_all.shape[1])]
            pca_df = pd.DataFrame(states_all, index=complete_dataset.index, columns=pca_columns)
            
            # åˆå¹¶PCAç‰¹å¾å’Œç›®æ ‡å˜é‡
            target_cols = [col for col in complete_dataset.columns 
                          if col.startswith('future_return_') or col.startswith('label_')]
            
            test_data = pd.concat([
                pca_df,
                complete_dataset[target_cols + ['close']]
            ], axis=1)
            
            # ç§»é™¤åŒ…å«NaNçš„è¡Œï¼ˆä¸»è¦æ˜¯æœ«å°¾çš„ç›®æ ‡å˜é‡NaNï¼‰
            test_data = test_data.dropna()
            
            print(f"      âœ… æµ‹è¯•æ•°æ®æ„å»ºå®Œæˆ: {test_data.shape}")
            print(f"      ğŸ“Š PCAç‰¹å¾: {len(pca_columns)} ç»´")
            print(f"      ğŸ¯ ç›®æ ‡å˜é‡: {len([col for col in target_cols if col.startswith('future_return_')])} ä¸ª")
            print(f"      ğŸ“… æ•°æ®æ—¶é—´èŒƒå›´: {test_data.index.min().date()} ~ {test_data.index.max().date()}")
            
            return test_data
            
        except Exception as e:
            print(f"   âŒ æµ‹è¯•æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise

    def generate_trading_signals(self, test_data: pd.DataFrame, selected_clusters: List[Dict]) -> pd.DataFrame:
        """
        ç”Ÿæˆäº¤æ˜“ä¿¡å·
        
        Parameters:
        -----------
        test_data : pd.DataFrame
            é¢„å¤„ç†åçš„æµ‹è¯•æ•°æ®
        selected_clusters : List[Dict]
            é€‰ä¸­çš„æœ€ä½³èšç±»
            
        Returns:
        --------
        pd.DataFrame
            åŒ…å«äº¤æ˜“ä¿¡å·çš„æ•°æ®
        """
        print(f"ğŸ“¡ ç”Ÿæˆäº¤æ˜“ä¿¡å·...")
        
        # è·å–PCAç‰¹å¾
        pca_columns = [col for col in test_data.columns if col.startswith('PC')]
        X_pca = test_data[pca_columns].fillna(0).values
        
        # ä¸ºæ¯ä¸ªé€‰ä¸­çš„èšç±»ç”Ÿæˆä¿¡å·
        signals = {}
        
        for i, cluster_info in enumerate(selected_clusters):
            k_value = cluster_info['k_value']
            cluster_id = cluster_info['cluster_id']
            
            print(f"   ğŸ“Š èšç±» {i+1}: k={k_value}, cluster_id={cluster_id}")
            
            # ä½¿ç”¨å¯¹åº”çš„èšç±»æ¨¡å‹
            cluster_model = self.cluster_models[k_value]
            cluster_labels = cluster_model.predict(X_pca)
            
            # ç”Ÿæˆä¿¡å·ï¼šå±äºç›®æ ‡èšç±»æ—¶ä¸º1ï¼Œå¦åˆ™ä¸º0
            signal = (cluster_labels == cluster_id).astype(int)
            signals[f'signal_k{k_value}_c{cluster_id}'] = signal
            
            signal_count = signal.sum()
            signal_ratio = signal_count / len(signal)
            print(f"      ä¿¡å·æ•°é‡: {signal_count}/{len(signal)} ({signal_ratio:.2%})")
        
        # ç»¼åˆä¿¡å·ï¼šä»»ä¸€èšç±»å‘å‡ºä¿¡å·åˆ™ä¸º1
        combined_signal = np.zeros(len(test_data))
        for signal_col in signals.keys():
            combined_signal = np.maximum(combined_signal, signals[signal_col])
        
        # ã€ä¼˜åŒ–ã€‘åŸºäºå†å²åŠ¨é‡è¿‡æ»¤ä¿¡å·ï¼ˆä¸ä½¿ç”¨æœªæ¥æ•°æ®ï¼‰
        # è®¡ç®—è¿‡å»5å¤©çš„åŠ¨é‡ï¼ˆæ”¶ç›Šç‡ï¼‰
        use_momentum_filter = True  # æ˜¯å¦ä½¿ç”¨åŠ¨é‡è¿‡æ»¤
        if use_momentum_filter and 'close' in test_data.columns:
            momentum_5d = test_data['close'].pct_change(periods=5).fillna(0).values
            # æ”¾å®½æ¡ä»¶ï¼šå…è®¸è½»å¾®ä¸‹è·Œè¶‹åŠ¿ä¸­çš„ä¿¡å·
            momentum_threshold = -0.02  # å…è®¸-2%ä»¥å†…çš„ä¸‹è·Œ
            combined_signal[(momentum_5d < momentum_threshold)] = 0
            print(f"   ğŸ” åŠ¨é‡è¿‡æ»¤ (é˜ˆå€¼={momentum_threshold}): ä¿ç•™ä¿¡å· {combined_signal.sum()}/{len(combined_signal)} ({combined_signal.mean():.2%})")
        else:
            print(f"   âš ï¸ æœªä½¿ç”¨åŠ¨é‡è¿‡æ»¤")
        
        signals['signal_combined'] = combined_signal
        
        # æ·»åŠ ä¿¡å·åˆ°æµ‹è¯•æ•°æ®
        result_data = test_data.copy()
        for signal_name, signal_values in signals.items():
            result_data[signal_name] = signal_values
        
        combined_count = combined_signal.sum()
        combined_ratio = combined_count / len(combined_signal)
        print(f"   âœ… ç»¼åˆä¿¡å·: {combined_count}/{len(combined_signal)} ({combined_ratio:.2%})")
        
        return result_data

    def calculate_strategy_performance(self, signal_data: pd.DataFrame) -> Dict:
        """
        è®¡ç®—ç­–ç•¥æ”¶ç›Š vs åŸºå‡†ï¼ˆæŒæœ‰ï¼‰
        
        Parameters:
        -----------
        signal_data : pd.DataFrame
            åŒ…å«ä¿¡å·çš„æ•°æ®
            
        Returns:
        --------
        dict
            ç­–ç•¥æ€§èƒ½æŒ‡æ ‡
        """
        print(f"ğŸ’° è®¡ç®—ç­–ç•¥æ€§èƒ½...")
        
        # ä½¿ç”¨future_return_5dä½œä¸ºé¢„æµ‹ç›®æ ‡æ”¶ç›Š
        returns = signal_data['future_return_5d'].fillna(0).values
        signal = signal_data['signal_combined'].values
        
        # åŸºå‡†ç­–ç•¥ï¼šå§‹ç»ˆæŒæœ‰
        benchmark_returns = returns
        benchmark_cumulative = np.cumprod(1 + benchmark_returns)
        
        # ç­–ç•¥æ”¶ç›Šï¼šä¿¡å·ä¸º1æ—¶ä¹°å…¥æŒæœ‰ï¼Œä¿¡å·ä¸º0æ—¶ç©ºä»“ï¼ˆæŒæœ‰ç°é‡‘ï¼‰
        # è¿™æ‰æ˜¯çœŸæ­£çš„æ‹©æ—¶ç­–ç•¥ï¼Œå¯ä»¥è§„é¿ä¸‹è·Œé£é™©
        strategy_returns = signal * returns
        strategy_cumulative = np.ones(len(strategy_returns))
        
        use_stop_loss = False  # æ˜¯å¦ä½¿ç”¨æ­¢æŸæœºåˆ¶
        stop_loss_threshold = -0.05  # æ­¢æŸé˜ˆå€¼ï¼ˆ-5%ï¼‰
        
        for i in range(1, len(strategy_returns)):
            if signal[i] == 1:
                # æœ‰ä¿¡å·æ—¶ï¼Œä¹°å…¥æŒæœ‰
                new_return = returns[i]
                
                # å¦‚æœå¯ç”¨æ­¢æŸï¼Œæ£€æŸ¥æ˜¯å¦è§¦å‘æ­¢æŸ
                if use_stop_loss and new_return < stop_loss_threshold:
                    # è§¦å‘æ­¢æŸï¼Œä¸å‚ä¸æœ¬æ¬¡äº¤æ˜“
                    strategy_cumulative[i] = strategy_cumulative[i-1]
                else:
                    # æ­£å¸¸å‚ä¸å¸‚åœº
                    strategy_cumulative[i] = strategy_cumulative[i-1] * (1 + new_return)
            else:
                # æ— ä¿¡å·æ—¶ï¼Œç©ºä»“ï¼Œç´¯è®¡æ”¶ç›Šä¿æŒä¸å˜ï¼ˆè§„é¿é£é™©ï¼‰
                strategy_cumulative[i] = strategy_cumulative[i-1]
        strategy_cumulative[0] = 1 + strategy_returns[0]
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        total_return_benchmark = benchmark_cumulative[-1] - 1
        total_return_strategy = strategy_cumulative[-1] - 1
        excess_return = total_return_strategy - total_return_benchmark
        
        # å¹´åŒ–æ”¶ç›Šç‡ï¼ˆå‡è®¾250ä¸ªäº¤æ˜“æ—¥ï¼‰
        n_days = len(returns)
        years = n_days / 250
        annual_return_benchmark = (1 + total_return_benchmark) ** (1/years) - 1 if years > 0 else 0
        annual_return_strategy = (1 + total_return_strategy) ** (1/years) - 1 if years > 0 else 0
        
        # æ³¢åŠ¨ç‡
        benchmark_volatility = np.std(benchmark_returns) * np.sqrt(250)
        strategy_volatility = np.std(strategy_returns) * np.sqrt(250)
        
        # å¤æ™®æ¯”ç‡ï¼ˆå‡è®¾æ— é£é™©åˆ©ç‡ä¸º3%ï¼‰
        risk_free_rate = 0.03
        sharpe_benchmark = (annual_return_benchmark - risk_free_rate) / benchmark_volatility if benchmark_volatility > 0 else 0
        sharpe_strategy = (annual_return_strategy - risk_free_rate) / strategy_volatility if strategy_volatility > 0 else 0
        
        # æœ€å¤§å›æ’¤
        benchmark_running_max = np.maximum.accumulate(benchmark_cumulative)
        benchmark_drawdown = (benchmark_cumulative - benchmark_running_max) / benchmark_running_max
        max_drawdown_benchmark = np.min(benchmark_drawdown)
        
        strategy_running_max = np.maximum.accumulate(strategy_cumulative)
        strategy_drawdown = (strategy_cumulative - strategy_running_max) / strategy_running_max
        max_drawdown_strategy = np.min(strategy_drawdown)
        
        # èƒœç‡ï¼ˆä»…è€ƒè™‘æœ‰ä¿¡å·çš„æ—¶é—´ç‚¹ï¼‰
        signal_mask = signal == 1
        win_rate = (returns[signal_mask] > 0).mean() if signal_mask.sum() > 0 else 0
        
        performance = {
            # æ€»æ”¶ç›Š
            'total_return_benchmark': total_return_benchmark,
            'total_return_strategy': total_return_strategy,
            'excess_return': excess_return,
            
            # å¹´åŒ–æ”¶ç›Š
            'annual_return_benchmark': annual_return_benchmark,
            'annual_return_strategy': annual_return_strategy,
            
            # é£é™©æŒ‡æ ‡
            'volatility_benchmark': benchmark_volatility,
            'volatility_strategy': strategy_volatility,
            'max_drawdown_benchmark': max_drawdown_benchmark,
            'max_drawdown_strategy': max_drawdown_strategy,
            
            # é£é™©è°ƒæ•´æ”¶ç›Š
            'sharpe_benchmark': sharpe_benchmark,
            'sharpe_strategy': sharpe_strategy,
            
            # äº¤æ˜“ç»Ÿè®¡
            'signal_count': signal_mask.sum(),
            'signal_ratio': signal_mask.mean(),
            'win_rate': win_rate,
            
            # æ—¶é—´åºåˆ—
            'benchmark_cumulative': benchmark_cumulative,
            'strategy_cumulative': strategy_cumulative,
            'benchmark_drawdown': benchmark_drawdown,
            'strategy_drawdown': strategy_drawdown,
            'dates': signal_data.index
        }
        
        print(f"   âœ… ç­–ç•¥æ€§èƒ½:")
        print(f"      åŸºå‡†æ€»æ”¶ç›Š: {total_return_benchmark:.2%}")
        print(f"      ç­–ç•¥æ€»æ”¶ç›Š: {total_return_strategy:.2%}")
        print(f"      è¶…é¢æ”¶ç›Š: {excess_return:.2%}")
        print(f"      åŸºå‡†å¤æ™®: {sharpe_benchmark:.3f}")
        print(f"      ç­–ç•¥å¤æ™®: {sharpe_strategy:.3f}")
        print(f"      åŸºå‡†å›æ’¤: {max_drawdown_benchmark:.2%}")
        print(f"      ç­–ç•¥å›æ’¤: {max_drawdown_strategy:.2%}")
        print(f"      ä¿¡å·èƒœç‡: {win_rate:.2%}")
        
        return performance

    def run_random_baseline(self, signal_data: pd.DataFrame, performance: Dict) -> Dict:
        """
        éšæœºåŸºå‡†å¯¹æ¯”ï¼ˆ100æ¬¡éšæœºä¿¡å·ï¼‰
        
        Parameters:
        -----------
        signal_data : pd.DataFrame
            åŒ…å«ä¿¡å·çš„æ•°æ®
        performance : Dict
            ç­–ç•¥æ€§èƒ½æŒ‡æ ‡
            
        Returns:
        --------
        dict
            éšæœºåŸºå‡†å¯¹æ¯”ç»“æœ
        """
        print(f"ğŸ² è¿è¡ŒéšæœºåŸºå‡†å¯¹æ¯” ({self.random_simulations}æ¬¡)...")
        
        returns = signal_data['future_return_5d'].fillna(0).values
        original_signal_ratio = performance['signal_ratio']
        
        # è¿è¡Œéšæœºæ¨¡æ‹Ÿ
        random_results = []
        
        for i in range(self.random_simulations):
            # ç”Ÿæˆéšæœºä¿¡å·ï¼ˆä¿æŒç›¸åŒçš„ä¿¡å·æ¯”ä¾‹ï¼‰
            n_samples = len(returns)
            n_signals = int(n_samples * original_signal_ratio)
            
            random_signal = np.zeros(n_samples)
            if n_signals > 0:
                random_indices = np.random.choice(n_samples, n_signals, replace=False)
                random_signal[random_indices] = 1
            
            # è®¡ç®—éšæœºç­–ç•¥æ”¶ç›Š
            random_strategy_returns = random_signal * returns
            random_cumulative = np.prod(1 + random_strategy_returns) - 1
            
            # è®¡ç®—éšæœºç­–ç•¥ç»Ÿè®¡
            random_volatility = np.std(random_strategy_returns) * np.sqrt(250)
            
            random_results.append({
                'total_return': random_cumulative,
                'volatility': random_volatility,
                'signal_count': n_signals
            })
        
        # ç»Ÿè®¡éšæœºç»“æœ
        random_returns = np.array([r['total_return'] for r in random_results])
        random_volatilities = np.array([r['volatility'] for r in random_results])
        
        strategy_return = performance['total_return_strategy']
        
        baseline_comparison = {
            'random_mean_return': random_returns.mean(),
            'random_std_return': random_returns.std(),
            'random_min_return': random_returns.min(),
            'random_max_return': random_returns.max(),
            'random_median_return': np.median(random_returns),
            
            'random_mean_volatility': random_volatilities.mean(),
            'random_std_volatility': random_volatilities.std(),
            
            'strategy_return': strategy_return,
            'strategy_percentile': (random_returns < strategy_return).mean(),
            'outperformance_ratio': (random_returns < strategy_return).mean(),
            
            'n_simulations': self.random_simulations,
            'random_returns': random_returns
        }
        
        print(f"   âœ… éšæœºåŸºå‡†å¯¹æ¯”:")
        print(f"      ç­–ç•¥æ”¶ç›Š: {strategy_return:.2%}")
        print(f"      éšæœºå¹³å‡: {baseline_comparison['random_mean_return']:.2%}")
        print(f"      éšæœºæ ‡å‡†å·®: {baseline_comparison['random_std_return']:.2%}")
        print(f"      ç­–ç•¥åˆ†ä½æ•°: {baseline_comparison['strategy_percentile']:.1%}")
        print(f"      ä¼˜äºéšæœºæ¯”ä¾‹: {baseline_comparison['outperformance_ratio']:.1%}")
        
        return baseline_comparison

    def save_strategy_equity(self, performance: Dict, baseline_comparison: Dict, 
                           selected_clusters: List[Dict], symbol: str = "000001") -> str:
        """
        ä¿å­˜ç­–ç•¥æƒç›Šæ›²çº¿å’Œåˆ†æç»“æœ
        
        Parameters:
        -----------
        performance : Dict
            ç­–ç•¥æ€§èƒ½æŒ‡æ ‡
        baseline_comparison : Dict
            éšæœºåŸºå‡†å¯¹æ¯”ç»“æœ
        selected_clusters : List[Dict]
            é€‰ä¸­çš„èšç±»ä¿¡æ¯
        symbol : str
            è‚¡ç¥¨ä»£ç 
            
        Returns:
        --------
        str
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        print(f"ğŸ’¾ ä¿å­˜ç­–ç•¥æƒç›Šæ›²çº¿å’Œåˆ†æç»“æœ...")
        
        # åˆ›å»ºæƒç›Šæ›²çº¿æ•°æ®
        equity_data = []
        dates = performance['dates']
        benchmark_cumulative = performance['benchmark_cumulative']
        strategy_cumulative = performance['strategy_cumulative']
        benchmark_drawdown = performance['benchmark_drawdown']
        strategy_drawdown = performance['strategy_drawdown']
        
        for i, date in enumerate(dates):
            equity_data.append({
                'date': date,
                'benchmark_equity': benchmark_cumulative[i],
                'strategy_equity': strategy_cumulative[i],
                'benchmark_drawdown': benchmark_drawdown[i],
                'strategy_drawdown': strategy_drawdown[i],
                'excess_equity': strategy_cumulative[i] - benchmark_cumulative[i]
            })
        
        equity_df = pd.DataFrame(equity_data)
        
        # ä¿å­˜æƒç›Šæ›²çº¿CSV
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        equity_file = os.path.join(self.reports_dir, f"strategy_equity_{symbol}_{timestamp}.csv")
        equity_df.to_csv(equity_file, index=False)
        
        print(f"   âœ… æƒç›Šæ›²çº¿å·²ä¿å­˜: {os.path.basename(equity_file)}")
        
        # åˆ›å»ºè¯¦ç»†åˆ†ææŠ¥å‘Š
        report_lines = []
        report_lines.append("=" * 70)
        report_lines.append("é˜¶æ®µ11ï¼šç­–ç•¥ä¿¡å·ä¸å›æµ‹é›å½¢ - åˆ†ææŠ¥å‘Š")
        report_lines.append("=" * 70)
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"è‚¡ç¥¨ä»£ç : {symbol}")
        report_lines.append(f"æµ‹è¯•æœŸé—´: {self.test_start_date} ~ {self.test_end_date}")
        report_lines.append("")
        
        # é€‰ä¸­çš„èšç±»ä¿¡æ¯
        report_lines.append("ğŸ¯ é€‰ä¸­çš„æœ€ä½³èšç±»:")
        for i, cluster_info in enumerate(selected_clusters):
            report_lines.append(f"   èšç±» {i+1}: k={cluster_info['k_value']}, cluster_id={cluster_info['cluster_id']}")
            report_lines.append(f"      è®­ç»ƒæ”¶ç›Š: {cluster_info['train_mean_return']:+.6f} (æ’å: {cluster_info['train_rank']})")
            report_lines.append(f"      æµ‹è¯•æ”¶ç›Š: {cluster_info['test_mean_return']:+.6f} (æ’å: {cluster_info['test_rank']})")
            report_lines.append(f"      å…¨å±€æ’å: {cluster_info['global_rank']}")
        report_lines.append("")
        
        # ç­–ç•¥æ€§èƒ½
        report_lines.append("ğŸ“Š ç­–ç•¥æ€§èƒ½:")
        report_lines.append(f"   åŸºå‡†æ€»æ”¶ç›Š: {performance['total_return_benchmark']:+.2%}")
        report_lines.append(f"   ç­–ç•¥æ€»æ”¶ç›Š: {performance['total_return_strategy']:+.2%}")
        report_lines.append(f"   è¶…é¢æ”¶ç›Š: {performance['excess_return']:+.2%}")
        report_lines.append("")
        report_lines.append(f"   åŸºå‡†å¹´åŒ–æ”¶ç›Š: {performance['annual_return_benchmark']:+.2%}")
        report_lines.append(f"   ç­–ç•¥å¹´åŒ–æ”¶ç›Š: {performance['annual_return_strategy']:+.2%}")
        report_lines.append("")
        report_lines.append(f"   åŸºå‡†æ³¢åŠ¨ç‡: {performance['volatility_benchmark']:.2%}")
        report_lines.append(f"   ç­–ç•¥æ³¢åŠ¨ç‡: {performance['volatility_strategy']:.2%}")
        report_lines.append("")
        report_lines.append(f"   åŸºå‡†å¤æ™®æ¯”ç‡: {performance['sharpe_benchmark']:.3f}")
        report_lines.append(f"   ç­–ç•¥å¤æ™®æ¯”ç‡: {performance['sharpe_strategy']:.3f}")
        report_lines.append("")
        report_lines.append(f"   åŸºå‡†æœ€å¤§å›æ’¤: {performance['max_drawdown_benchmark']:+.2%}")
        report_lines.append(f"   ç­–ç•¥æœ€å¤§å›æ’¤: {performance['max_drawdown_strategy']:+.2%}")
        report_lines.append("")
        report_lines.append(f"   ä¿¡å·æ•°é‡: {performance['signal_count']}")
        report_lines.append(f"   ä¿¡å·æ¯”ä¾‹: {performance['signal_ratio']:.2%}")
        report_lines.append(f"   ä¿¡å·èƒœç‡: {performance['win_rate']:.2%}")
        report_lines.append("")
        
        # éšæœºåŸºå‡†å¯¹æ¯”
        report_lines.append("ğŸ² éšæœºåŸºå‡†å¯¹æ¯”:")
        report_lines.append(f"   æ¨¡æ‹Ÿæ¬¡æ•°: {baseline_comparison['n_simulations']}")
        report_lines.append(f"   ç­–ç•¥æ”¶ç›Š: {baseline_comparison['strategy_return']:+.2%}")
        report_lines.append(f"   éšæœºå¹³å‡æ”¶ç›Š: {baseline_comparison['random_mean_return']:+.2%}")
        report_lines.append(f"   éšæœºæ”¶ç›Šæ ‡å‡†å·®: {baseline_comparison['random_std_return']:.2%}")
        report_lines.append(f"   éšæœºæ”¶ç›ŠèŒƒå›´: {baseline_comparison['random_min_return']:+.2%} ~ {baseline_comparison['random_max_return']:+.2%}")
        report_lines.append(f"   ç­–ç•¥åˆ†ä½æ•°: {baseline_comparison['strategy_percentile']:.1%}")
        report_lines.append(f"   ä¼˜äºéšæœºæ¯”ä¾‹: {baseline_comparison['outperformance_ratio']:.1%}")
        report_lines.append("")
        
        # éªŒæ”¶ç»“æœ
        report_lines.append("âœ… éªŒæ”¶ç»“æœ:")
        
        # æ£€æŸ¥ç­–ç•¥æ”¶ç›Šä¸å°äºåŸºå‡†
        benchmark_check = performance['total_return_strategy'] >= performance['total_return_benchmark']
        report_lines.append(f"   ç­–ç•¥æ”¶ç›Šä¸å°äºåŸºå‡†: {'âœ… é€šè¿‡' if benchmark_check else 'âŒ æœªé€šè¿‡'}")
        report_lines.append(f"      ç­–ç•¥ {performance['total_return_strategy']:+.2%} vs åŸºå‡† {performance['total_return_benchmark']:+.2%}")
        
        # æ£€æŸ¥å›æ’¤ä¸è¿‡åˆ†æ”¾å¤§
        drawdown_check = abs(performance['max_drawdown_strategy']) <= abs(performance['max_drawdown_benchmark']) * 1.5
        report_lines.append(f"   å›æ’¤ä¸è¿‡åˆ†æ”¾å¤§: {'âœ… é€šè¿‡' if drawdown_check else 'âŒ æœªé€šè¿‡'}")
        report_lines.append(f"      ç­–ç•¥å›æ’¤ {performance['max_drawdown_strategy']:+.2%} vs åŸºå‡†å›æ’¤ {performance['max_drawdown_benchmark']:+.2%}")
        
        # æ£€æŸ¥éšæœºåŸºå‡†ä¼˜åŠ¿
        random_check = baseline_comparison['strategy_percentile'] >= 0.6
        report_lines.append(f"   ä¼˜äºéšæœºåŸºå‡†: {'âœ… é€šè¿‡' if random_check else 'âŒ æœªé€šè¿‡'}")
        report_lines.append(f"      ç­–ç•¥åœ¨éšæœºåŸºå‡†ä¸­æ’å {baseline_comparison['strategy_percentile']:.1%} åˆ†ä½")
        
        overall_pass = benchmark_check and drawdown_check and random_check
        report_lines.append("")
        report_lines.append(f"ğŸ† æ€»ä½“éªŒæ”¶: {'âœ… é€šè¿‡' if overall_pass else 'âŒ æœªé€šè¿‡'}")
        
        report_lines.append("")
        report_lines.append("=" * 70)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = os.path.join(self.reports_dir, f"strategy_analysis_{symbol}_{timestamp}.txt")
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("\n".join(report_lines))
        
        print(f"   ğŸ“‹ åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {os.path.basename(report_file)}")
        
        return equity_file

    def run_complete_backtest(self, symbol: str = "000001", top_n: int = 2) -> Dict:
        """
        è¿è¡Œå®Œæ•´çš„ç­–ç•¥å›æµ‹æµç¨‹
        
        Parameters:
        -----------
        symbol : str, default="000001"
            è‚¡ç¥¨ä»£ç 
        top_n : int, default=2
            é€‰æ‹©top Nä¸ªèšç±»
            
        Returns:
        --------
        dict
            å®Œæ•´çš„å›æµ‹ç»“æœ
        """
        print("=" * 70)
        print("é˜¶æ®µ11ï¼šç­–ç•¥ä¿¡å·ä¸å›æµ‹é›å½¢")
        print("=" * 70)
        print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"è‚¡ç¥¨ä»£ç : {symbol}")
        print(f"æµ‹è¯•æœŸé—´: {self.test_start_date} ~ {self.test_end_date}")
        print(f"é€‰æ‹©ç­–ç•¥: top{top_n} è®­ç»ƒæ”¶ç›Šèšç±»")
        print()
        
        try:
            # 1. åŠ è½½èšç±»è¯„ä¼°ç»“æœ
            cluster_results = self.load_cluster_evaluation_results()
            
            # 2. é€‰æ‹©è®­ç»ƒé›†æ”¶ç›Šæœ€é«˜çš„èšç±»
            selection_results = self.select_best_clusters(cluster_results['comparison_df'], top_n)
            selected_clusters = selection_results['selected_clusters']
            
            # 4. å‡†å¤‡æµ‹è¯•æ•°æ®ï¼ˆä½¿ç”¨InfluxDBæ–°æ•°æ® + pca_stateå®Œæ•´æµç¨‹ï¼‰
            test_data = self.prepare_test_data(symbol)
            
            # 5. ç”Ÿæˆäº¤æ˜“ä¿¡å·
            signal_data = self.generate_trading_signals(test_data, selected_clusters)
            
            # 6. è®¡ç®—ç­–ç•¥æ€§èƒ½
            performance = self.calculate_strategy_performance(signal_data)
            
            # 7. éšæœºåŸºå‡†å¯¹æ¯”
            baseline_comparison = self.run_random_baseline(signal_data, performance)
            
            # 8. ä¿å­˜æƒç›Šæ›²çº¿å’Œåˆ†æç»“æœ
            equity_file = self.save_strategy_equity(performance, baseline_comparison, selected_clusters, symbol)
            
            # 9. æ•´åˆå®Œæ•´ç»“æœ
            results = {
                'symbol': symbol,
                'test_period': f"{self.test_start_date} ~ {self.test_end_date}",
                'selected_clusters': selected_clusters,
                'performance': performance,
                'baseline_comparison': baseline_comparison,
                'signal_data': signal_data,
                'equity_file': equity_file,
                'backtest_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            # 10. æœ€ç»ˆæŠ¥å‘Š
            print("\n" + "=" * 70)
            print("ğŸ‰ é˜¶æ®µ11å®Œæˆï¼")
            print("=" * 70)
            
            # éªŒæ”¶æ£€æŸ¥
            benchmark_check = performance['total_return_strategy'] >= performance['total_return_benchmark']
            drawdown_check = abs(performance['max_drawdown_strategy']) <= abs(performance['max_drawdown_benchmark']) * 1.5
            random_check = baseline_comparison['strategy_percentile'] >= 0.6
            overall_pass = benchmark_check and drawdown_check and random_check
            
            print(f"ğŸ“Š ç­–ç•¥æ”¶ç›Š: {performance['total_return_strategy']:+.2%} (åŸºå‡†: {performance['total_return_benchmark']:+.2%})")
            print(f"ğŸ“‰ æœ€å¤§å›æ’¤: {performance['max_drawdown_strategy']:+.2%} (åŸºå‡†: {performance['max_drawdown_benchmark']:+.2%})")
            print(f"ğŸ² éšæœºæ’å: {baseline_comparison['strategy_percentile']:.1%} åˆ†ä½")
            print(f"ğŸ† éªŒæ”¶ç»“æœ: {'âœ… é€šè¿‡' if overall_pass else 'âŒ æœªé€šè¿‡'}")
            print(f"ğŸ’¾ æƒç›Šæ›²çº¿: {os.path.basename(equity_file)}")
            print("=" * 70)
            
            return results
            
        except Exception as e:
            print(f"\nâŒ å›æµ‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise


def main():
    """
    ä¸»å‡½æ•°ï¼šè¿è¡Œé˜¶æ®µ11ç­–ç•¥ä¿¡å·ä¸å›æµ‹é›å½¢
    """
    try:
        # åˆå§‹åŒ–ç­–ç•¥å›æµ‹å™¨
        backtest = StrategyBacktest()
        
        # è¿è¡Œå®Œæ•´å›æµ‹
        results = backtest.run_complete_backtest(
            symbol="000001",  # å¹³å®‰é“¶è¡Œ
            top_n=3          # é€‰æ‹©top3èšç±»
        )
        
        print(f"\nâœ¨ å›æµ‹å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {backtest.reports_dir}")
        return results
        
    except Exception as e:
        print(f"\nğŸ’¥ ä¸»å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    main()