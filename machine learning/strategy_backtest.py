#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é˜¶æ®µ11ï¼šç­–ç•¥ä¿¡å·ä¸å›æµ‹é›å½¢

åŠ¨ä½œï¼š
1. é€‰è®­ç»ƒé›†æ”¶ç›Šæœ€é«˜çš„ cluster åˆ—è¡¨ï¼ˆtop1 æˆ– top2ï¼‰
2. ç”Ÿæˆæµ‹è¯•é›† signal=1/0
3. è®¡ç®—ç­–ç•¥æ”¶ç›Š vs åŸºå‡†ï¼ˆæŒæœ‰ï¼‰ â†’ ä¿å­˜æƒç›Šæ›²çº¿
4. è¾“å‡º reports/strategy_equity.csv
5. æœ‰éšæœºåŸºå‡†ï¼ˆ100 æ¬¡éšæœºä¿¡å·ï¼‰å¯¹æ¯”

éªŒæ”¶ï¼š
- ç­–ç•¥æ”¶ç›Šä¸å°äºåŸºå‡†ä¸”å›æ’¤ä¸è¿‡åˆ†æ”¾å¤§
- æœ‰éšæœºåŸºå‡†å¯¹æ¯”

æ•°æ®è¯´æ˜ï¼š
- ä½¿ç”¨ InfluxDB å¯¼å…¥æ–°æ•°æ®ä½œä¸ºæµ‹è¯•é›†ï¼ˆ2025å¹´1æœˆ1æ—¥è‡³2025å¹´8æœˆ1æ—¥ï¼‰
- ä½¿ç”¨ pca_state çš„æ•°æ®é¢„å¤„ç†
- ä¸¥æ ¼æŒ‰ç…§åŸæœ‰ä»£ç çš„å‘½åæ ¼å¼å’Œå‚æ•°è®¾å®š

ä½œè€…: Assistant
æ—¥æœŸ: 2025-09-29
"""

import os
import sys
import glob
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import pickle
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# å¯¼å…¥å¿…è¦æ¨¡å—
from sklearn.cluster import KMeans
from sklearn.preprocessing import RobustScaler
from pca_state import run_complete_feature_pipeline, run_complete_target_pipeline, PCAStateGenerator

class StrategyBacktest:
    """
    ç­–ç•¥ä¿¡å·ä¸å›æµ‹é›å½¢
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. ä» cluster_evaluate çš„èšç±»ç»“æœä¸­é€‰æ‹©æ”¶ç›Šæœ€é«˜çš„èšç±»
    2. å¯¹æ–°æµ‹è¯•æ•°æ®ï¼ˆ2025å¹´1æœˆ1æ—¥è‡³2025å¹´8æœˆ1æ—¥ï¼‰ç”Ÿæˆäº¤æ˜“ä¿¡å·
    3. è®¡ç®—ç­–ç•¥æ”¶ç›Šå¹¶ä¸åŸºå‡†å¯¹æ¯”
    4. éšæœºåŸºå‡†å¯¹æ¯”éªŒè¯
    5. ä¿å­˜æƒç›Šæ›²çº¿å’Œåˆ†ææŠ¥å‘Š
    """
    
    def __init__(self, reports_dir: str = "machine learning/ML output/reports"):
        """
        åˆå§‹åŒ–ç­–ç•¥å›æµ‹å™¨
        
        Parameters:
        -----------
        reports_dir : str
            æŠ¥å‘Šä¿å­˜ç›®å½•
        """
        # è®¾ç½®ç›®å½•è·¯å¾„
        self.project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        
        if os.path.isabs(reports_dir):
            self.reports_dir = reports_dir
        else:
            self.reports_dir = os.path.join(self.project_root, reports_dir)
        
        self.ml_output_dir = os.path.join(self.project_root, "machine learning/ML output")
        self.models_dir = os.path.join(self.ml_output_dir, "models")
        self.states_dir = os.path.join(self.ml_output_dir, "states")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.reports_dir, exist_ok=True)
        
        # ç­–ç•¥å‚æ•°
        self.test_start_date = "2023-01-01"
        self.test_end_date = "2024-12-01"
        self.random_simulations = 100
        
        # å­˜å‚¨æ¨¡å‹å’Œæ•°æ®
        self.cluster_models = {}
        
        print(f"ğŸ¯ ç­–ç•¥å›æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ æŠ¥å‘Šç›®å½•: {self.reports_dir}")
        print(f"ğŸ“… æµ‹è¯•æœŸé—´: {self.test_start_date} ~ {self.test_end_date}")
        
        # å­˜å‚¨è®­ç»ƒé˜¶æ®µé€‰æ‹©çš„æœ€ä½³PCä¿¡æ¯
        self.best_pc = None
        self.pc_direction = None
        self.pc_threshold = None
        self.pc_threshold_quantile = None

    def load_cluster_evaluation_results(self) -> Dict:
        """
        åŠ è½½ cluster_evaluate çš„èšç±»ç»“æœ
        
        Returns:
        --------
        dict
            èšç±»è¯„ä¼°ç»“æœ
        """
        print("ğŸ“Š åŠ è½½èšç±»è¯„ä¼°ç»“æœ...")
        
        # åŠ è½½èšç±»æ¨¡å‹
        cluster_models_file = os.path.join(self.reports_dir, "cluster_models.pkl")
        if not os.path.exists(cluster_models_file):
            raise FileNotFoundError(f"èšç±»æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {cluster_models_file}")
        
        with open(cluster_models_file, 'rb') as f:
            self.cluster_models = pickle.load(f)
        
        print(f"   âœ… åŠ è½½äº† {len(self.cluster_models)} ä¸ªèšç±»æ¨¡å‹")
        
        # åŠ è½½PCå…ƒæ•°æ®ï¼ˆä»cluster_evaluateçš„è®­ç»ƒé˜¶æ®µä¿å­˜ï¼‰
        pc_metadata_file = os.path.join(self.reports_dir, "pc_metadata.pkl")
        if os.path.exists(pc_metadata_file):
            with open(pc_metadata_file, 'rb') as f:
                pc_metadata = pickle.load(f)
            
            self.best_pc = pc_metadata.get('best_pc', 'PC1')
            self.pc_direction = pc_metadata.get('pc_direction', 1.0)
            self.pc_threshold = pc_metadata.get('pc_threshold', 0.0)
            self.pc_threshold_quantile = pc_metadata.get('threshold_quantile', 0.6)
            ic_value = pc_metadata.get('ic_value', 0.0)
            
            print(f"   âœ… åŠ è½½PCå…ƒæ•°æ®: {self.best_pc} (IC={ic_value:+.4f}, æ–¹å‘={self.pc_direction:+.1f}, é—¨æ§›={self.pc_threshold:.4f})")
        else:
            print(f"   âš ï¸ æœªæ‰¾åˆ°PCå…ƒæ•°æ®æ–‡ä»¶ï¼Œå°†ä½¿ç”¨é»˜è®¤PC1")
            self.best_pc = 'PC1'
            self.pc_direction = 1.0
            self.pc_threshold = 0.0
            self.pc_threshold_quantile = 0.6
        
        # åŠ è½½èšç±»æ¯”è¾ƒç»“æœ
        comparison_file = os.path.join(self.reports_dir, "cluster_comparison.csv")
        if not os.path.exists(comparison_file):
            raise FileNotFoundError(f"èšç±»æ¯”è¾ƒæ–‡ä»¶ä¸å­˜åœ¨: {comparison_file}")
        
        comparison_df = pd.read_csv(comparison_file)
        
        print(f"   ğŸ“‹ èšç±»æ¯”è¾ƒæ•°æ®: {len(comparison_df)} æ¡è®°å½•")
        
        return {
            'cluster_models': self.cluster_models,
            'comparison_df': comparison_df
        }

    def select_best_clusters(self, comparison_df: pd.DataFrame, top_n: int = 3, 
                           min_cluster_pct: float = 0.10, max_cluster_pct: float = 0.60) -> Dict:
        """
        é€‰æ‹©å…¨å±€æ’åæœ€é«˜çš„èšç±»ï¼ˆæŒ‰global_rankæ’åºï¼‰
        
        çº¦æŸæ¡ä»¶ï¼ˆé¿å…æç«¯ç°‡ï¼‰ï¼š
        1. ç°‡å æ¯”å¿…é¡»åœ¨ [min_cluster_pct, max_cluster_pct] åŒºé—´å†…
        2. æ ·æœ¬å¤–æ”¶ç›Šï¼ˆtest_mean_returnï¼‰å¿…é¡»ä¸ºæ­£
        3. å¿…é¡»é€šè¿‡éªŒè¯ï¼ˆvalidation_passed=Trueï¼‰
        
        Parameters:
        -----------
        comparison_df : pd.DataFrame
            èšç±»æ¯”è¾ƒæ•°æ®ï¼Œå¿…é¡»åŒ…å«train_samplesåˆ—
        top_n : int, default=3
            é€‰æ‹© top N ä¸ªèšç±»
        min_cluster_pct : float, default=0.10
            æœ€å°ç°‡å æ¯”ï¼ˆ10%ï¼‰ï¼Œä½äºæ­¤å€¼çš„ç°‡ä¼šè¢«è¿‡æ»¤
        max_cluster_pct : float, default=0.60
            æœ€å¤§ç°‡å æ¯”ï¼ˆ60%ï¼‰ï¼Œé«˜äºæ­¤å€¼çš„ç°‡ä¼šè¢«è¿‡æ»¤
            
        Returns:
        --------
        dict
            æœ€ä½³èšç±»ä¿¡æ¯
        """
        print(f"ğŸ¯ é€‰æ‹©å…¨å±€æ’åæœ€é«˜çš„ top{top_n} èšç±»...")
        print(f"   ï¿½ ç°‡å æ¯”çº¦æŸ: [{min_cluster_pct:.0%}, {max_cluster_pct:.0%}]")
        print(f"   ğŸ“ˆ æ ·æœ¬å¤–æ”¶ç›Šçº¦æŸ: å¿…é¡» > 0")
        
        # === æ­¥éª¤1: åªé€‰æ‹©éªŒè¯é€šè¿‡çš„èšç±» ===
        valid_clusters = comparison_df[comparison_df['validation_passed'] == True].copy()
        
        if len(valid_clusters) == 0:
            print("   âš ï¸ è­¦å‘Šï¼šæ²¡æœ‰éªŒè¯é€šè¿‡çš„èšç±»ï¼Œä½¿ç”¨æ‰€æœ‰èšç±»")
            valid_clusters = comparison_df.copy()
        
        print(f"   âœ… éªŒè¯é€šè¿‡: {len(valid_clusters)}/{len(comparison_df)} ä¸ªèšç±»")
        
        # === æ­¥éª¤2: è¿‡æ»¤ç°‡å æ¯”å¼‚å¸¸çš„ç°‡ ===
        if 'train_samples' in valid_clusters.columns:
            group_totals = valid_clusters.groupby('k_value')['train_samples'].transform('sum')
            # é¿å…é™¤ä»¥0
            group_totals = group_totals.replace(0, np.nan)
            valid_clusters['cluster_pct'] = valid_clusters['train_samples'] / group_totals
            
            # è¿‡æ»¤æ‰å æ¯”è¿‡å°/è¿‡å¤§çš„ç°‡
            before_count = len(valid_clusters)
            valid_clusters = valid_clusters[
                (valid_clusters['cluster_pct'].notna()) &
                (valid_clusters['cluster_pct'] >= min_cluster_pct) & 
                (valid_clusters['cluster_pct'] <= max_cluster_pct)
            ].copy()
            after_count = len(valid_clusters)
            
            if before_count > after_count:
                filtered = before_count - after_count
                print(f"   ğŸ—‘ï¸  è¿‡æ»¤å æ¯”å¼‚å¸¸ç°‡: {filtered} ä¸ª (å æ¯”ä¸åœ¨[{min_cluster_pct:.0%}, {max_cluster_pct:.0%}])")
            
            if len(valid_clusters) == 0:
                print("   âš ï¸ è­¦å‘Š: æ‰€æœ‰ç°‡éƒ½è¢«è¿‡æ»¤ï¼Œæ”¾å®½å æ¯”è¦æ±‚")
                valid_clusters = comparison_df[comparison_df['validation_passed'] == True].copy()
                if 'train_samples' in valid_clusters.columns:
                    group_totals = valid_clusters.groupby('k_value')['train_samples'].transform('sum').replace(0, np.nan)
                    valid_clusters['cluster_pct'] = valid_clusters['train_samples'] / group_totals
        else:
            print("   âš ï¸ è­¦å‘Š: æ•°æ®ä¸­æ— train_samplesåˆ—ï¼Œè·³è¿‡å æ¯”è¿‡æ»¤")
        
        # === æ­¥éª¤3: è¿‡æ»¤æ ·æœ¬å¤–æ”¶ç›Šä¸ºè´Ÿçš„ç°‡ ===
        if 'test_mean_return' in valid_clusters.columns:
            before_count = len(valid_clusters)
            valid_clusters = valid_clusters[valid_clusters['test_mean_return'] > 0].copy()
            after_count = len(valid_clusters)
            
            if before_count > after_count:
                filtered = before_count - after_count
                print(f"   ğŸ—‘ï¸  è¿‡æ»¤æ ·æœ¬å¤–è´Ÿæ”¶ç›Šç°‡: {filtered} ä¸ª")
            
            if len(valid_clusters) == 0:
                print("   âš ï¸ è­¦å‘Š: æ‰€æœ‰ç°‡æ ·æœ¬å¤–æ”¶ç›Šéƒ½ä¸ºè´Ÿï¼Œé€€åŒ–é€‰æ‹©")
                valid_clusters = comparison_df[comparison_df['validation_passed'] == True].copy()
                if 'train_samples' in valid_clusters.columns:
                    total_train_samples = valid_clusters['train_samples'].sum()
                    valid_clusters['cluster_pct'] = valid_clusters['train_samples'] / total_train_samples
        
        # === æ­¥éª¤4: æŒ‰global_rankæ’åºï¼ˆä»å°åˆ°å¤§ï¼Œrankè¶Šå°è¶Šå¥½ï¼‰ï¼Œé€‰æ‹© top N ===
        top_clusters = valid_clusters.nsmallest(top_n, 'global_rank')
        
        selected_clusters = []
        for _, row in top_clusters.iterrows():
            cluster_pct = row.get('cluster_pct', None)
            
            cluster_info = {
                'k_value': int(row['k_value']),
                'cluster_id': int(row['cluster_id']),
                'train_mean_return': row['train_mean_return'],
                'test_mean_return': row['test_mean_return'],
                'train_rank': int(row['train_rank']),
                'test_rank': int(row['test_rank']),
                'global_rank': int(row['global_rank']),
                'train_samples': int(row['train_samples']) if 'train_samples' in row else None,
                'cluster_pct': float(cluster_pct) if cluster_pct is not None else None
            }
            selected_clusters.append(cluster_info)
            
            pct_info = f"å æ¯”: {cluster_pct:.1%}" if cluster_pct is not None else ""
            print(f"   âœ… é€‰ä¸­: k={cluster_info['k_value']}, cluster_id={cluster_info['cluster_id']} "
                  f"(å…¨å±€æ’å: {cluster_info['global_rank']}) {pct_info}")
            print(f"      è®­ç»ƒæ”¶ç›Š: {cluster_info['train_mean_return']:+.6f} (è®­ç»ƒæ’å: {cluster_info['train_rank']})")
            print(f"      æµ‹è¯•æ”¶ç›Š: {cluster_info['test_mean_return']:+.6f} (æµ‹è¯•æ’å: {cluster_info['test_rank']})")
        
        return {
            'selected_clusters': selected_clusters,
            'selection_method': f'top_{top_n}_global_rank_with_constraints'
        }

    def prepare_test_data(self, symbol: str = "000001") -> pd.DataFrame:
        """
        å‡†å¤‡æµ‹è¯•æ•°æ®ï¼ˆ2025å¹´1æœˆ1æ—¥è‡³2025å¹´8æœˆ1æ—¥ï¼‰
        ä½¿ç”¨ pca_state çš„å®Œæ•´æ•°æ®é¢„å¤„ç†æµç¨‹
        
        Parameters:
        -----------
        symbol : str
            è‚¡ç¥¨ä»£ç 
            
        Returns:
        --------
        pd.DataFrame
            é¢„å¤„ç†åçš„æµ‹è¯•æ•°æ®ï¼ŒåŒ…å«PCAç‰¹å¾å’Œç›®æ ‡å˜é‡
        """
        print(f"ğŸ”§ å‡†å¤‡æµ‹è¯•æ•°æ®: {symbol} ({self.test_start_date} ~ {self.test_end_date})")
        print("   ä½¿ç”¨ pca_state çš„å®Œæ•´æ•°æ®é¢„å¤„ç†æµç¨‹")
        try:
            # é…ç½®å‚æ•°ï¼ˆä¸ pca_state.main() ç›¸åŒï¼‰
            config = {
                'symbol': symbol,
                'start_date': self.test_start_date,  # 2025-01-01
                'end_date': self.test_end_date,      # 2025-08-01
                'use_auto_features': True,           # ä½¿ç”¨è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆ
                'final_k_features': 15,              # æœ€ç»ˆç‰¹å¾æ•°é‡
                'target_periods': [1, 5, 10],        # ç›®æ ‡æ—¶é—´çª—å£
                'pca_components': 0.9,               # PCAè§£é‡Šæ–¹å·®æ¯”ä¾‹
                'train_ratio': 0.8                   # è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆç”¨äºå†…éƒ¨åˆ‡åˆ†ï¼‰
            }
            
            print("   ğŸ“‹ æ‰§è¡Œé…ç½®:")
            for key, value in config.items():
                print(f"      {key}: {value}")
            print()
            
            # === æ­¥éª¤1: å®Œæ•´ç‰¹å¾å·¥ç¨‹æµç¨‹ ===
            print("   ğŸ”§ æ­¥éª¤1: å®Œæ•´ç‰¹å¾å·¥ç¨‹æµç¨‹")
            feature_results = run_complete_feature_pipeline(
                symbol=config['symbol'],
                start_date=config['start_date'],
                end_date=config['end_date'],
                use_auto_features=config['use_auto_features'],
                final_k_features=config['final_k_features']
            )
            
            if not feature_results.get('success'):
                raise ValueError(f"ç‰¹å¾å·¥ç¨‹å¤±è´¥: {feature_results.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
            print("      âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ")
            
            # === æ­¥éª¤2: å®Œæ•´ç›®æ ‡å˜é‡å·¥ç¨‹æµç¨‹ ===
            print("   ğŸ¯ æ­¥éª¤2: å®Œæ•´ç›®æ ‡å˜é‡å·¥ç¨‹æµç¨‹")
            target_results = run_complete_target_pipeline(
                scaled_features_df=feature_results['scaled_features_df'],
                symbol=config['symbol'],
                target_periods=config['target_periods']
            )
            
            if not target_results.get('success'):
                raise ValueError(f"ç›®æ ‡å˜é‡å·¥ç¨‹å¤±è´¥: {target_results.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
            print("      âœ… ç›®æ ‡å˜é‡å·¥ç¨‹å®Œæˆ")
            
            # === æ­¥éª¤3: PCAçŠ¶æ€ç”Ÿæˆ ===
            print("   ï¿½ æ­¥éª¤3: PCAçŠ¶æ€ç”Ÿæˆ")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡å‡†åŒ–ç‰¹å¾CSVæ–‡ä»¶
            csv_path = feature_results.get('csv_path')
            if not csv_path or not os.path.exists(csv_path):
                raise ValueError("æ ‡å‡†åŒ–ç‰¹å¾CSVæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡ŒPCAé™ç»´")
            
            # åˆå§‹åŒ–PCAçŠ¶æ€ç”Ÿæˆå™¨
            pca_generator = PCAStateGenerator()
            
            # ç”ŸæˆPCAçŠ¶æ€ï¼ˆä½¿ç”¨å®Œæ•´æµç¨‹ï¼‰
            pca_results = pca_generator.generate_pca_states(
                csv_path=csv_path,
                symbol=config['symbol'],
                n_components=config['pca_components'],
                train_ratio=config['train_ratio']
            )
            
            if not pca_results or 'n_components' not in pca_results:
                raise ValueError("PCAçŠ¶æ€ç”Ÿæˆå¤±è´¥")
            
            print("      âœ… PCAçŠ¶æ€ç”Ÿæˆå®Œæˆ")
            
            # === æ­¥éª¤4: æ„å»ºæœ€ç»ˆæµ‹è¯•æ•°æ® ===
            print("   ğŸ”¨ æ­¥éª¤4: æ„å»ºæœ€ç»ˆæµ‹è¯•æ•°æ®")
            
            # è·å–å®Œæ•´æ•°æ®é›†ï¼ˆåŒ…å«ç›®æ ‡å˜é‡ï¼‰
            complete_dataset = target_results['complete_dataset']
            
            # ã€å…³é”®ä¿®å¤ã€‘PCAç”±äºpurgeå¯¼è‡´è®­ç»ƒé›†å‡å°‘äº†purge_periodsè¡Œ
            # éœ€è¦æ ¹æ®PCAçš„train_indexå’Œtest_indexæ¥å¯¹é½æ•°æ®
            train_index = pca_results['train_index']
            test_index = pca_results['test_index']
            
            # åˆå¹¶trainå’Œtestçš„ç´¢å¼•ï¼ˆæ³¨æ„ï¼štrain_indexå·²ç»è¢«purgeè¿‡äº†ï¼‰
            # æ‰€ä»¥æˆ‘ä»¬éœ€è¦ä½¿ç”¨PCAå®é™…ä½¿ç”¨çš„ç´¢å¼•
            pca_used_indices = train_index.union(test_index)
            
            # ä»complete_datasetä¸­æå–å¯¹åº”çš„æ•°æ®
            complete_dataset_aligned = complete_dataset.loc[pca_used_indices]
            
            print(f"      ğŸ“Š å®Œæ•´æ•°æ®é›†: {len(complete_dataset)} è¡Œ")
            print(f"      ğŸ“Š PCAä½¿ç”¨æ•°æ®: {len(pca_used_indices)} è¡Œ (è®­ç»ƒ:{len(train_index)} + æµ‹è¯•:{len(test_index)})")
            print(f"      ğŸš« Purge gap: {len(complete_dataset) - len(pca_used_indices)} è¡Œ")
            
            # è·å–PCAé™ç»´åçš„ç‰¹å¾æ•°æ®
            states_all = np.vstack([pca_results['states_train'], pca_results['states_test']])
            
            # åˆ›å»ºPCAç‰¹å¾DataFrameï¼ˆä½¿ç”¨å¯¹é½åçš„ç´¢å¼•ï¼‰
            pca_columns = [f'PC{i+1}' for i in range(states_all.shape[1])]
            pca_df = pd.DataFrame(states_all, index=complete_dataset_aligned.index, columns=pca_columns)
            
            # åˆå¹¶PCAç‰¹å¾å’Œç›®æ ‡å˜é‡ï¼ˆä½¿ç”¨å¯¹é½åçš„æ•°æ®ï¼‰
            target_cols = [col for col in complete_dataset_aligned.columns 
                          if col.startswith('future_return_') or col.startswith('label_')]
            
            test_data = pd.concat([
                pca_df,
                complete_dataset_aligned[target_cols + ['close']]
            ], axis=1)
            
            # ç§»é™¤åŒ…å«NaNçš„è¡Œï¼ˆä¸»è¦æ˜¯æœ«å°¾çš„ç›®æ ‡å˜é‡NaNï¼‰
            test_data = test_data.dropna()
            
            print(f"      âœ… æµ‹è¯•æ•°æ®æ„å»ºå®Œæˆ: {test_data.shape}")
            print(f"      ğŸ“Š PCAç‰¹å¾: {len(pca_columns)} ç»´")
            print(f"      ğŸ¯ ç›®æ ‡å˜é‡: {len([col for col in target_cols if col.startswith('future_return_')])} ä¸ª")
            print(f"      ğŸ“… æ•°æ®æ—¶é—´èŒƒå›´: {test_data.index.min().date()} ~ {test_data.index.max().date()}")
            
            # === æ­¥éª¤5: PCä¿¡æ¯å·²ä»cluster_evaluateçš„è®­ç»ƒé˜¶æ®µåŠ è½½ ===
            print("   â„¹ï¸ æ­¥éª¤5: ä½¿ç”¨å·²åŠ è½½çš„PCå…ƒæ•°æ®")
            print(f"      ğŸ“Œ æœ€ä½³PC: {self.best_pc} (æ–¹å‘: {self.pc_direction:+.1f})")
            if self.pc_threshold is not None:
                quantile = self.pc_threshold_quantile if self.pc_threshold_quantile is not None else 0.6
                print(f"      ğŸ¯ å¼ºåº¦é—¨æ§›: {self.pc_threshold:.4f} (q={quantile:.2f})")
            print("      ğŸ’¡ PCä¿¡æ¯æ¥æº: cluster_evaluateè®­ç»ƒé˜¶æ®µï¼ˆå†å²æ•°æ®ï¼‰")
            
            return test_data
            
        except Exception as e:
            print(f"   âŒ æµ‹è¯•æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise

    def generate_trading_signals(self, test_data: pd.DataFrame, selected_clusters: List[Dict]) -> pd.DataFrame:
        """
        ç”Ÿæˆäº¤æ˜“ä¿¡å·ï¼ˆæ”¹è¿›ç‰ˆï¼šé¿å…å‰è§†åå·®ï¼‰
        
        ç­–ç•¥é€»è¾‘ï¼š
        1. èšç±»çŠ¶æ€è¿‡æ»¤ï¼šå±äºé€‰ä¸­ç°‡æ—¶å€™é€‰
        2. PCå¼ºåº¦é—¨æ§›ï¼šä½¿ç”¨è®­ç»ƒé˜¶æ®µé€‰æ‹©çš„æœ€ä½³PCã€æ–¹å‘å’Œé—¨æ§›å€¼ï¼ˆé¿å…å‰è§†åå·®ï¼‰
        3. æŒæœ‰æœŸï¼šä¿¡å·è§¦å‘åæŒæœ‰3æœŸ
        
        å…³é”®æ”¹è¿›ï¼š
        - æœ€ä½³PCçš„é€‰æ‹©ã€æ–¹å‘ç»Ÿä¸€ã€é—¨æ§›è®¡ç®—å‡åœ¨è®­ç»ƒé˜¶æ®µå®Œæˆ
        - æµ‹è¯•é˜¶æ®µä»…åº”ç”¨è®­ç»ƒé˜¶æ®µç¡®å®šçš„è§„åˆ™ï¼Œä¸å†é‡æ–°é€‰æ‹©æˆ–è®¡ç®—
        - å½»åº•é¿å…æµ‹è¯•æ•°æ®å‚ä¸è§„åˆ™é€‰æ‹©çš„å‰è§†åå·®
        
        Parameters:
        -----------
        test_data : pd.DataFrame
            é¢„å¤„ç†åçš„æµ‹è¯•æ•°æ®
        selected_clusters : List[Dict]
            é€‰ä¸­çš„æœ€ä½³èšç±»
            
        Returns:
        --------
        pd.DataFrame
            åŒ…å«äº¤æ˜“ä¿¡å·çš„æ•°æ®
        """
        print(f"ğŸ“¡ ç”Ÿæˆäº¤æ˜“ä¿¡å· (èšç±»çŠ¶æ€ + PCå¼ºåº¦é—¨æ§› + æŒæœ‰æœŸ)...")
        
        # è·å–PCAç‰¹å¾
        pca_columns = [col for col in test_data.columns if col.startswith('PC')]
        X_pca = test_data[pca_columns].fillna(0).values
        
        # === æ­¥éª¤1: èšç±»çŠ¶æ€è¿‡æ»¤ ===
        print(f"   æ­¥éª¤1: èšç±»çŠ¶æ€è¿‡æ»¤")
        signals = {}
        
        for i, cluster_info in enumerate(selected_clusters):
            k_value = cluster_info['k_value']
            cluster_id = cluster_info['cluster_id']
            
            print(f"      èšç±» {i+1}: k={k_value}, cluster_id={cluster_id}")
            
            # ä½¿ç”¨å¯¹åº”çš„èšç±»æ¨¡å‹
            cluster_model = self.cluster_models[k_value]
            cluster_labels = cluster_model.predict(X_pca)
            
            # ç”Ÿæˆä¿¡å·ï¼šå±äºç›®æ ‡èšç±»æ—¶ä¸º1ï¼Œå¦åˆ™ä¸º0
            signal = (cluster_labels == cluster_id).astype(int)
            signals[f'signal_k{k_value}_c{cluster_id}'] = signal
            
            signal_count = signal.sum()
            signal_ratio = signal_count / len(signal)
            print(f"         çŠ¶æ€ä¿¡å·: {signal_count}/{len(signal)} ({signal_ratio:.2%})")
        
        # ç»¼åˆèšç±»çŠ¶æ€ä¿¡å·ï¼šä»»ä¸€èšç±»å‘å‡ºä¿¡å·åˆ™ä¸º1
        state_signal = np.zeros(len(test_data), dtype=int)
        for signal_col in signals.keys():
            state_signal = np.maximum(state_signal, signals[signal_col])
        
        print(f"      âœ… ç»¼åˆçŠ¶æ€ä¿¡å·: {state_signal.sum()}/{len(state_signal)} ({state_signal.mean():.2%})")
        
        # === æ­¥éª¤2: ä½¿ç”¨è®­ç»ƒé˜¶æ®µé€‰æ‹©çš„æœ€ä½³PCï¼ˆé¿å…å‰è§†åå·®ï¼‰ ===
        print(f"   æ­¥éª¤2: åº”ç”¨è®­ç»ƒé˜¶æ®µé€‰æ‹©çš„æœ€ä½³PC")
        
        if self.best_pc is None or self.pc_direction is None or self.pc_threshold is None:
            print(f"      âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°è®­ç»ƒé˜¶æ®µçš„PCé€‰æ‹©ç»“æœï¼Œè·³è¿‡PCé—¨æ§›è¿‡æ»¤")
            combined_signal = state_signal
        else:
            # ä½¿ç”¨è®­ç»ƒé˜¶æ®µé€‰æ‹©çš„æœ€ä½³PCå’Œæ–¹å‘
            best_col = self.best_pc
            orient = self.pc_direction
            thr = self.pc_threshold
            
            print(f"      æœ€ä½³PC: {best_col} (è®­ç»ƒé˜¶æ®µé€‰æ‹©)")
            print(f"      æ–¹å‘: {'æ­£å‘' if orient > 0 else 'åå‘'} (ç»Ÿä¸€ä¸ºIC>0)")
            threshold_q = self.pc_threshold_quantile if self.pc_threshold_quantile is not None else 0.6
            print(f"      é—¨æ§›å€¼: {thr:.4f} (è®­ç»ƒé˜¶æ®µq={threshold_q:.2f})")
            
            # è®¡ç®—æ•´ä¸ªæµ‹è¯•æ•°æ®çš„PCå¼ºåº¦
            strength = test_data[best_col].fillna(0).values * orient
            
            # === æ­¥éª¤3: åº”ç”¨å¼ºåº¦é—¨æ§›ï¼ˆä½¿ç”¨è®­ç»ƒé˜¶æ®µçš„é—¨æ§›ï¼‰ ===
            print(f"   æ­¥éª¤3: åº”ç”¨å¼ºåº¦é—¨æ§›")
            
            # åº”ç”¨é—¨æ§›ï¼šçŠ¶æ€ä¿¡å· & å¼ºåº¦è¶…è¿‡é—¨æ§›
            gated = (state_signal == 1) & (strength > thr)
            print(f"      é—¨æ§›åä¿¡å·: {gated.sum()}/{len(gated)} ({gated.mean():.2%})")
            
            # === æ­¥éª¤4: æŒæœ‰æœŸï¼ˆhold=3ï¼‰ ===
            print(f"   æ­¥éª¤4: æŒæœ‰æœŸ (hold=3)")
            hold_n = 3
            n = len(test_data)  # æ•°æ®é•¿åº¦
            final_signal = np.zeros_like(gated, dtype=int)
            i = 0
            while i < n:
                if gated[i]:
                    final_signal[i:i+hold_n] = 1
                    i += hold_n
                else:
                    i += 1
            
            combined_signal = final_signal
            print(f"      âœ… æœ€ç»ˆä¿¡å·: {combined_signal.sum()}/{len(combined_signal)} ({combined_signal.mean():.2%})")
            
            # ä¿å­˜å…ƒä¿¡æ¯
            signals['signal_strength_pc'] = best_col
            signals['signal_strength_ic'] = f"è®­ç»ƒé˜¶æ®µé€‰æ‹©"
            signals['signal_threshold_q'] = threshold_q
            signals['signal_threshold_value'] = thr
            signals['signal_hold_n'] = hold_n
        
        signals['signal_combined'] = combined_signal
        
        # æ·»åŠ ä¿¡å·åˆ°æµ‹è¯•æ•°æ®
        result_data = test_data.copy()
        for signal_name, signal_values in signals.items():
            if isinstance(signal_values, np.ndarray):
                result_data[signal_name] = signal_values
            else:
                result_data[signal_name] = signal_values
        
        return result_data

    def calculate_strategy_performance(self, signal_data: pd.DataFrame,
                                       transaction_cost: float = 0.002,
                                       slippage: float = 0.001) -> Dict:
        """
        è®¡ç®—ç­–ç•¥æ”¶ç›Š vs åŸºå‡†ï¼ˆæŒæœ‰ï¼‰
        
        æ”¹è¿›ï¼š
        1. ä¸¥æ ¼T+1æ‰§è¡Œï¼ˆä»Šå¤©ä¿¡å·â†’æ˜å¤©ä»“ä½ï¼‰
        2. æŒ‰å›åˆè®¡è´¹ï¼ˆå¼€+å¹³ä¸ºä¸€ä¸ªå›åˆï¼‰
        3. ç»Ÿä¸€èƒœç‡ã€æ”¶ç›Šç­‰ç»Ÿè®¡å£å¾„
        
        Parameters:
        -----------
        signal_data : pd.DataFrame
            åŒ…å«ä¿¡å·çš„æ•°æ®
        transaction_cost : float, default=0.002
            äº¤æ˜“æˆæœ¬ï¼ˆå•è¾¹ï¼‰
        slippage : float, default=0.001
            æ»‘ç‚¹ï¼ˆå•è¾¹ï¼‰
            
        Returns:
        --------
        dict
            ç­–ç•¥æ€§èƒ½æŒ‡æ ‡
        """
        print(f"ğŸ’° è®¡ç®—ç­–ç•¥æ€§èƒ½ (T+1æ‰§è¡Œ + å›åˆè®¡è´¹)...")
        
        # ä½¿ç”¨future_return_5dä½œä¸ºé¢„æµ‹ç›®æ ‡æ”¶ç›Š
        returns = signal_data['future_return_5d'].fillna(0).values
        signal = signal_data['signal_combined'].values
        
        # === å…³é”®ä¿®å¤1: T+1æ‰§è¡Œ ===
        # ä»Šå¤©çš„ä¿¡å·å†³å®šæ˜å¤©çš„ä»“ä½ï¼Œé¿å…look-ahead bias
        signal_t_plus_1 = np.roll(signal, 1)
        signal_t_plus_1[0] = 0  # ç¬¬ä¸€å¤©æ— ä¿¡å·
        
        print(f"   ğŸ”„ T+1æ‰§è¡Œå¯¹é½:")
        print(f"      åŸå§‹ä¿¡å·: {signal.sum()}/{len(signal)} ({signal.mean():.2%})")
        print(f"      å¯¹é½ä¿¡å·: {signal_t_plus_1.sum()}/{len(signal_t_plus_1)} ({signal_t_plus_1.mean():.2%})")
        
        # === åŸºå‡†ç­–ç•¥ï¼šå§‹ç»ˆæŒæœ‰ ===
        benchmark_returns = returns
        benchmark_cumulative = np.cumprod(1 + benchmark_returns)
        
        # === ç­–ç•¥æ”¶ç›Šï¼šä½¿ç”¨T+1å¯¹é½çš„ä¿¡å· ===
        strategy_returns = signal_t_plus_1 * returns
        strategy_cumulative = np.ones(len(strategy_returns))
        
        for i in range(1, len(strategy_returns)):
            if signal_t_plus_1[i] == 1:
                # æœ‰ä¿¡å·æ—¶ï¼Œä¹°å…¥æŒæœ‰
                strategy_cumulative[i] = strategy_cumulative[i-1] * (1 + returns[i])
            else:
                # æ— ä¿¡å·æ—¶ï¼Œç©ºä»“ï¼Œç´¯è®¡æ”¶ç›Šä¿æŒä¸å˜
                strategy_cumulative[i] = strategy_cumulative[i-1]
        strategy_cumulative[0] = 1 + strategy_returns[0]
        
        # === å…³é”®ä¿®å¤2: æŒ‰å›åˆè®¡è´¹ ===
        # æ¢æ‰‹ç»Ÿè®¡ï¼šè®¡ç®—ä¿¡å·ç¿»è½¬æ¬¡æ•°
        signal_changes = np.abs(np.diff(signal, prepend=signal[0]))
        flips = signal_changes.sum()
        roundtrips = flips / 2.0  # æ¯ä¸¤ä¸ªç¿»è½¬æ„æˆä¸€æ¬¡å®Œæ•´å›åˆï¼ˆå¼€+å¹³ï¼‰
        turnover_rate = roundtrips / len(signal)
        
        # äº¤æ˜“æˆæœ¬ï¼šæŒ‰å›åˆè®¡è´¹ï¼ˆåŒè¾¹ï¼‰
        per_roundtrip_cost = (transaction_cost + slippage) * 2
        total_transaction_cost = roundtrips * per_roundtrip_cost
        
        print(f"   ğŸ’¸ äº¤æ˜“æˆæœ¬:")
        print(f"      å›åˆæ•°: {roundtrips:.1f}")
        print(f"      æ¢æ‰‹ç‡: {turnover_rate:.2%}")
        print(f"      å•å›åˆæˆæœ¬: {per_roundtrip_cost:.4f}")
        print(f"      æ€»äº¤æ˜“æˆæœ¬: {total_transaction_cost:.4f}")
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        gross_return = strategy_cumulative[-1] - 1
        net_return = gross_return - total_transaction_cost
        total_return_benchmark = benchmark_cumulative[-1] - 1
        total_return_strategy = net_return  # ä½¿ç”¨æ‰£é™¤æˆæœ¬åçš„å‡€æ”¶ç›Š
        excess_return = total_return_strategy - total_return_benchmark
        
        print(f"   ğŸ“Š æ”¶ç›Šæ‹†è§£:")
        print(f"      æ¯›æ”¶ç›Š: {gross_return:+.4f}")
        print(f"      äº¤æ˜“æˆæœ¬: {total_transaction_cost:.4f}")
        print(f"      å‡€æ”¶ç›Š: {net_return:+.4f}")
        print(f"      æˆæœ¬ä¾µèš€æ¯”ä¾‹: {(total_transaction_cost/abs(gross_return)*100):.1f}%" if gross_return != 0 else "      æˆæœ¬ä¾µèš€æ¯”ä¾‹: N/A")
        
        # å¹´åŒ–æ”¶ç›Šç‡ï¼ˆå‡è®¾250ä¸ªäº¤æ˜“æ—¥ï¼‰
        n_days = len(returns)
        years = n_days / 250
        annual_return_benchmark = (1 + total_return_benchmark) ** (1/years) - 1 if years > 0 else 0
        annual_return_strategy = (1 + total_return_strategy) ** (1/years) - 1 if years > 0 else 0
        
        # æ³¢åŠ¨ç‡
        benchmark_volatility = np.std(benchmark_returns) * np.sqrt(250)
        strategy_volatility = np.std(strategy_returns) * np.sqrt(250)
        
        # å¤æ™®æ¯”ç‡ï¼ˆå‡è®¾æ— é£é™©åˆ©ç‡ä¸º3%ï¼‰
        risk_free_rate = 0.03
        sharpe_benchmark = (annual_return_benchmark - risk_free_rate) / benchmark_volatility if benchmark_volatility > 0 else 0
        sharpe_strategy = (annual_return_strategy - risk_free_rate) / strategy_volatility if strategy_volatility > 0 else 0
        
        # æœ€å¤§å›æ’¤
        benchmark_running_max = np.maximum.accumulate(benchmark_cumulative)
        benchmark_drawdown = (benchmark_cumulative - benchmark_running_max) / benchmark_running_max
        max_drawdown_benchmark = np.min(benchmark_drawdown)
        
        strategy_running_max = np.maximum.accumulate(strategy_cumulative)
        strategy_drawdown = (strategy_cumulative - strategy_running_max) / strategy_running_max
        max_drawdown_strategy = np.min(strategy_drawdown)
        
        # === å…³é”®ä¿®å¤3: èƒœç‡ç»Ÿè®¡ï¼ˆä½¿ç”¨T+1å¯¹é½çš„ä¿¡å·ï¼‰ ===
        signal_mask = signal_t_plus_1 == 1
        win_rate = (returns[signal_mask] > 0).mean() if signal_mask.sum() > 0 else 0
        
        performance = {
            # æ€»æ”¶ç›Š
            'total_return_benchmark': total_return_benchmark,
            'total_return_strategy': total_return_strategy,
            'excess_return': excess_return,
            'gross_return': gross_return,
            
            # å¹´åŒ–æ”¶ç›Š
            'annual_return_benchmark': annual_return_benchmark,
            'annual_return_strategy': annual_return_strategy,
            
            # é£é™©æŒ‡æ ‡
            'volatility_benchmark': benchmark_volatility,
            'volatility_strategy': strategy_volatility,
            'max_drawdown_benchmark': max_drawdown_benchmark,
            'max_drawdown_strategy': max_drawdown_strategy,
            
            # é£é™©è°ƒæ•´æ”¶ç›Š
            'sharpe_benchmark': sharpe_benchmark,
            'sharpe_strategy': sharpe_strategy,
            
            # äº¤æ˜“ç»Ÿè®¡
            'signal_count': signal_mask.sum(),
            'signal_ratio': signal_mask.mean(),
            'win_rate': win_rate,
            'roundtrips': roundtrips,
            'turnover_rate': turnover_rate,
            'transaction_cost': total_transaction_cost,
            
            # æ—¶é—´åºåˆ—
            'benchmark_cumulative': benchmark_cumulative,
            'strategy_cumulative': strategy_cumulative,
            'benchmark_drawdown': benchmark_drawdown,
            'strategy_drawdown': strategy_drawdown,
            'dates': signal_data.index
        }
        
        print(f"   âœ… ç­–ç•¥æ€§èƒ½:")
        print(f"      åŸºå‡†æ€»æ”¶ç›Š: {total_return_benchmark:.2%}")
        print(f"      ç­–ç•¥å‡€æ”¶ç›Š: {total_return_strategy:.2%}")
        print(f"      è¶…é¢æ”¶ç›Š: {excess_return:.2%}")
        print(f"      åŸºå‡†å¤æ™®: {sharpe_benchmark:.3f}")
        print(f"      ç­–ç•¥å¤æ™®: {sharpe_strategy:.3f}")
        print(f"      åŸºå‡†å›æ’¤: {max_drawdown_benchmark:.2%}")
        print(f"      ç­–ç•¥å›æ’¤: {max_drawdown_strategy:.2%}")
        print(f"      ä¿¡å·èƒœç‡: {win_rate:.2%} (T+1å¯¹é½)")
        
        return performance

    def run_random_baseline(self, signal_data: pd.DataFrame, performance: Dict) -> Dict:
        """
        éšæœºåŸºå‡†å¯¹æ¯”ï¼ˆ100æ¬¡éšæœºä¿¡å·ï¼‰
        
        æ”¹è¿›ï¼š
        1. T+1å¯¹é½éšæœºä¿¡å·ï¼Œä¿æŒå…¬å¹³å¯¹æ¯”
        2. åŒ¹é…ç›¸åŒçš„ä¿¡å·æ¯”ä¾‹å’ŒæŒæœ‰æœŸ
        
        Parameters:
        -----------
        signal_data : pd.DataFrame
            åŒ…å«ä¿¡å·çš„æ•°æ®
        performance : Dict
            ç­–ç•¥æ€§èƒ½æŒ‡æ ‡
            
        Returns:
        --------
        dict
            éšæœºåŸºå‡†å¯¹æ¯”ç»“æœ
        """
        print(f"ğŸ² è¿è¡ŒéšæœºåŸºå‡†å¯¹æ¯” ({self.random_simulations}æ¬¡)...")
        
        returns = signal_data['future_return_5d'].fillna(0).values
        original_signal_ratio = performance['signal_ratio']
        
        print(f"   ğŸ“Š åŒ¹é…ç­–ç•¥ä¿¡å·æ¯”ä¾‹: {original_signal_ratio:.2%}")
        print(f"   ğŸ”„ ä½¿ç”¨T+1æ‰§è¡Œï¼ˆä¸ç­–ç•¥ä¸€è‡´ï¼‰")
        
        # è¿è¡Œéšæœºæ¨¡æ‹Ÿ
        random_results = []
        
        for i in range(self.random_simulations):
            # ç”Ÿæˆéšæœºä¿¡å·ï¼ˆä¿æŒç›¸åŒçš„ä¿¡å·æ¯”ä¾‹ï¼‰
            n_samples = len(returns)
            n_signals = int(n_samples * original_signal_ratio)
            
            random_signal = np.zeros(n_samples)
            if n_signals > 0:
                random_indices = np.random.choice(n_samples, n_signals, replace=False)
                random_signal[random_indices] = 1
            
            # T+1å¯¹é½ï¼ˆä¸ç­–ç•¥ä¿æŒä¸€è‡´ï¼‰
            random_signal_t1 = np.roll(random_signal, 1)
            random_signal_t1[0] = 0
            
            # è®¡ç®—éšæœºç­–ç•¥æ”¶ç›Š
            random_strategy_returns = random_signal_t1 * returns
            random_cumulative_returns = []
            cumulative = 1.0
            
            for j in range(len(random_strategy_returns)):
                if random_signal_t1[j] == 1:
                    cumulative *= (1 + returns[j])
                random_cumulative_returns.append(cumulative)
            
            total_random_return = cumulative - 1
            
            # è®¡ç®—éšæœºç­–ç•¥ç»Ÿè®¡
            random_volatility = np.std(random_strategy_returns) * np.sqrt(250)
            
            random_results.append({
                'total_return': total_random_return,
                'volatility': random_volatility,
                'signal_count': n_signals
            })
        
        # ç»Ÿè®¡éšæœºç»“æœ
        random_returns = np.array([r['total_return'] for r in random_results])
        random_volatilities = np.array([r['volatility'] for r in random_results])
        
        strategy_return = performance['total_return_strategy']
        
        baseline_comparison = {
            'random_mean_return': random_returns.mean(),
            'random_std_return': random_returns.std(),
            'random_min_return': random_returns.min(),
            'random_max_return': random_returns.max(),
            'random_median_return': np.median(random_returns),
            
            'random_mean_volatility': random_volatilities.mean(),
            'random_std_volatility': random_volatilities.std(),
            
            'strategy_return': strategy_return,
            'strategy_percentile': (random_returns < strategy_return).mean(),
            'outperformance_ratio': (random_returns < strategy_return).mean(),
            
            'n_simulations': self.random_simulations,
            'random_returns': random_returns
        }
        
        print(f"   âœ… éšæœºåŸºå‡†å¯¹æ¯”:")
        print(f"      ç­–ç•¥æ”¶ç›Š: {strategy_return:.2%}")
        print(f"      éšæœºå¹³å‡: {baseline_comparison['random_mean_return']:.2%}")
        print(f"      éšæœºæ ‡å‡†å·®: {baseline_comparison['random_std_return']:.2%}")
        print(f"      ç­–ç•¥åˆ†ä½æ•°: {baseline_comparison['strategy_percentile']:.1%}")
        print(f"      ä¼˜äºéšæœºæ¯”ä¾‹: {baseline_comparison['outperformance_ratio']:.1%}")
        
        return baseline_comparison

    def save_strategy_equity(self, performance: Dict, baseline_comparison: Dict, 
                           selected_clusters: List[Dict], symbol: str = "000001") -> str:
        """
        ä¿å­˜ç­–ç•¥æƒç›Šæ›²çº¿å’Œåˆ†æç»“æœ
        
        Parameters:
        -----------
        performance : Dict
            ç­–ç•¥æ€§èƒ½æŒ‡æ ‡
        baseline_comparison : Dict
            éšæœºåŸºå‡†å¯¹æ¯”ç»“æœ
        selected_clusters : List[Dict]
            é€‰ä¸­çš„èšç±»ä¿¡æ¯
        symbol : str
            è‚¡ç¥¨ä»£ç 
            
        Returns:
        --------
        str
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        print(f"ğŸ’¾ ä¿å­˜ç­–ç•¥æƒç›Šæ›²çº¿å’Œåˆ†æç»“æœ...")
        
        # åˆ›å»ºæƒç›Šæ›²çº¿æ•°æ®
        equity_data = []
        dates = performance['dates']
        benchmark_cumulative = performance['benchmark_cumulative']
        strategy_cumulative = performance['strategy_cumulative']
        benchmark_drawdown = performance['benchmark_drawdown']
        strategy_drawdown = performance['strategy_drawdown']
        
        for i, date in enumerate(dates):
            equity_data.append({
                'date': date,
                'benchmark_equity': benchmark_cumulative[i],
                'strategy_equity': strategy_cumulative[i],
                'benchmark_drawdown': benchmark_drawdown[i],
                'strategy_drawdown': strategy_drawdown[i],
                'excess_equity': strategy_cumulative[i] - benchmark_cumulative[i]
            })
        
        equity_df = pd.DataFrame(equity_data)
        
        # ä¿å­˜æƒç›Šæ›²çº¿CSV
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        equity_file = os.path.join(self.reports_dir, f"strategy_equity_{symbol}_{timestamp}.csv")
        equity_df.to_csv(equity_file, index=False)
        
        print(f"   âœ… æƒç›Šæ›²çº¿å·²ä¿å­˜: {os.path.basename(equity_file)}")
        
        # åˆ›å»ºè¯¦ç»†åˆ†ææŠ¥å‘Š
        report_lines = []
        report_lines.append("=" * 70)
        report_lines.append("é˜¶æ®µ11ï¼šç­–ç•¥ä¿¡å·ä¸å›æµ‹é›å½¢ - åˆ†ææŠ¥å‘Š")
        report_lines.append("=" * 70)
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"è‚¡ç¥¨ä»£ç : {symbol}")
        report_lines.append(f"æµ‹è¯•æœŸé—´: {self.test_start_date} ~ {self.test_end_date}")
        report_lines.append("")
        
        # é€‰ä¸­çš„èšç±»ä¿¡æ¯
        report_lines.append("ğŸ¯ é€‰ä¸­çš„æœ€ä½³èšç±»:")
        for i, cluster_info in enumerate(selected_clusters):
            report_lines.append(f"   èšç±» {i+1}: k={cluster_info['k_value']}, cluster_id={cluster_info['cluster_id']}")
            report_lines.append(f"      è®­ç»ƒæ”¶ç›Š: {cluster_info['train_mean_return']:+.6f} (æ’å: {cluster_info['train_rank']})")
            report_lines.append(f"      æµ‹è¯•æ”¶ç›Š: {cluster_info['test_mean_return']:+.6f} (æ’å: {cluster_info['test_rank']})")
            report_lines.append(f"      å…¨å±€æ’å: {cluster_info['global_rank']}")
        report_lines.append("")
        
        # ç­–ç•¥æ€§èƒ½
        report_lines.append("ğŸ“Š ç­–ç•¥æ€§èƒ½:")
        report_lines.append(f"   åŸºå‡†æ€»æ”¶ç›Š: {performance['total_return_benchmark']:+.2%}")
        report_lines.append(f"   ç­–ç•¥æ¯›æ”¶ç›Š: {performance['gross_return']:+.2%}")
        report_lines.append(f"   äº¤æ˜“æˆæœ¬: {performance['transaction_cost']:.4f}")
        report_lines.append(f"   ç­–ç•¥å‡€æ”¶ç›Š: {performance['total_return_strategy']:+.2%}")
        report_lines.append(f"   è¶…é¢æ”¶ç›Š: {performance['excess_return']:+.2%}")
        report_lines.append("")
        report_lines.append(f"   åŸºå‡†å¹´åŒ–æ”¶ç›Š: {performance['annual_return_benchmark']:+.2%}")
        report_lines.append(f"   ç­–ç•¥å¹´åŒ–æ”¶ç›Š: {performance['annual_return_strategy']:+.2%}")
        report_lines.append("")
        report_lines.append(f"   åŸºå‡†æ³¢åŠ¨ç‡: {performance['volatility_benchmark']:.2%}")
        report_lines.append(f"   ç­–ç•¥æ³¢åŠ¨ç‡: {performance['volatility_strategy']:.2%}")
        report_lines.append("")
        report_lines.append(f"   åŸºå‡†å¤æ™®æ¯”ç‡: {performance['sharpe_benchmark']:.3f}")
        report_lines.append(f"   ç­–ç•¥å¤æ™®æ¯”ç‡: {performance['sharpe_strategy']:.3f}")
        report_lines.append("")
        report_lines.append(f"   åŸºå‡†æœ€å¤§å›æ’¤: {performance['max_drawdown_benchmark']:+.2%}")
        report_lines.append(f"   ç­–ç•¥æœ€å¤§å›æ’¤: {performance['max_drawdown_strategy']:+.2%}")
        report_lines.append("")
        report_lines.append(f"   ä¿¡å·æ•°é‡: {performance['signal_count']} (T+1å¯¹é½)")
        report_lines.append(f"   ä¿¡å·æ¯”ä¾‹: {performance['signal_ratio']:.2%}")
        report_lines.append(f"   ä¿¡å·èƒœç‡: {performance['win_rate']:.2%} (T+1å¯¹é½)")
        report_lines.append(f"   å›åˆæ•°: {performance['roundtrips']:.1f}")
        report_lines.append(f"   æ¢æ‰‹ç‡: {performance['turnover_rate']:.2%}")
        report_lines.append("")
        
        # éšæœºåŸºå‡†å¯¹æ¯”
        report_lines.append("ğŸ² éšæœºåŸºå‡†å¯¹æ¯”:")
        report_lines.append(f"   æ¨¡æ‹Ÿæ¬¡æ•°: {baseline_comparison['n_simulations']}")
        report_lines.append(f"   ç­–ç•¥æ”¶ç›Š: {baseline_comparison['strategy_return']:+.2%}")
        report_lines.append(f"   éšæœºå¹³å‡æ”¶ç›Š: {baseline_comparison['random_mean_return']:+.2%}")
        report_lines.append(f"   éšæœºæ”¶ç›Šæ ‡å‡†å·®: {baseline_comparison['random_std_return']:.2%}")
        report_lines.append(f"   éšæœºæ”¶ç›ŠèŒƒå›´: {baseline_comparison['random_min_return']:+.2%} ~ {baseline_comparison['random_max_return']:+.2%}")
        report_lines.append(f"   ç­–ç•¥åˆ†ä½æ•°: {baseline_comparison['strategy_percentile']:.1%}")
        report_lines.append(f"   ä¼˜äºéšæœºæ¯”ä¾‹: {baseline_comparison['outperformance_ratio']:.1%}")
        report_lines.append("")
        
        # éªŒæ”¶ç»“æœ
        report_lines.append("âœ… éªŒæ”¶ç»“æœ:")
        
        # æ£€æŸ¥ç­–ç•¥æ”¶ç›Šä¸å°äºåŸºå‡†
        benchmark_check = performance['total_return_strategy'] >= performance['total_return_benchmark']
        report_lines.append(f"   ç­–ç•¥æ”¶ç›Šä¸å°äºåŸºå‡†: {'âœ… é€šè¿‡' if benchmark_check else 'âŒ æœªé€šè¿‡'}")
        report_lines.append(f"      ç­–ç•¥ {performance['total_return_strategy']:+.2%} vs åŸºå‡† {performance['total_return_benchmark']:+.2%}")
        
        # æ£€æŸ¥å›æ’¤ä¸è¿‡åˆ†æ”¾å¤§
        drawdown_check = abs(performance['max_drawdown_strategy']) <= abs(performance['max_drawdown_benchmark']) * 1.5
        report_lines.append(f"   å›æ’¤ä¸è¿‡åˆ†æ”¾å¤§: {'âœ… é€šè¿‡' if drawdown_check else 'âŒ æœªé€šè¿‡'}")
        report_lines.append(f"      ç­–ç•¥å›æ’¤ {performance['max_drawdown_strategy']:+.2%} vs åŸºå‡†å›æ’¤ {performance['max_drawdown_benchmark']:+.2%}")
        
        # æ£€æŸ¥éšæœºåŸºå‡†ä¼˜åŠ¿
        random_check = baseline_comparison['strategy_percentile'] >= 0.6
        report_lines.append(f"   ä¼˜äºéšæœºåŸºå‡†: {'âœ… é€šè¿‡' if random_check else 'âŒ æœªé€šè¿‡'}")
        report_lines.append(f"      ç­–ç•¥åœ¨éšæœºåŸºå‡†ä¸­æ’å {baseline_comparison['strategy_percentile']:.1%} åˆ†ä½")
        
        overall_pass = benchmark_check and drawdown_check and random_check
        report_lines.append("")
        report_lines.append(f"ğŸ† æ€»ä½“éªŒæ”¶: {'âœ… é€šè¿‡' if overall_pass else 'âŒ æœªé€šè¿‡'}")
        
        report_lines.append("")
        report_lines.append("=" * 70)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = os.path.join(self.reports_dir, f"strategy_analysis_{symbol}_{timestamp}.txt")
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("\n".join(report_lines))
        
        print(f"   ğŸ“‹ åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {os.path.basename(report_file)}")
        
        return equity_file

    def run_complete_backtest(self, symbol: str = "000001", top_n: int = 2) -> Dict:
        """
        è¿è¡Œå®Œæ•´çš„ç­–ç•¥å›æµ‹æµç¨‹
        
        Parameters:
        -----------
        symbol : str, default="000001"
            è‚¡ç¥¨ä»£ç 
        top_n : int, default=2
            é€‰æ‹©top Nä¸ªèšç±»
            
        Returns:
        --------
        dict
            å®Œæ•´çš„å›æµ‹ç»“æœ
        """
        print("=" * 70)
        print("é˜¶æ®µ11ï¼šç­–ç•¥ä¿¡å·ä¸å›æµ‹é›å½¢")
        print("=" * 70)
        print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"è‚¡ç¥¨ä»£ç : {symbol}")
        print(f"æµ‹è¯•æœŸé—´: {self.test_start_date} ~ {self.test_end_date}")
        print(f"é€‰æ‹©ç­–ç•¥: top{top_n} è®­ç»ƒæ”¶ç›Šèšç±»")
        print()
        
        try:
            # 1. åŠ è½½èšç±»è¯„ä¼°ç»“æœ
            cluster_results = self.load_cluster_evaluation_results()
            
            # 2. é€‰æ‹©è®­ç»ƒé›†æ”¶ç›Šæœ€é«˜çš„èšç±»
            selection_results = self.select_best_clusters(cluster_results['comparison_df'], top_n)
            selected_clusters = selection_results['selected_clusters']
            
            # 4. å‡†å¤‡æµ‹è¯•æ•°æ®ï¼ˆä½¿ç”¨InfluxDBæ–°æ•°æ® + pca_stateå®Œæ•´æµç¨‹ï¼‰
            test_data = self.prepare_test_data(symbol)
            
            # 5. ç”Ÿæˆäº¤æ˜“ä¿¡å·
            signal_data = self.generate_trading_signals(test_data, selected_clusters)
            
            # 6. è®¡ç®—ç­–ç•¥æ€§èƒ½
            performance = self.calculate_strategy_performance(signal_data)
            
            # 7. éšæœºåŸºå‡†å¯¹æ¯”
            baseline_comparison = self.run_random_baseline(signal_data, performance)
            
            # 8. ä¿å­˜æƒç›Šæ›²çº¿å’Œåˆ†æç»“æœ
            equity_file = self.save_strategy_equity(performance, baseline_comparison, selected_clusters, symbol)
            
            # 9. æ•´åˆå®Œæ•´ç»“æœ
            results = {
                'symbol': symbol,
                'test_period': f"{self.test_start_date} ~ {self.test_end_date}",
                'selected_clusters': selected_clusters,
                'performance': performance,
                'baseline_comparison': baseline_comparison,
                'signal_data': signal_data,
                'equity_file': equity_file,
                'backtest_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            # 10. æœ€ç»ˆæŠ¥å‘Š
            print("\n" + "=" * 70)
            print("ğŸ‰ é˜¶æ®µ11å®Œæˆï¼")
            print("=" * 70)
            
            # éªŒæ”¶æ£€æŸ¥
            benchmark_check = performance['total_return_strategy'] >= performance['total_return_benchmark']
            drawdown_check = abs(performance['max_drawdown_strategy']) <= abs(performance['max_drawdown_benchmark']) * 1.5
            random_check = baseline_comparison['strategy_percentile'] >= 0.6
            overall_pass = benchmark_check and drawdown_check and random_check
            
            print(f"ğŸ“Š ç­–ç•¥æ”¶ç›Š: {performance['total_return_strategy']:+.2%} (åŸºå‡†: {performance['total_return_benchmark']:+.2%})")
            print(f"ğŸ“‰ æœ€å¤§å›æ’¤: {performance['max_drawdown_strategy']:+.2%} (åŸºå‡†: {performance['max_drawdown_benchmark']:+.2%})")
            print(f"ğŸ² éšæœºæ’å: {baseline_comparison['strategy_percentile']:.1%} åˆ†ä½")
            print(f"ğŸ† éªŒæ”¶ç»“æœ: {'âœ… é€šè¿‡' if overall_pass else 'âŒ æœªé€šè¿‡'}")
            print(f"ğŸ’¾ æƒç›Šæ›²çº¿: {os.path.basename(equity_file)}")
            print("=" * 70)
            
            return results
            
        except Exception as e:
            print(f"\nâŒ å›æµ‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise


def main():
    """
    ä¸»å‡½æ•°ï¼šè¿è¡Œé˜¶æ®µ11ç­–ç•¥ä¿¡å·ä¸å›æµ‹é›å½¢
    """
    try:
        # åˆå§‹åŒ–ç­–ç•¥å›æµ‹å™¨
        backtest = StrategyBacktest()
        
        # è¿è¡Œå®Œæ•´å›æµ‹
        results = backtest.run_complete_backtest(
            symbol="000001",  # å¹³å®‰é“¶è¡Œ
            top_n=3          # é€‰æ‹©top3èšç±»
        )
        
        print(f"\nâœ¨ å›æµ‹å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {backtest.reports_dir}")
        return results
        
    except Exception as e:
        print(f"\nğŸ’¥ ä¸»å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    main()