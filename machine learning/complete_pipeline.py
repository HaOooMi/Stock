#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´æ•°æ®ç®¡é“ - ç‰¹å¾å·¥ç¨‹ + ç›®æ ‡å·¥ç¨‹é›†æˆ

åŠŸèƒ½ï¼š
1. ä»InfluxDBåŠ è½½è‚¡ç¥¨æ•°æ®
2. ç”ŸæˆæŠ€æœ¯ç‰¹å¾
3. ç‰¹å¾é€‰æ‹©å’Œæ ‡å‡†åŒ–
4. ç”Ÿæˆç›®æ ‡å˜é‡å’Œæ ‡ç­¾
5. ä¿å­˜å®Œæ•´æ•°æ®é›†

"""

import os
import sys
from datetime import datetime

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

from feature_engineering import FeatureEngineer
from target_engineering import TargetEngineer


def create_complete_dataset(symbol: str, start_date: str, end_date: str,
                           use_auto_features: bool = False,
                           final_k_features: int = 20,
                           target_periods: list = [1, 5, 10],
                           include_scaling: bool = True):
    """
    åˆ›å»ºå®Œæ•´çš„æœºå™¨å­¦ä¹ æ•°æ®é›†
    
    Parameters:
    -----------
    symbol : str
        è‚¡ç¥¨ä»£ç 
    start_date : str  
        å¼€å§‹æ—¥æœŸ
    end_date : str
        ç»“æŸæ—¥æœŸ
    use_auto_features : bool, default=False
        æ˜¯å¦ä½¿ç”¨è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆ
    final_k_features : int, default=20
        æœ€ç»ˆä¿ç•™çš„ç‰¹å¾æ•°é‡
    target_periods : list, default=[1, 5, 10]
        ç›®æ ‡å˜é‡çš„æ—¶é—´çª—å£
    include_scaling : bool, default=True
        æ˜¯å¦åŒ…å«ç‰¹å¾æ ‡å‡†åŒ–
    
    Returns:
    --------
    tuple: (å®Œæ•´æ•°æ®é›†DataFrame, ç‰¹å¾å·¥ç¨‹ç»“æœ, ç›®æ ‡å·¥ç¨‹å™¨, ä¿å­˜è·¯å¾„)
    """
    
    # å¼€å§‹æ•°æ®ç®¡é“
    print("=" * 60)
    
    # ===== é˜¶æ®µ1: ç‰¹å¾å·¥ç¨‹ =====
    print("ğŸ“Š é˜¶æ®µ1: ç‰¹å¾å·¥ç¨‹")
    print("-" * 30)
    
    # åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹å™¨
    feature_engineer = FeatureEngineer(use_talib=True, use_tsfresh=use_auto_features)
    
    # åŠ è½½æ•°æ®
    print(f"ğŸ“ˆ åŠ è½½è‚¡ç¥¨æ•°æ®: {symbol} ({start_date} ~ {end_date})")
    raw_data = feature_engineer.load_stock_data(symbol, start_date, end_date)
    
    if len(raw_data) < 100:
        raise ValueError(f"æ•°æ®é‡å¤ªå°‘({len(raw_data)}è¡Œ)ï¼Œå»ºè®®è‡³å°‘100è¡Œæ•°æ®")
    
    # ç”Ÿæˆç‰¹å¾
    print("ğŸ­ ç”ŸæˆæŠ€æœ¯ç‰¹å¾...")
    features_df = feature_engineer.prepare_features(
        raw_data,
        use_auto_features=use_auto_features,
        window_size=20,
        max_auto_features=30
    )
    
    # ç‰¹å¾é€‰æ‹©
    # æ‰§è¡Œç‰¹å¾é€‰æ‹©
    selection_results = feature_engineer.select_features(
        features_df,
        final_k=final_k_features,
        variance_threshold=0.01,
        correlation_threshold=0.9,
        train_ratio=0.8
    )
    
    final_features_df = selection_results['final_features_df']
    
    # ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆå¯é€‰ï¼‰
    if include_scaling:
        # æ‰§è¡Œç‰¹å¾æ ‡å‡†åŒ–
        scale_results = feature_engineer.scale_features(
            final_features_df,
            scaler_type='robust',
            train_ratio=0.8,
            save_path=f'machine learning/ML output/scaler_{symbol}.pkl'
        )
        scaled_features_df = scale_results['scaled_df']
        print(f"   âœ… ç¼©æ”¾å™¨å·²ä¿å­˜: {scale_results['scaler_path']}")
        if scale_results.get('csv_path'):
            print(f"   ğŸ“Š æ ‡å‡†åŒ–ç‰¹å¾å·²ä¿å­˜: {scale_results['csv_path']}")
    else:
        scaled_features_df = final_features_df
    
    # ç‰¹å¾åˆ†æ
    print("ğŸ“Š åˆ†æç‰¹å¾è´¨é‡...")
    analysis_results = feature_engineer.analyze_features(scaled_features_df)
    
    # ===== é˜¶æ®µ2: ç›®æ ‡å·¥ç¨‹ =====
    # é˜¶æ®µ2: ç›®æ ‡å·¥ç¨‹  
    print("-" * 30)
    
    # åˆå§‹åŒ–ç›®æ ‡å·¥ç¨‹å™¨
    target_engineer = TargetEngineer()
    
    # åˆ›å»ºå®Œæ•´æ•°æ®é›†ï¼ˆç‰¹å¾ + ç›®æ ‡ï¼‰
    print("ğŸ”¨ åˆ›å»ºå®Œæ•´æ•°æ®é›†...")
    complete_dataset = target_engineer.create_complete_dataset(
        scaled_features_df,
        periods=target_periods,
        price_col='close',
        include_labels=True,
        label_types=['binary', 'quantile']
    )
    
    # ===== é˜¶æ®µ3: ä¿å­˜æ•°æ® =====
    print("\nğŸ’¾ é˜¶æ®µ3: ä¿å­˜æ•°æ®")
    print("-" * 30)
    
    # æ·»åŠ æ—¶é—´æˆ³åç¼€
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    suffix = f"features{final_k_features}_{timestamp}"
    
    # ä¿å­˜å®Œæ•´æ•°æ®é›†
    save_path = target_engineer.save_dataset(complete_dataset, symbol, suffix)
    
    # ===== æœ€ç»ˆæ€»ç»“ =====
    print("\n" + "=" * 60)
    print("Data pipeline completed")
    print(f"   ğŸ“Š åŸå§‹æ•°æ®: {len(raw_data)} è¡Œ")
    print(f"   ğŸ­ ç”Ÿæˆç‰¹å¾: {len(features_df.columns)-1} ä¸ª")
    print(f"   Features selected: {len(selection_results['final_features'])}")
    print(f"   Scaling: {'Yes' if include_scaling else 'No'}")
    print(f"   Target window: {target_periods} days")
    print(f"   ğŸ’¾ ä¿å­˜è·¯å¾„: {save_path}")
    
    # æ•°æ®å¯ç”¨æ€§æ£€æŸ¥
    max_period = max(target_periods)
    total_samples = len(complete_dataset)
    trainable_samples = total_samples - max_period
    print(f"\nğŸ“Š æ•°æ®å¯ç”¨æ€§:")
    print(f"   ğŸ”¢ æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"   ğŸ“ å¯è®­ç»ƒæ ·æœ¬: {trainable_samples} (æ’é™¤å°¾éƒ¨{max_period}è¡ŒNaN)")
    print(f"   âš ï¸ æ³¨æ„: å°¾éƒ¨{max_period}è¡Œç›®æ ‡ä¸ºNaNï¼Œä¸å‚ä¸è®­ç»ƒ")
    
    return complete_dataset, selection_results, target_engineer, save_path


def main():
    """
    ä¸»å‡½æ•° - æ‰§è¡Œå®Œæ•´çš„æ•°æ®ç®¡é“
    """
    print("Stock ML Data Pipeline")
    print("=" * 60)
    
    try:
        # é…ç½®å‚æ•°
        config = {
            'symbol': '000001',  # å¹³å®‰é“¶è¡Œ
            'start_date': '2023-01-01',
            'end_date': '2024-12-31', 
            'use_auto_features': False,  # æ˜¯å¦ä½¿ç”¨è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆ
            'final_k_features': 15,      # æœ€ç»ˆç‰¹å¾æ•°é‡
            'target_periods': [1, 5, 10], # ç›®æ ‡æ—¶é—´çª—å£
            'include_scaling': True       # æ˜¯å¦æ ‡å‡†åŒ–
        }
        
        print("Configuration:")
        for key, value in config.items():
            print(f"   {key}: {value}")
        print()
        
        # æ‰§è¡Œå®Œæ•´ç®¡é“
        dataset, features_info, target_eng, save_path = create_complete_dataset(**config)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœé¢„è§ˆ
        print("\nğŸ“„ æœ€ç»ˆæ•°æ®é›†é¢„è§ˆ:")
        print("-" * 40)
        
        # æ˜¾ç¤ºåˆ—ä¿¡æ¯
        feature_cols = [col for col in dataset.columns 
                       if not col.startswith(('future_return_', 'label_'))]
        target_cols = [col for col in dataset.columns if col.startswith('future_return_')]
        label_cols = [col for col in dataset.columns if col.startswith('label_')]
        
        print(f"ç‰¹å¾åˆ— ({len(feature_cols)}ä¸ª): {feature_cols[:5]}...")
        print(f"ç›®æ ‡åˆ— ({len(target_cols)}ä¸ª): {target_cols}")
        print(f"æ ‡ç­¾åˆ— ({len(label_cols)}ä¸ª): {label_cols}")
        
        # æ˜¾ç¤ºæ•°æ®æ ·æœ¬
        print("\nå‰5è¡Œæ•°æ®æ ·æœ¬:")
        print(dataset.head()[['close'] + target_cols[:2]].round(4))
        
        print("\nå°¾éƒ¨5è¡Œæ•°æ®æ ·æœ¬ (éªŒè¯NaN):")
        print(dataset.tail()[['close'] + target_cols[:2]].round(4))
        
        print(f"\nâœ… æ•°æ®ç®¡é“æ‰§è¡ŒæˆåŠŸï¼")
        print(f"ğŸ’¾ å®Œæ•´æ•°æ®é›†å·²ä¿å­˜åˆ°: {save_path}")
        print(f"ğŸ¯ å¯ç”¨äºæœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯")
        
        return dataset, save_path
        
    except Exception as e:
        print(f"âŒ æ•°æ®ç®¡é“æ‰§è¡Œå¤±è´¥: {str(e)}")
        raise


if __name__ == "__main__":
    # æ‰§è¡Œå®Œæ•´æ•°æ®ç®¡é“
    main()