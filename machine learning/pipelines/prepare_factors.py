#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å› å­å‡†å¤‡ç®¡é“ - å› å­å·¥å‚å®Œæ•´æµç¨‹

æœ¬ç®¡é“å®Œå…¨åŸºäºå·²æœ‰çš„æ¨ªæˆªé¢è¯„ä¼°æ¡†æ¶ (evaluation/) æ„å»ºï¼Œ
å®ç°ä»å› å­ç”Ÿæˆåˆ°è´¨é‡æ£€æŸ¥ã€æŠ¥å‘Šè¾“å‡ºçš„å…¨æµç¨‹è‡ªåŠ¨åŒ–ã€‚

æµç¨‹ï¼š
1. åŠ è½½å¸‚åœºæ•°æ®ï¼ˆDataLoaderï¼‰
   - ä»InfluxDBåŠ è½½OHLCVæ•°æ®
   - åº”ç”¨å¯äº¤æ˜“æ€§è¿‡æ»¤
   
2. ç”Ÿæˆå› å­ï¼ˆFactorFactoryï¼‰
   - åŠ¨é‡æ—: ROC_N, Price_to_SMA, RankMomentum
   - æ³¢åŠ¨ç‡æ—: RealizedVol, Parkinson, Skewness
   - é‡ä»·æ—: Turnover, VolumePriceCorr, VWAP_Dev
   
3. æ¨ªæˆªé¢è´¨é‡æ£€æŸ¥ â­æ ¸å¿ƒæ­¥éª¤
   ä½¿ç”¨ evaluation/CrossSectionAnalyzer è¿›è¡Œå®Œæ•´è¯„ä¼°ï¼š
   - calculate_forward_returns: è®¡ç®—è¿œæœŸæ”¶ç›Š
   - preprocess: Winsorize + æ ‡å‡†åŒ– + ä¸­æ€§åŒ–
   - analyze: è®¡ç®—IC/ICIR/Spread/å•è°ƒæ€§
   - è¯„ä¼°æ ‡å‡†: ICâ‰¥0.02, ICIRâ‰¥0.5, Spread>0
   
4. å› å­å…¥åº“ï¼ˆFactorLibraryManagerï¼‰
   - åªæœ‰é€šè¿‡æ¨ªæˆªé¢æ£€æŸ¥çš„å› å­æ‰å…¥åº“
   - ä¿å­˜è´¨é‡æŠ¥å‘Šå’Œå…ƒæ•°æ®
   
5. ç”ŸæˆæŠ¥å‘Š â­è¾“å‡ºæ ‡å‡†åŒ–
   ä½¿ç”¨ evaluation/tearsheet ç”ŸæˆæŠ¥å‘Šï¼š
   - HTML tearsheet: tearsheet_{factor}_{period}.html
   - ICåºåˆ—CSV: ic_{factor}_{period}.csv
   - åˆ†ä½æ•°æ”¶ç›ŠCSV: quantile_returns_{factor}_{period}.csv
   - è‡ªåŠ¨ç”Ÿæˆæ‰€æœ‰å›¾è¡¨ (ICèµ°å»Šå›¾ã€ç´¯è®¡æ”¶ç›Šã€Spreadç­‰)

è¾“å‡ºç›®å½•ç»“æ„:
/ML output/reports/baseline_v1/factors/
  â”œâ”€â”€ tearsheet_ROC_20_5d.html
  â”œâ”€â”€ ic_ROC_20_5d.csv
  â””â”€â”€ quantile_returns_ROC_20_5d.csv
/ML output/figures/baseline_v1/factors/
  â”œâ”€â”€ ic_series_ROC_20_5d.png
  â”œâ”€â”€ quantile_cumret_ROC_20_5d.png
  â””â”€â”€ spread_cumret_ROC_20_5d.png
/ML output/datasets/baseline_v1/
  â””â”€â”€ qualified_factors_YYYYMMDD.parquet

éªŒæ”¶æ ‡å‡†:
- â‰¥10ä¸ªç¨³å®šå› å­é€šè¿‡æ£€æŸ¥
- æ¨ªæˆªé¢Rank ICæ˜¾è‘— (IC>0.02, p<0.05)
- åˆå…¥åç»„åˆICæœ‰å®è´¨æå‡ (>0.03)

è¯¦ç»†æ–‡æ¡£: pipelines/README_PREPARE_FACTORS.md
"""

import os
import sys
import yaml
import json
import pandas as pd
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
ml_root = os.path.dirname(current_dir)
if ml_root not in sys.path:
    sys.path.insert(0, ml_root)

from features.factor_factory import FactorFactory
from features.factor_library_manager import FactorLibraryManager
from data.data_loader import DataLoader
from data.tradability_filter import TradabilityFilter
from data.financial_data_loader import FinancialDataLoader
from data.data_snapshot import DataSnapshot  # æ•°æ®å¿«ç…§ç®¡ç†
# ä½¿ç”¨ä½ å·²æœ‰çš„æ¨ªæˆªé¢è¯„ä¼°æ¡†æ¶ï¼
from evaluation.cross_section_analyzer import CrossSectionAnalyzer
from evaluation.cross_section_metrics import calculate_forward_returns
from evaluation.factor_preprocessing import preprocess_factor_pipeline
from evaluation.tearsheet import generate_html_tearsheet
from evaluation.visualization import (  # å›¾è¡¨ç”Ÿæˆ
    plot_ic_time_series,
    plot_ic_distribution,
    plot_quantile_cumulative_returns,
    plot_quantile_mean_returns,
    plot_spread_cumulative_returns,
    plot_monthly_ic_heatmap,
    plot_turnover_time_series,
    create_factor_tearsheet_plots
)


def load_config(config_path: str = "configs/ml_baseline.yml") -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if not os.path.isabs(config_path):
        config_path = os.path.join(ml_root, config_path)
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    return config


def prepare_factors(config_path: str = "configs/ml_baseline.yml",
                   start_date: str = None,
                   end_date: str = None,
                   tickers: list = None):
    """
    å› å­å‡†å¤‡å®Œæ•´æµç¨‹
    
    Parameters:
    -----------
    config_path : str
        é…ç½®æ–‡ä»¶è·¯å¾„
    start_date : str
        å¼€å§‹æ—¥æœŸ
    end_date : str
        ç»“æŸæ—¥æœŸ
    tickers : list, optional
        è‚¡ç¥¨åˆ—è¡¨ï¼ˆNoneè¡¨ç¤ºå…¨å¸‚åœºï¼‰
    """
    print("=" * 80)
    print("å› å­å·¥å‚ v1 - å®Œæ•´æµç¨‹")
    print("=" * 80)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"æ•°æ®åŒºé—´: {start_date} ~ {end_date}")
    print(f"è‚¡ç¥¨èŒƒå›´: {tickers if tickers else 'å…¨å¸‚åœº'}")
    print()
    
    # 1. åŠ è½½é…ç½®
    print("\n" + "=" * 80)
    print("æ­¥éª¤ 1: åŠ è½½é…ç½®")
    print("=" * 80)
    config = load_config(config_path)
    
    # ä»é…ç½®ä¸­æå–å‚æ•°
    influxdb_config = config['data']['influxdb']
    target_config = config.get('target', {})  # æ³¨æ„ï¼šé…ç½®æ–‡ä»¶ç”¨çš„æ˜¯ 'target' å•æ•°
    tradability_config = config['data'].get('universe', {})  # ä½¿ç”¨ 'universe' è€Œä¸æ˜¯ 'tradability_filter'
    
    # å¦‚æœæ²¡æœ‰ä¼ å…¥æ—¥æœŸå‚æ•°ï¼Œä»é…ç½®æ–‡ä»¶è¯»å–
    if start_date is None:
        start_date = config['data'].get('start_date', '2018-01-01')
    if end_date is None:
        end_date = config['data'].get('end_date', '2024-12-31')
    
    # å¦‚æœæ²¡æœ‰ä¼ å…¥è‚¡ç¥¨åˆ—è¡¨ï¼Œä»é…ç½®æ–‡ä»¶è¯»å–
    # æ³¨æ„ï¼šInfluxDB ä¸­å­˜å‚¨çš„è‚¡ç¥¨ä»£ç æ˜¯çº¯æ•°å­—æ ¼å¼ï¼ˆå¦‚ '000001'ï¼‰ï¼Œä¸å¸¦åç¼€
    if tickers is None:
        tickers = config['data'].get('symbol', None)
        if isinstance(tickers, str):
            tickers = [tickers]
    
    # è®¾ç½®é»˜è®¤å€¼
    if 'type' not in target_config:
        target_config['type'] = 'forward_return'
    if 'horizon' not in target_config:
        target_config['horizon'] = target_config.get('forward_periods', 5)
    
    print(f"âœ… é…ç½®åŠ è½½å®Œæˆ")
    print(f"   InfluxDB: {influxdb_config['url']}")
    print(f"   é¢„æµ‹ç›®æ ‡: {target_config.get('name', 'future_return_5d')} ({target_config['horizon']}æ—¥)")
    print(f"   æ—¥æœŸèŒƒå›´: {start_date} ~ {end_date}")
    print(f"   è‚¡ç¥¨ä»£ç : {tickers}")
    
    # 2. åŠ è½½æ•°æ®ï¼ˆä½¿ç”¨MarketDataLoaderæ‰¹é‡åŠ è½½ï¼‰
    print("\n" + "=" * 80)
    print("æ­¥éª¤ 2: åŠ è½½å¸‚åœºæ•°æ®")
    print("=" * 80)
    
    # ä½¿ç”¨MarketDataLoaderæ‰¹é‡åŠ è½½å¤šè‚¡ç¥¨æ•°æ®
    from data.market_data_loader import MarketDataLoader
    
    market_loader = MarketDataLoader(
        url=influxdb_config['url'],
        token=influxdb_config['token'],
        org=influxdb_config['org'],
        bucket=influxdb_config['bucket']
    )
    
    # å¦‚æœæœªæŒ‡å®štickersï¼Œä»é…ç½®æ–‡ä»¶è·å–è‚¡ç¥¨æ± 
    if not tickers:
        tickers = config['data'].get('symbol', None)
        if isinstance(tickers, str):
            tickers = [tickers]
        if not tickers:
            print(f"\nâš ï¸  æœªæŒ‡å®šè‚¡ç¥¨åˆ—è¡¨ï¼Œè¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½® data.symbol")
            raise ValueError("å¿…é¡»åœ¨é…ç½®æ–‡ä»¶ä¸­æä¾› data.symbol å‚æ•°")
        print(f"   ğŸ“‹ ä»é…ç½®æ–‡ä»¶åŠ è½½è‚¡ç¥¨æ± : {len(tickers)} åªè‚¡ç¥¨")
    
    # æ‰¹é‡åŠ è½½å¸‚åœºæ•°æ®ï¼ˆè¿”å›MultiIndex[date, ticker]æ ¼å¼ï¼‰
    features_df = market_loader.load_market_data_batch(
        symbols=tickers,
        start_date=start_date,
        end_date=end_date
    )
    
    if features_df.empty:
        raise ValueError(f"æœªåŠ è½½åˆ°ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥InfluxDBè¿æ¥å’Œè‚¡ç¥¨ä»£ç ")
    
    # è®¡ç®—ç›®æ ‡å˜é‡ï¼ˆè¿œæœŸæ”¶ç›Šï¼‰
    from evaluation.cross_section_metrics import calculate_forward_returns
    
    prices_df = features_df[['close']]
    targets_df = calculate_forward_returns(
        prices=prices_df,
        periods=[1, 5, 10, 20],
        method='simple'
    )
    
    print(f"\nâœ… æ•°æ®åŠ è½½å®Œæˆ")
    print(f"   ç‰¹å¾æ•°æ®å½¢çŠ¶: {features_df.shape}")
    print(f"   ç›®æ ‡æ•°æ®å½¢çŠ¶: {targets_df.shape}")
    print(f"   æ—¥æœŸèŒƒå›´: {features_df.index.get_level_values('date').min()} ~ {features_df.index.get_level_values('date').max()}")
    print(f"   è‚¡ç¥¨æ•°é‡: {features_df.index.get_level_values('ticker').nunique()}")
    print(f"   è‚¡ç¥¨åˆ—è¡¨: {', '.join(features_df.index.get_level_values('ticker').unique()[:5])}..." if len(tickers) > 5 else f"   è‚¡ç¥¨åˆ—è¡¨: {', '.join(tickers)}")
    
    # 2.5 äº¤æ˜“å¯è¡Œæ€§è¿‡æ»¤
    print("\n" + "=" * 80)
    print("æ­¥éª¤ 2.5: äº¤æ˜“å¯è¡Œæ€§è¿‡æ»¤")
    print("=" * 80)
    
    tradability_filter = TradabilityFilter(
        min_volume=tradability_config.get('min_volume', 2000),
        min_amount=tradability_config.get('min_amount', 10000000),
        min_price=tradability_config.get('min_price', 1.0),
        min_turnover=tradability_config.get('min_turnover', 0.1),
        min_listing_days=tradability_config.get('min_listing_days', 60),
        exclude_st=tradability_config.get('exclude_st', True),
        exclude_limit_moves=tradability_config.get('exclude_limit_moves', False),
        limit_threshold=tradability_config.get('limit_threshold', 0.098)
    )
    
    # åº”ç”¨äº¤æ˜“å¯è¡Œæ€§è¿‡æ»¤ï¼Œè¿”å›å¸¦æœ‰ tradable_flag åˆ—çš„æ•°æ®å’Œè¿‡æ»¤æ—¥å¿—
    filter_log_path = os.path.join(ml_root, "ML output/reports/baseline_v1/tradability_filter_log.csv")
    os.makedirs(os.path.dirname(filter_log_path), exist_ok=True)
    features_df, filter_log_df = tradability_filter.apply_filters(
        features_df, 
        save_log=True, 
        log_path=filter_log_path
    )
    
    # ç”Ÿæˆå¯äº¤æ˜“æ€§æ©ç ï¼ˆåŸºäº tradable_flag åˆ—ï¼‰
    tradable_mask = features_df['tradable_flag'] == 1
    tradable_ratio = tradable_mask.sum() / len(tradable_mask) * 100
    
    print(f"âœ… äº¤æ˜“å¯è¡Œæ€§è¿‡æ»¤å®Œæˆ")
    print(f"   æ€»æ ·æœ¬æ•°: {len(tradable_mask)}")
    print(f"   å¯äº¤æ˜“æ ·æœ¬: {tradable_mask.sum()} ({tradable_ratio:.1f}%)")
    print(f"   è¢«è¿‡æ»¤æ ·æœ¬: {(~tradable_mask).sum()} ({100-tradable_ratio:.1f}%)")
    print(f"   è¿‡æ»¤æ—¥å¿—: {filter_log_path}")
    
    # 2.6 åŠ è½½è´¢åŠ¡æ•°æ®ï¼ˆå¦‚æœé…ç½®å¯ç”¨ï¼‰
    financial_features = None
    pit_config = config['data'].get('pit', {})
    
    if pit_config.get('enabled', False):
        print("\n" + "=" * 80)
        print("æ­¥éª¤ 2.6: åŠ è½½è´¢åŠ¡æ•°æ® (PITå¯¹é½)")
        print("=" * 80)
        
        try:
            financial_loader = FinancialDataLoader(
                announce_lag_days=pit_config.get('financial_lag_days', 90),
                ffill_limit=pit_config.get('financial_ffill_limit', 95)
            )
            
            financial_dfs = []
            for ticker in tickers:
                try:
                    fin_df = financial_loader.load_financial_data(
                        symbol=ticker,
                        start_date=start_date,
                        end_date=end_date
                    )
                    if fin_df is not None and not fin_df.empty:
                        financial_dfs.append(fin_df)
                except Exception as e:
                    print(f"   âš ï¸  {ticker} è´¢åŠ¡æ•°æ®åŠ è½½å¤±è´¥: {e}")
            
            if financial_dfs:
                financial_features = pd.concat(financial_dfs)
                print(f"âœ… è´¢åŠ¡æ•°æ®åŠ è½½å®Œæˆ")
                print(f"   è´¢åŠ¡ç‰¹å¾æ•°: {financial_features.shape[1]}")
                print(f"   æ ·æœ¬æ•°: {len(financial_features)}")
            else:
                print(f"âš ï¸  æœªåŠ è½½åˆ°è´¢åŠ¡æ•°æ®ï¼Œè·³è¿‡è´¢åŠ¡å› å­")
        except Exception as e:
            print(f"âš ï¸  è´¢åŠ¡æ•°æ®åŠ è½½å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            print(f"   å°†è·³è¿‡è´¢åŠ¡å› å­ï¼Œä»…ä½¿ç”¨å¸‚åœºæ•°æ®å› å­")
    else:
        print("\nğŸ“‹ è´¢åŠ¡æ•°æ®æœªå¯ç”¨ (pit.enabled=False)")
    
    # 2.7 åˆ›å»ºæ•°æ®å¿«ç…§ï¼ˆæ•°æ®ç‰ˆæœ¬ç®¡ç†ï¼‰
    snapshot_config = config.get('snapshot', {})
    snapshot_manager = None
    
    if snapshot_config.get('enabled', True):  # é»˜è®¤å¯ç”¨
        print("\n" + "=" * 80)
        print("æ­¥éª¤ 2.7: åˆ›å»ºæ•°æ®å¿«ç…§")
        print("=" * 80)
        
        try:
            snapshot_manager = DataSnapshot(
                output_dir=os.path.join(ml_root, "ML output"),
                snapshot_id=None  # è‡ªåŠ¨ç”Ÿæˆ
            )
            
            # å‡†å¤‡å¿«ç…§æ•°æ®ï¼ˆå¸‚åœºæ•°æ® + å¯äº¤æ˜“æ€§maskï¼‰
            snapshot_data = features_df.copy()
            snapshot_data['tradable_flag'] = tradable_mask.astype(int)
            
            # åˆ›å»ºå¿«ç…§
            filters_info = {
                'min_volume': tradability_config.get('min_volume', 2000),
                'min_amount': tradability_config.get('min_amount', 10000000),
                'min_price': tradability_config.get('min_price', 1.0),
                'exclude_st': tradability_config.get('exclude_st', True),
                'tradable_ratio': float(tradable_ratio)
            }
            
            snapshot_path = snapshot_manager.create_snapshot(
                data=snapshot_data,
                symbol='_'.join(tickers[:3]) + (f'_etc{len(tickers)}' if len(tickers) > 3 else ''),
                start_date=start_date,
                end_date=end_date,
                filters=filters_info,
                random_seed=42,
                save_parquet=True
            )
            
            print(f"âœ… æ•°æ®å¿«ç…§åˆ›å»ºå®Œæˆ")
            print(f"   å¿«ç…§ID: {snapshot_manager.snapshot_id}")
            print(f"   å¿«ç…§è·¯å¾„: {snapshot_path}")
        except Exception as e:
            print(f"âš ï¸  æ•°æ®å¿«ç…§åˆ›å»ºå¤±è´¥: {e}")
            print(f"   å°†ç»§ç»­æ‰§è¡Œï¼Œä½†ä¸ä¼šä¿å­˜å¿«ç…§")
    else:
        print("\nğŸ“‹ æ•°æ®å¿«ç…§æœªå¯ç”¨ (snapshot.enabled=False)")
    
    # 3. ç”Ÿæˆå› å­
    print("\n" + "=" * 80)
    print("æ­¥éª¤ 3: ç”Ÿæˆå› å­")
    print("=" * 80)
    
    factory = FactorFactory()
    
    # ç”Ÿæˆæ‰€æœ‰å› å­æ—
    print("\nğŸ­ ç”Ÿæˆå› å­...")
    all_factors_df = factory.generate_all_factors(features_df)
    
    # å¦‚æœæœ‰è´¢åŠ¡æ•°æ®ï¼Œå¯ä»¥ç”Ÿæˆè´¢åŠ¡å› å­ï¼ˆéœ€è¦FactorFactoryæ”¯æŒï¼‰
    if financial_features is not None:
        print(f"\nğŸ“Š è´¢åŠ¡æ•°æ®å¯ç”¨ï¼Œå¯ç”Ÿæˆè´¢åŠ¡ç›¸å…³å› å­")
        # TODO: åœ¨FactorFactoryä¸­æ·»åŠ è´¢åŠ¡å› å­ç”Ÿæˆæ–¹æ³•
        # factory.generate_financial_factors(financial_features)
    
    print(f"\nâœ… å› å­ç”Ÿæˆå®Œæˆ")
    print(f"   ç”Ÿæˆå› å­æ•°: {all_factors_df.shape[1]}")
    print(f"   å› å­æ—ç»Ÿè®¡:")
    
    # ç»Ÿè®¡å„æ—å› å­
    factor_families = factory.get_factor_registry()
    family_counts = {}
    for factor_info in factor_families.values():
        family = factor_info['family']
        family_counts[family] = family_counts.get(family, 0) + 1
    
    for family, count in family_counts.items():
        print(f"   - {family}: {count} ä¸ª")
    
    # 4. æ¨ªæˆªé¢è´¨é‡æ£€æŸ¥ï¼ˆä½¿ç”¨ä½ å·²æœ‰çš„è¯„ä¼°æ¡†æ¶ï¼ï¼‰
    print("\n" + "=" * 80)
    print("æ­¥éª¤ 4: æ¨ªæˆªé¢å› å­è¯„ä¼°ï¼ˆAlphalensé£æ ¼ï¼‰")
    print("=" * 80)
    
    # å‡†å¤‡ä»·æ ¼æ•°æ®ç”¨äºè®¡ç®—forward returns
    prices_df = features_df[['close']] if 'close' in features_df.columns else None
    
    # è®¡ç®—è¿œæœŸæ”¶ç›Šï¼ˆä½¿ç”¨ä½ çš„cross_section_metricsï¼‰
    print(f"\nğŸ“Š è®¡ç®—è¿œæœŸæ”¶ç›Š...")
    forward_horizons = [1, 5, 10, 20]
    forward_returns_df = calculate_forward_returns(
        prices=prices_df,
        periods=forward_horizons,
        method='simple'
    )
    print(f"   âœ… è¿œæœŸæ”¶ç›Šè®¡ç®—å®Œæˆ: {forward_returns_df.shape}")
    
    # ä½¿ç”¨æ­¥éª¤2.5ç”Ÿæˆçš„å¯äº¤æ˜“æ€§mask
    # tradable_mask å·²ç»åœ¨å‰é¢çš„ TradabilityFilter ä¸­ç”Ÿæˆ
    if tradable_mask is not None and tradable_mask.sum() > 0:
        print(f"   âœ… ä½¿ç”¨å¯äº¤æ˜“æ€§mask (å¯äº¤æ˜“æ ·æœ¬: {tradable_mask.sum()})")
        # è½¬æ¢ä¸ºDataFrameæ ¼å¼ä»¥åŒ¹é…CrossSectionAnalyzerçš„è¦æ±‚
        tradable_mask_df = pd.DataFrame({'tradable': tradable_mask}, index=features_df.index)
    else:
        tradable_mask_df = None
        print(f"   âš ï¸  æœªç”Ÿæˆå¯äº¤æ˜“æ€§maskï¼Œå°†ä½¿ç”¨å…¨éƒ¨æ ·æœ¬")
    
    # å‡†å¤‡å¸‚å€¼å’Œè¡Œä¸šæ•°æ®ï¼ˆç”¨äºä¸­æ€§åŒ–ï¼‰
    market_cap = features_df[['market_cap']] if 'market_cap' in features_df.columns else None
    industry = features_df[['industry']] if 'industry' in features_df.columns else None
    
    # ===== ä»é…ç½®æ–‡ä»¶è¯»å–è´¨é‡æ£€æŸ¥é˜ˆå€¼ =====
    quality_config = config['features'].get('factor_factory', {}).get('quality_check', {})
    
    # ä¸¥æ ¼æ ‡å‡†ï¼ˆç”Ÿäº§çº§ï¼‰
    strict_config = quality_config.get('strict', {})
    IC_THRESHOLD_STRICT = strict_config.get('ic_threshold', 0.02)
    ICIR_THRESHOLD_STRICT = strict_config.get('icir_threshold', 0.5)
    IC_PVALUE_STRICT = strict_config.get('ic_pvalue', 0.05)
    
    # æ¢ç´¢æ ‡å‡†ï¼ˆç ”ç©¶çº§ï¼‰
    explore_config = quality_config.get('exploratory', {})
    IC_THRESHOLD_EXPLORE = explore_config.get('ic_threshold', 0.005)
    ICIR_THRESHOLD_EXPLORE = explore_config.get('icir_threshold', 0.15)
    IC_PVALUE_THRESHOLD = explore_config.get('ic_pvalue', 0.10)
    
    # é€šç”¨æ ‡å‡†
    common_config = quality_config.get('common', {})
    SPREAD_THRESHOLD = common_config.get('spread_threshold', 0.0)
    CORR_THRESHOLD = common_config.get('corr_threshold', 0.8)
    PSI_THRESHOLD = common_config.get('psi_threshold', 0.25)
    USE_ABS_IC = common_config.get('use_abs_ic', True)
    
    # è‡ªåŠ¨é™çº§å¼€å…³
    AUTO_FALLBACK = quality_config.get('auto_fallback_to_exploratory', True)
    
    print(f"\nğŸ“‹ è´¨é‡æ£€æŸ¥é…ç½® (ä» ml_baseline.yml è¯»å–):")
    print(f"   ä¸¥æ ¼æ ‡å‡†: |IC|â‰¥{IC_THRESHOLD_STRICT}, |ICIR|â‰¥{ICIR_THRESHOLD_STRICT}, p<{IC_PVALUE_STRICT}")
    print(f"   æ¢ç´¢æ ‡å‡†: |IC|â‰¥{IC_THRESHOLD_EXPLORE}, |ICIR|â‰¥{ICIR_THRESHOLD_EXPLORE}, p<{IC_PVALUE_THRESHOLD}")
    print(f"   é€šç”¨æ ‡å‡†: Spread>{SPREAD_THRESHOLD}, MaxCorr<{CORR_THRESHOLD}, PSI<{PSI_THRESHOLD}")
    print(f"   ä½¿ç”¨|IC|: {USE_ABS_IC}, è‡ªåŠ¨é™çº§: {AUTO_FALLBACK}")
    
    # é€ä¸ªå› å­è¯„ä¼°
    qualified_factors = []       # ä¸¥æ ¼é€šè¿‡
    exploratory_factors = []     # æ¢ç´¢é€šè¿‡ï¼ˆå®½æ¾æ ‡å‡†ï¼‰
    quality_reports = {}
    
    print(f"\nğŸ” å¼€å§‹æ¨ªæˆªé¢è¯„ä¼° (å…± {all_factors_df.shape[1]} ä¸ªå› å­)...\n")
    
    # é¢„å¤„ç†é…ç½® - ä»é…ç½®æ–‡ä»¶è¯»å–
    preprocess_config = config['features'].get('preprocessing', {})
    # ç¡®ä¿æœ‰é»˜è®¤å€¼
    if 'winsorize' not in preprocess_config:
        preprocess_config['winsorize'] = True
    if 'standardize' not in preprocess_config:
        preprocess_config['standardize'] = True
    if 'neutralize' not in preprocess_config:
        preprocess_config['neutralize'] = False
    
    print(f"   é¢„å¤„ç†é…ç½®: winsorize={preprocess_config['winsorize']}, "
          f"standardize={preprocess_config['standardize']}, "
          f"neutralize={preprocess_config['neutralize']}")
    
    for i, factor_name in enumerate(all_factors_df.columns, 1):
        print(f"[{i}/{all_factors_df.shape[1]}] è¯„ä¼°å› å­: {factor_name}")
        
        try:
            # æ„å»ºå•å› å­DataFrame
            single_factor_df = all_factors_df[[factor_name]]
            
            # ä½¿ç”¨ä½ çš„CrossSectionAnalyzerï¼ï¼ˆä¸€æ¬¡æ€§å®Œæˆæ‰€æœ‰åˆ†æï¼ŒåŒ…æ‹¬æ·±åº¦è´¨é‡æ£€æŸ¥ï¼‰
            analyzer = CrossSectionAnalyzer(
                factors=single_factor_df,
                forward_returns=forward_returns_df,
                prices=prices_df if 'close' in features_df.columns else None,
                tradable_mask=tradable_mask_df,  # ä½¿ç”¨æ­¥éª¤2.5ç”Ÿæˆçš„å¯äº¤æ˜“æ€§mask
                market_cap=market_cap,
                industry=industry
            )
            
            # é¢„å¤„ç†ï¼ˆä½¿ç”¨é»˜è®¤å‚æ•°ï¼‰
            analyzer.preprocess(
                winsorize=preprocess_config.get('winsorize', True),
                standardize=preprocess_config.get('standardize', True),
                neutralize=preprocess_config.get('neutralize', False)
            )
            
            # è¿è¡Œå®Œæ•´åˆ†æï¼ˆä¸€æ¬¡åˆ°ä½ï¼ŒåŒ…å«æ·±åº¦è´¨é‡æ£€æŸ¥ï¼‰
            analyzer.analyze(
                n_quantiles=5,
                ic_method='spearman',
                spread_method='top_minus_mean',  # å®ç›˜æ›´ç¨³å¥
                periods_per_year=252,
                check_quality=True  # å¼€å¯æ·±åº¦æ£€æŸ¥ï¼ˆPSI/KS/ICè¡°å‡ï¼‰
            )
            
            # è·å–ç»“æœ
            results = analyzer.get_results()
            
            # æå–å…³é”®æŒ‡æ ‡ï¼ˆkeyä¸º(factor_name, 'ret_5d')ï¼‰
            key_5d = (factor_name, 'ret_5d')
            
            # å®‰å…¨è·å–å„é¡¹æŒ‡æ ‡ï¼ˆè‚¡ç¥¨æ•°å¤ªå°‘æ—¶å¯èƒ½ç¼ºå¤±ï¼‰
            ic_summary = results.get('ic_summary', {}).get(key_5d, {})
            spread_summary = results.get('spread_summary', {}).get(key_5d, {})
            monotonicity = results.get('monotonicity', {}).get(key_5d, {})
            quality_report = results.get('quality_reports', {}).get(factor_name, {})
            
            # å¦‚æœç¼ºå°‘å…³é”®æŒ‡æ ‡ï¼Œè·³è¿‡æ­¤å› å­
            if not ic_summary:
                print(f"   âš ï¸  ICæ•°æ®ä¸è¶³ï¼Œè·³è¿‡")
                continue
            
            # åˆ¤æ–­æ˜¯å¦é€šè¿‡ï¼ˆæ¨ªæˆªé¢è¯„ä¼°çš„æ ¸å¿ƒæŒ‡æ ‡ï¼‰
            ic_mean = ic_summary.get('mean', 0)
            ic_pvalue = ic_summary.get('p_value', 1)
            icir_annual = ic_summary.get('icir_annual', 0)
            spread_mean = spread_summary.get('mean', np.nan)
            kendall_tau = monotonicity.get('kendall_tau', np.nan)
            mono_pvalue = monotonicity.get('p_value', 1)
            
            # ===== è®¡ç®—ç”¨äºç­›é€‰çš„ICå€¼ï¼ˆæ”¯æŒç»å¯¹å€¼æ¨¡å¼ï¼‰=====
            ic_for_filter = abs(ic_mean) if USE_ABS_IC else ic_mean
            # å¯¹äºè´Ÿå‘å› å­ï¼ŒICIRä¹Ÿå–ç»å¯¹å€¼
            icir_for_filter = abs(icir_annual) if USE_ABS_IC else icir_annual
            
            # ===== ä¸¥æ ¼æ ‡å‡†ï¼ˆç”Ÿäº§çº§ï¼‰=====
            pass_ic_strict = ic_for_filter >= IC_THRESHOLD_STRICT and ic_pvalue < IC_PVALUE_STRICT
            pass_icir_strict = icir_for_filter >= ICIR_THRESHOLD_STRICT
            
            # ===== æ¢ç´¢æ ‡å‡†ï¼ˆç ”ç©¶çº§ï¼‰=====
            pass_ic_explore = ic_for_filter >= IC_THRESHOLD_EXPLORE and ic_pvalue < IC_PVALUE_THRESHOLD
            pass_icir_explore = icir_for_filter >= ICIR_THRESHOLD_EXPLORE
            
            # ===== é€šç”¨æ£€æŸ¥ =====
            pass_spread = spread_mean > SPREAD_THRESHOLD if not np.isnan(spread_mean) else True  # NaNæ—¶é»˜è®¤é€šè¿‡
            pass_mono = kendall_tau > 0 and mono_pvalue < 0.05 if not np.isnan(kendall_tau) else True
            
            # æ·±åº¦è´¨é‡æ£€æŸ¥ç»“æœï¼ˆä½¿ç”¨é…ç½®çš„é˜ˆå€¼ï¼‰
            pass_psi = quality_report.get('psi', 1.0) < PSI_THRESHOLD
            pass_ks = quality_report.get('ks_p', 0) > 0.05
            
            # ç›¸å…³æ€§æ£€æŸ¥ï¼ˆä¸å·²æœ‰æ¢ç´¢å› å­ï¼Œä½¿ç”¨é…ç½®çš„é˜ˆå€¼ï¼‰
            pass_corr = True
            max_corr = 0.0
            check_against = exploratory_factors if exploratory_factors else []
            if check_against:
                existing_factors = all_factors_df[check_against]
                corrs = existing_factors.corrwith(single_factor_df[factor_name]).abs()
                max_corr = corrs.max()
                pass_corr = max_corr < CORR_THRESHOLD
            
            # ===== åˆ¤æ–­é€šè¿‡å±‚çº§ =====
            # ä¸¥æ ¼é€šè¿‡ï¼šICã€ICIRéƒ½æ»¡è¶³ä¸¥æ ¼æ ‡å‡†ï¼Œä¸”ç›¸å…³æ€§OK
            strict_pass = pass_ic_strict and pass_icir_strict and pass_corr
            
            # æ¢ç´¢é€šè¿‡ï¼šICã€ICIRæ»¡è¶³æ¢ç´¢æ ‡å‡†ï¼Œä¸”ç›¸å…³æ€§OK
            exploratory_pass = pass_ic_explore and pass_icir_explore and pass_corr
            
            # å…¼å®¹æ—§é€»è¾‘çš„overall_passï¼ˆç°åœ¨ä½¿ç”¨æ¢ç´¢æ ‡å‡†ï¼Œè®©æ›´å¤šå› å­è¿›å…¥ä¸‹ä¸€æ­¥ï¼‰
            overall_pass = exploratory_pass
            
            # ä¿å­˜æŠ¥å‘Šï¼ˆåŒ…å«åŒå±‚åˆ¤å®šç»“æœï¼‰
            quality_reports[factor_name] = {
                'ic_mean': ic_mean,
                'ic_abs': abs(ic_mean),  # æ–°å¢ï¼šç»å¯¹å€¼IC
                'ic_direction': 'positive' if ic_mean >= 0 else 'negative',  # æ–°å¢ï¼šæ–¹å‘
                'icir_annual': icir_annual,
                'icir_abs': abs(icir_annual),  # æ–°å¢ï¼šç»å¯¹å€¼ICIR
                'ic_pvalue': ic_pvalue,
                'spread': spread_mean,
                'monotonicity_tau': kendall_tau,
                'max_correlation': max_corr,
                'ic_half_life': quality_report.get('ic_half_life', np.nan),
                'psi': quality_report.get('psi', np.nan),
                'ks_stat': quality_report.get('ks_stat', np.nan),
                'ks_p': quality_report.get('ks_p', np.nan),
                # ä¸¥æ ¼æ ‡å‡†åˆ¤å®š
                'pass_ic_strict': pass_ic_strict,
                'pass_icir_strict': pass_icir_strict,
                'strict_pass': strict_pass,
                # æ¢ç´¢æ ‡å‡†åˆ¤å®š
                'pass_ic_explore': pass_ic_explore,
                'pass_icir_explore': pass_icir_explore,
                'exploratory_pass': exploratory_pass,
                # é€šç”¨æ£€æŸ¥
                'pass_spread': pass_spread,
                'pass_correlation': pass_corr,
                'pass_psi': pass_psi,
                'pass_ks': pass_ks,
                # å…¼å®¹æ—§å­—æ®µ
                'pass_ic': pass_ic_explore,
                'pass_icir': pass_icir_explore,
                'overall_pass': overall_pass,
                'full_results': results
            }
            
            # æ ¹æ®é€šè¿‡å±‚çº§åˆ†ç±»
            if strict_pass:
                qualified_factors.append(factor_name)
                exploratory_factors.append(factor_name)
                direction_mark = "â¬†ï¸" if ic_mean >= 0 else "â¬‡ï¸"
                print(f"   âœ… ä¸¥æ ¼é€šè¿‡ {direction_mark}")
                print(f"      IC={ic_mean:.4f} (|IC|={abs(ic_mean):.4f}, ICIR={icir_annual:.2f})")
                spread_str = f"{spread_mean:.4f}" if not np.isnan(spread_mean) else "N/A"
                print(f"      Spread={spread_str}, MaxCorr={max_corr:.3f}")
            elif exploratory_pass:
                exploratory_factors.append(factor_name)
                direction_mark = "â¬†ï¸" if ic_mean >= 0 else "â¬‡ï¸"
                print(f"   ğŸ” æ¢ç´¢é€šè¿‡ {direction_mark}")
                print(f"      IC={ic_mean:.4f} (|IC|={abs(ic_mean):.4f}, ICIR={icir_annual:.2f})")
                print(f"      (æœªè¾¾ä¸¥æ ¼æ ‡å‡†ï¼Œä½†å¯ç”¨äºæ’åºæ¨¡å‹å®éªŒ)")
            else:
                fail_reasons = []
                if not pass_ic_explore: fail_reasons.append(f"|IC|<{IC_THRESHOLD_EXPLORE}æˆ–p>{IC_PVALUE_THRESHOLD}")
                if not pass_icir_explore: fail_reasons.append(f"|ICIR|<{ICIR_THRESHOLD_EXPLORE}")
                if not pass_corr: fail_reasons.append(f"MaxCorr>{CORR_THRESHOLD}")
                
                print(f"   âŒ æ‹’ç» | {', '.join(fail_reasons)}")
        
        except Exception as e:
            print(f"   âš ï¸  è¯„ä¼°å¤±è´¥: {str(e)}")
            quality_reports[factor_name] = {
                'overall_pass': False,
                'error': str(e)
            }
        
        print()
    
    print(f"âœ… æ¨ªæˆªé¢è¯„ä¼°å®Œæˆ")
    print(f"   ä¸¥æ ¼é€šè¿‡: {len(qualified_factors)} / {all_factors_df.shape[1]} ({len(qualified_factors)/all_factors_df.shape[1]*100:.1f}%)")
    print(f"   æ¢ç´¢é€šè¿‡: {len(exploratory_factors)} / {all_factors_df.shape[1]} ({len(exploratory_factors)/all_factors_df.shape[1]*100:.1f}%)")
    
    # ===== åˆå¹¶ä¸¥æ ¼é€šè¿‡å’Œæ¢ç´¢é€šè¿‡çš„å› å­ =====
    # ç­–ç•¥ï¼šå°†æ¢ç´¢é€šè¿‡ä½†ä¸åœ¨ä¸¥æ ¼é€šè¿‡ä¸­çš„å› å­ä¹ŸåŠ å…¥ï¼Œç”¨äºæ’åºæ¨¡å‹å®éªŒ
    # è¿™æ ·å¯ä»¥æœ‰æ›´å¤šå› å­ä¾›æ¨¡å‹å­¦ä¹ ï¼ŒåŒæ—¶ä¿ç•™è´¨é‡åˆ†çº§ä¿¡æ¯
    original_strict_count = len(qualified_factors)
    
    if AUTO_FALLBACK:
        # åˆå¹¶æ¢ç´¢å› å­ï¼ˆå»é‡ï¼‰
        for factor in exploratory_factors:
            if factor not in qualified_factors:
                qualified_factors.append(factor)
        
        if len(qualified_factors) > original_strict_count:
            print(f"\nğŸ“Š å› å­åˆå¹¶: ä¸¥æ ¼é€šè¿‡ {original_strict_count} + æ¢ç´¢è¡¥å…… {len(qualified_factors) - original_strict_count} = å…± {len(qualified_factors)} ä¸ªå› å­")
    
    # å¦‚æœä¸¥æ ¼é€šè¿‡ä¸º0ï¼Œä½¿ç”¨æ¢ç´¢å› å­
    if original_strict_count == 0 and len(exploratory_factors) > 0:
        print(f"\nâš ï¸  ä¸¥æ ¼é€šè¿‡å› å­æ•°ä¸º0ï¼Œä½¿ç”¨æ¢ç´¢é€šè¿‡çš„ {len(exploratory_factors)} ä¸ªå› å­")
        print(f"   è¿™äº›å› å­å¯ç”¨äºæ’åºæ¨¡å‹å®éªŒï¼Œä½†å»ºè®®åç»­ä¼˜åŒ–å› å­è´¨é‡")
    elif original_strict_count == 0 and len(exploratory_factors) == 0:
        print(f"\nâŒ ä¸¥æ ¼å’Œæ¢ç´¢éƒ½æ²¡æœ‰é€šè¿‡çš„å› å­ï¼Œè¯·æ£€æŸ¥å› å­è´¨é‡æˆ–æ”¾å®½ç­›é€‰æ ‡å‡†")
    
    # ===== 4.5 ä¿å­˜å®Œæ•´çš„å› å­ä½“æ£€æŠ¥å‘Š =====
    print("\n" + "-" * 60)
    print("ä¿å­˜å› å­ä½“æ£€è¯¦ç»†æŠ¥å‘Š")
    print("-" * 60)
    
    screening_dir = os.path.join(ml_root, "ML output/reports/baseline_v1/factor_screening")
    ic_series_dir = os.path.join(screening_dir, "ic_series")
    os.makedirs(ic_series_dir, exist_ok=True)
    
    # 1. ä¿å­˜æ‰€æœ‰å› å­çš„è¯¦ç»†ä½“æ£€æ•°æ®ï¼ˆCSVæ ¼å¼ï¼Œæ–¹ä¾¿æŸ¥çœ‹ï¼‰
    screening_records = []
    for factor_name, report in quality_reports.items():
        record = {
            'å› å­åç§°': factor_name,
            'ICæ–¹å‘': report.get('ic_direction', ''),
            'ICå‡å€¼': report.get('ic_mean', np.nan),
            '|IC|': report.get('ic_abs', np.nan),
            'ICIRå¹´åŒ–': report.get('icir_annual', np.nan),
            '|ICIR|': report.get('icir_abs', np.nan),
            'IC_På€¼': report.get('ic_pvalue', np.nan),
            'Spreadå‡å€¼': report.get('spread', np.nan),
            'å•è°ƒæ€§Tau': report.get('monotonicity_tau', np.nan),
            'æœ€å¤§ç›¸å…³æ€§': report.get('max_correlation', np.nan),
            'ICåŠè¡°æœŸ': report.get('ic_half_life', np.nan),
            'PSI': report.get('psi', np.nan),
            'KSç»Ÿè®¡é‡': report.get('ks_stat', np.nan),
            'KS_På€¼': report.get('ks_p', np.nan),
            # ä¸¥æ ¼æ ‡å‡†
            'ä¸¥æ ¼é€šè¿‡IC': report.get('pass_ic_strict', False),
            'ä¸¥æ ¼é€šè¿‡ICIR': report.get('pass_icir_strict', False),
            'ä¸¥æ ¼é€šè¿‡': report.get('strict_pass', False),
            # æ¢ç´¢æ ‡å‡†
            'æ¢ç´¢é€šè¿‡IC': report.get('pass_ic_explore', False),
            'æ¢ç´¢é€šè¿‡ICIR': report.get('pass_icir_explore', False),
            'æ¢ç´¢é€šè¿‡': report.get('exploratory_pass', False),
            # é€šç”¨æ£€æŸ¥
            'é€šè¿‡Spread': report.get('pass_spread', False),
            'é€šè¿‡ç›¸å…³æ€§': report.get('pass_correlation', True),
            'é€šè¿‡PSI': report.get('pass_psi', True),
            'é€šè¿‡KS': report.get('pass_ks', True),
            'å¤±è´¥åŸå› ': ''
        }
        
        # è®°å½•å¤±è´¥åŸå› ï¼ˆåŸºäºæ¢ç´¢æ ‡å‡†ï¼‰
        if not report.get('exploratory_pass', False):
            fail_reasons = []
            if not report.get('pass_ic_explore', False): 
                fail_reasons.append(f'|IC|<{IC_THRESHOLD_EXPLORE}æˆ–p>{IC_PVALUE_THRESHOLD}')
            if not report.get('pass_icir_explore', False): 
                fail_reasons.append(f'|ICIR|<{ICIR_THRESHOLD_EXPLORE}')
            if not report.get('pass_correlation', True): 
                fail_reasons.append(f'MaxCorr>{CORR_THRESHOLD}')
            if 'error' in report: 
                fail_reasons.append(f"é”™è¯¯:{report['error']}")
            record['å¤±è´¥åŸå› '] = '; '.join(fail_reasons)
        
        screening_records.append(record)
    
    screening_df = pd.DataFrame(screening_records)
    # æŒ‰ICå‡å€¼æ’åºï¼ˆç»å¯¹å€¼é™åºï¼‰
    screening_df = screening_df.sort_values('ICå‡å€¼', ascending=False, key=abs)
    
    screening_csv_path = os.path.join(screening_dir, f"factor_screening_detail_{datetime.now().strftime('%Y%m%d')}.csv")
    screening_df.to_csv(screening_csv_path, index=False, encoding='utf-8-sig')
    print(f"   âœ… å› å­ä½“æ£€è¯¦æƒ…: {screening_csv_path}")
    
    # 2. ä¿å­˜æ¯ä¸ªå› å­çš„ICæ—¶é—´åºåˆ—ï¼ˆä¾¿äºåˆ†æICè¡°å‡å’Œç¨³å®šæ€§ï¼‰
    ic_saved_count = 0
    for factor_name, report in quality_reports.items():
        full_results = report.get('full_results', {})
        if not full_results:
            continue
        
        # å°è¯•æå–ICæ—¶é—´åºåˆ—
        daily_ic = full_results.get('daily_ic', None)
        if daily_ic is not None and len(daily_ic) > 0:
            key_5d = (factor_name, 'ret_5d')
            try:
                if isinstance(daily_ic, pd.DataFrame):
                    if key_5d in daily_ic.columns:
                        ic_series = daily_ic[key_5d]
                    elif 5 in daily_ic.columns:
                        ic_series = daily_ic[5]
                    else:
                        ic_series = daily_ic.iloc[:, 0] if daily_ic.shape[1] > 0 else None
                elif isinstance(daily_ic, dict):
                    ic_series = daily_ic.get(key_5d, daily_ic.get(5, None))
                else:
                    ic_series = daily_ic
                
                if ic_series is not None and len(ic_series) > 0:
                    ic_series_path = os.path.join(ic_series_dir, f"{factor_name}_ic_5d.csv")
                    if isinstance(ic_series, pd.Series):
                        ic_series.to_csv(ic_series_path, header=['ic'])
                    else:
                        pd.Series(ic_series).to_csv(ic_series_path, header=['ic'])
                    ic_saved_count += 1
            except Exception as e:
                pass  # å¿½ç•¥ä¿å­˜å¤±è´¥çš„æƒ…å†µ
    
    print(f"   âœ… ICæ—¶é—´åºåˆ—: {ic_saved_count} ä¸ªå› å­ -> {ic_series_dir}")
    
    # 3. ä¿å­˜å®Œæ•´çš„JSONæ ¼å¼æŠ¥å‘Šï¼ˆåŒ…å«æ›´å¤šç»†èŠ‚ï¼Œä¾¿äºç¨‹åºè¯»å–ï¼‰
    json_report = {
        'generated_at': datetime.now().isoformat(),
        'data_range': {'start': start_date, 'end': end_date},
        'total_factors': len(quality_reports),
        'qualified_factors_strict': len([f for f in quality_reports if quality_reports[f].get('strict_pass', False)]),
        'qualified_factors_explore': len([f for f in quality_reports if quality_reports[f].get('exploratory_pass', False)]),
        'pass_rate': len(qualified_factors) / len(quality_reports) * 100 if quality_reports else 0,
        'thresholds': {
            'strict': {
                'ic_threshold': IC_THRESHOLD_STRICT,
                'icir_threshold': ICIR_THRESHOLD_STRICT,
            },
            'exploratory': {
                'ic_threshold': IC_THRESHOLD_EXPLORE,
                'icir_threshold': ICIR_THRESHOLD_EXPLORE,
                'ic_pvalue': IC_PVALUE_THRESHOLD,
            },
            'common': {
                'spread_threshold': SPREAD_THRESHOLD,
                'corr_threshold': CORR_THRESHOLD,
                'use_abs_ic': USE_ABS_IC,
            }
        },
        'factors': {}
    }
    
    for factor_name, report in quality_reports.items():
        # ç§»é™¤ full_resultsï¼ˆå¤ªå¤§äº†ï¼Œä¸é€‚åˆæ”¾JSONï¼‰
        factor_report = {k: v for k, v in report.items() if k != 'full_results'}
        # å¤„ç† NaN å€¼å’Œ numpy ç±»å‹
        for key, value in factor_report.items():
            if isinstance(value, (np.floating, float)) and np.isnan(value):
                factor_report[key] = None
            elif isinstance(value, (np.bool_, bool)):
                factor_report[key] = bool(value)
            elif isinstance(value, (np.integer,)):
                factor_report[key] = int(value)
            elif isinstance(value, (np.floating,)):
                factor_report[key] = float(value)
        json_report['factors'][factor_name] = factor_report
    
    json_path = os.path.join(screening_dir, f"factor_screening_summary_{datetime.now().strftime('%Y%m%d')}.json")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(json_report, f, indent=2, ensure_ascii=False)
    print(f"   âœ… JSONæ±‡æ€»æŠ¥å‘Š: {json_path}")
    
    # 4. æ‰“å°ä½“æ£€ç»Ÿè®¡
    strict_count = sum(1 for r in quality_reports.values() if r.get('strict_pass', False))
    explore_count = sum(1 for r in quality_reports.values() if r.get('exploratory_pass', False))
    
    print(f"\nğŸ“Š å› å­ä½“æ£€ç»Ÿè®¡:")
    print(f"   æ€»å› å­æ•°: {len(quality_reports)}")
    print(f"   ä¸¥æ ¼é€šè¿‡: {strict_count} ({strict_count/len(quality_reports)*100:.1f}%)")
    print(f"   æ¢ç´¢é€šè¿‡: {explore_count} ({explore_count/len(quality_reports)*100:.1f}%)")
    print(f"   æœªé€šè¿‡: {len(quality_reports) - explore_count}")
    
    # ç»Ÿè®¡å„é¡¹æ£€æŸ¥çš„é€šè¿‡æƒ…å†µ
    pass_counts = {
        '|IC|æ¢ç´¢è¾¾æ ‡': sum(1 for r in quality_reports.values() if r.get('pass_ic_explore', False)),
        '|IC|ä¸¥æ ¼è¾¾æ ‡': sum(1 for r in quality_reports.values() if r.get('pass_ic_strict', False)),
        '|ICIR|æ¢ç´¢è¾¾æ ‡': sum(1 for r in quality_reports.values() if r.get('pass_icir_explore', False)),
        '|ICIR|ä¸¥æ ¼è¾¾æ ‡': sum(1 for r in quality_reports.values() if r.get('pass_icir_strict', False)),
        'Spread>0': sum(1 for r in quality_reports.values() if r.get('pass_spread', False)),
        'ä½ç›¸å…³æ€§': sum(1 for r in quality_reports.values() if r.get('pass_correlation', True)),
    }
    print(f"\n   å„é¡¹æ£€æŸ¥é€šè¿‡ç‡:")
    for check_name, count in pass_counts.items():
        print(f"   - {check_name}: {count}/{len(quality_reports)} ({count/len(quality_reports)*100:.1f}%)")
    
    # ç»Ÿè®¡æ­£å‘/è´Ÿå‘å› å­
    positive_factors = [f for f, r in quality_reports.items() if r.get('ic_direction') == 'positive' and r.get('exploratory_pass', False)]
    negative_factors = [f for f, r in quality_reports.items() if r.get('ic_direction') == 'negative' and r.get('exploratory_pass', False)]
    print(f"\n   å› å­æ–¹å‘åˆ†å¸ƒ (æ¢ç´¢é€šè¿‡):")
    print(f"   - æ­£å‘å› å­ â¬†ï¸: {len(positive_factors)}")
    print(f"   - è´Ÿå‘å› å­ â¬‡ï¸: {len(negative_factors)}")
    
    # 5. å› å­å…¥åº“
    print("\n" + "=" * 80)
    print("æ­¥éª¤ 5: å› å­å…¥åº“ç®¡ç†")
    print("=" * 80)
    
    manager = FactorLibraryManager()
    
    # æ·»åŠ é€šè¿‡çš„å› å­
    print(f"\nğŸ“¥ å°† {len(qualified_factors)} ä¸ªé€šè¿‡çš„å› å­åŠ å…¥åº“ä¸­...\n")
    
    factor_registry = factory.get_factor_registry()
    
    for factor_name in qualified_factors:
        factor_info = factor_registry.get(factor_name, {})
        quality_report = quality_reports[factor_name]
        
        manager.add_factor(
            factor_name=factor_name,
            quality_report=quality_report,
            formula=factor_info.get('formula', ''),
            family=factor_info.get('family', ''),
            reference=factor_info.get('reference', '')
        )
    
    print(f"\nâœ… å› å­å…¥åº“å®Œæˆ")
    
    # 6. ç”Ÿæˆå®Œæ•´æŠ¥å‘Šï¼ˆä½¿ç”¨ä½ çš„tearsheetï¼ï¼‰
    print("\n" + "=" * 80)
    print("æ­¥éª¤ 6: ç”ŸæˆTearsheetæŠ¥å‘Š + å¯è§†åŒ–å›¾è¡¨")
    print("=" * 80)
    
    # ä¸ºæ¯ä¸ªé€šè¿‡çš„å› å­ç”Ÿæˆå®Œæ•´çš„tearsheetæŠ¥å‘Š
    reports_dir = os.path.join(ml_root, "ML output/reports/baseline_v1/factors")
    figures_dir = os.path.join(ml_root, "ML output/figures/baseline_v1/factors")
    os.makedirs(reports_dir, exist_ok=True)
    os.makedirs(figures_dir, exist_ok=True)
    
    print(f"\nğŸ“ ç”Ÿæˆ {len(qualified_factors)} ä¸ªå› å­çš„è¯¦ç»†æŠ¥å‘Š + å›¾è¡¨...\n")
    
    for i, factor_name in enumerate(qualified_factors, 1):
        print(f"[{i}/{len(qualified_factors)}] ç”ŸæˆæŠ¥å‘Š: {factor_name}")
        
        try:
            report = quality_reports[factor_name]
            full_results = report['full_results']
            
            # ===== 6.1 ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ =====
            factor_figures_dir = os.path.join(figures_dir, factor_name)
            os.makedirs(factor_figures_dir, exist_ok=True)
            
            plot_paths = {}
            
            # å‡†å¤‡ICåºåˆ—æ•°æ®
            key_5d = (factor_name, 'ret_5d')
            
            # 1. ICæ—¶é—´åºåˆ—å›¾
            if 'daily_ic' in full_results:
                daily_ic = full_results['daily_ic']
                if key_5d in daily_ic.columns:
                    ic_series = daily_ic[key_5d]
                elif isinstance(daily_ic, pd.DataFrame) and 5 in daily_ic.columns:
                    ic_series = daily_ic[5]
                elif isinstance(daily_ic, dict) and 5 in daily_ic:
                    ic_series = daily_ic[5]
                else:
                    ic_series = None
                
                if ic_series is not None and len(ic_series) > 0:
                    try:
                        ic_path = os.path.join(factor_figures_dir, f"ic_series_{factor_name}_5d.png")
                        plot_ic_time_series(
                            ic_series,
                            title=f"IC Time Series: {factor_name} @ 5d",
                            save_path=ic_path
                        )
                        plot_paths['ic_series'] = ic_path
                    except Exception as e:
                        print(f"      âš ï¸  ICæ—¶é—´åºåˆ—å›¾ç”Ÿæˆå¤±è´¥: {e}")
                    
                    # 2. ICåˆ†å¸ƒå›¾
                    try:
                        ic_dist_path = os.path.join(factor_figures_dir, f"ic_dist_{factor_name}_5d.png")
                        plot_ic_distribution(
                            ic_series,
                            title=f"IC Distribution: {factor_name} @ 5d",
                            save_path=ic_dist_path
                        )
                        plot_paths['ic_distribution'] = ic_dist_path
                    except Exception as e:
                        print(f"      âš ï¸  ICåˆ†å¸ƒå›¾ç”Ÿæˆå¤±è´¥: {e}")
                    
                    # 3. æœˆåº¦ICçƒ­åŠ›å›¾
                    try:
                        ic_heatmap_path = os.path.join(factor_figures_dir, f"ic_heatmap_{factor_name}_5d.png")
                        plot_monthly_ic_heatmap(
                            ic_series,
                            title=f"Monthly IC Heatmap: {factor_name} @ 5d",
                            save_path=ic_heatmap_path
                        )
                        plot_paths['ic_heatmap'] = ic_heatmap_path
                    except Exception as e:
                        print(f"      âš ï¸  æœˆåº¦ICçƒ­åŠ›å›¾ç”Ÿæˆå¤±è´¥: {e}")
            
            # 4. åˆ†ä½æ•°ç´¯è®¡æ”¶ç›Šå›¾
            if 'cumulative_returns' in full_results:
                cum_rets = full_results['cumulative_returns']
                if key_5d in cum_rets:
                    cum_ret_data = cum_rets[key_5d]
                elif 5 in cum_rets:
                    cum_ret_data = cum_rets[5]
                else:
                    cum_ret_data = None
                
                if cum_ret_data is not None and len(cum_ret_data) > 0:
                    try:
                        cum_path = os.path.join(factor_figures_dir, f"quantile_cumret_{factor_name}_5d.png")
                        plot_quantile_cumulative_returns(
                            cum_ret_data,
                            title=f"Quantile Cumulative Returns: {factor_name} @ 5d",
                            save_path=cum_path
                        )
                        plot_paths['cumulative_returns'] = cum_path
                    except Exception as e:
                        print(f"      âš ï¸  åˆ†ä½æ•°ç´¯è®¡æ”¶ç›Šå›¾ç”Ÿæˆå¤±è´¥: {e}")
            
            # 5. åˆ†ä½æ•°å¹³å‡æ”¶ç›ŠæŸ±çŠ¶å›¾
            if 'quantile_returns' in full_results:
                q_rets = full_results['quantile_returns']
                if key_5d in q_rets:
                    q_ret_data = q_rets[key_5d]
                elif 5 in q_rets:
                    q_ret_data = q_rets[5]
                else:
                    q_ret_data = None
                
                if q_ret_data is not None and len(q_ret_data) > 0:
                    try:
                        mean_ret_path = os.path.join(factor_figures_dir, f"quantile_meanret_{factor_name}_5d.png")
                        plot_quantile_mean_returns(
                            q_ret_data,
                            title=f"Quantile Mean Returns: {factor_name} @ 5d",
                            save_path=mean_ret_path
                        )
                        plot_paths['mean_returns'] = mean_ret_path
                    except Exception as e:
                        print(f"      âš ï¸  åˆ†ä½æ•°å¹³å‡æ”¶ç›Šå›¾ç”Ÿæˆå¤±è´¥: {e}")
            
            # 6. Spreadç´¯è®¡æ”¶ç›Šå›¾
            if 'spreads' in full_results:
                spreads = full_results['spreads']
                if key_5d in spreads:
                    spread_data = spreads[key_5d]
                elif 5 in spreads:
                    spread_data = spreads[5]
                else:
                    spread_data = None
                
                if spread_data is not None and len(spread_data) > 0:
                    try:
                        spread_path = os.path.join(factor_figures_dir, f"spread_cumret_{factor_name}_5d.png")
                        plot_spread_cumulative_returns(
                            spread_data,
                            title=f"Spread Cumulative Returns: {factor_name} @ 5d",
                            save_path=spread_path
                        )
                        plot_paths['spread_cumulative'] = spread_path
                    except Exception as e:
                        print(f"      âš ï¸  Spreadç´¯è®¡æ”¶ç›Šå›¾ç”Ÿæˆå¤±è´¥: {e}")
            
            print(f"      ğŸ“Š ç”Ÿæˆ {len(plot_paths)} ä¸ªå›¾è¡¨")
            
            # ===== 6.2 ç”ŸæˆHTML Tearsheet =====
            tearsheet_path = os.path.join(reports_dir, f"tearsheet_{factor_name}_5d.html")
            
            generate_html_tearsheet(
                analyzer_results=full_results,
                factor_name=factor_name,
                return_period='ret_5d',
                output_path=tearsheet_path,
                plot_paths=plot_paths  # ä¼ å…¥å›¾è¡¨è·¯å¾„
            )
            
            # ===== 6.3 ä¿å­˜CSVæ•°æ® =====
            # ä¿å­˜ICæ—¶é—´åºåˆ—CSV
            try:
                if 'ic_series' in full_results and 5 in full_results['ic_series']:
                    ic_series_path = os.path.join(reports_dir, f"ic_{factor_name}_5d.csv")
                    full_results['ic_series'][5].to_csv(ic_series_path)
                elif 'daily_ic' in full_results:
                    ic_series_path = os.path.join(reports_dir, f"ic_{factor_name}_5d.csv")
                    if key_5d in full_results['daily_ic'].columns:
                        full_results['daily_ic'][key_5d].to_csv(ic_series_path, header=['ic'])
            except Exception as e:
                print(f"      âš ï¸  IC CSVä¿å­˜å¤±è´¥: {e}")
            
            # ä¿å­˜åˆ†ä½æ•°æ”¶ç›ŠCSV
            try:
                if 'quantile_returns' in full_results:
                    q_rets = full_results['quantile_returns']
                    if key_5d in q_rets:
                        quantile_returns_path = os.path.join(reports_dir, f"quantile_returns_{factor_name}_5d.csv")
                        q_rets[key_5d].to_csv(quantile_returns_path)
                    elif 5 in q_rets:
                        quantile_returns_path = os.path.join(reports_dir, f"quantile_returns_{factor_name}_5d.csv")
                        q_rets[5].to_csv(quantile_returns_path)
            except Exception as e:
                print(f"      âš ï¸  åˆ†ä½æ•°æ”¶ç›ŠCSVä¿å­˜å¤±è´¥: {e}")
            
            print(f"   âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            print(f"      HTML: {tearsheet_path}")
            print(f"      å›¾è¡¨ç›®å½•: {factor_figures_dir}")
        
        except Exception as e:
            print(f"   âš ï¸  æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
        
        print()
    
    # å› å­æ¸…å•æŠ¥å‘Š
    print(f"\nğŸ“Š ç”Ÿæˆå› å­åº“æ±‡æ€»æŠ¥å‘Š...")
    report_df = manager.generate_factor_report()
    
    # æ—åˆ«è¡¨ç°æŠ¥å‘Š
    family_df = manager.analyze_factor_family_performance()
    
    print(f"\næ—åˆ«è¡¨ç°æ±‡æ€»:")
    print(family_df.to_string(index=False))
    
    # ä¿å­˜é€šè¿‡çš„å› å­æ•°æ®
    print(f"\nğŸ’¾ ä¿å­˜å› å­æ•°æ®...")
    
    qualified_factors_df = all_factors_df[qualified_factors]
    
    # ä¿å­˜è·¯å¾„
    datasets_dir = os.path.join(ml_root, "ML output/datasets/baseline_v1")
    os.makedirs(datasets_dir, exist_ok=True)
    
    # ä¿å­˜åŸå§‹å› å­ (Parquetæ ¼å¼)
    output_path = os.path.join(datasets_dir, f"qualified_factors_{datetime.now().strftime('%Y%m%d')}.parquet")
    qualified_factors_df.to_parquet(output_path)
    print(f"   âœ… åŸå§‹å› å­ (Parquet): {output_path}")
    
    # åŒæ—¶ä¿å­˜CSVæ ¼å¼ï¼ˆå…¼å®¹æ€§ï¼‰
    csv_path = os.path.join(datasets_dir, f"qualified_factors_{datetime.now().strftime('%Y%m%d')}.csv")
    qualified_factors_df.to_csv(csv_path)
    print(f"   âœ… åŸå§‹å› å­ (CSV): {csv_path}")
    
    # ===== ä¿å­˜ä¸­æ€§åŒ–åçš„å› å­ï¼ˆå¦‚æœå¯ç”¨äº†ä¸­æ€§åŒ–ï¼‰=====
    print(f"\nğŸ“Š ä¸­æ€§åŒ–çŠ¶æ€æ£€æŸ¥:")
    print(f"   neutralize é…ç½®: {preprocess_config.get('neutralize', False)}")
    print(f"   market_cap æ•°æ®: {'æœ‰' if market_cap is not None else 'æ— '}")
    print(f"   industry æ•°æ®: {'æœ‰' if industry is not None else 'æ— '}")
    
    # æ£€æŸ¥ market_cap æ˜¯å¦å…¨ä¸º NaN
    if market_cap is not None:
        valid_mc = market_cap['market_cap'].notna().sum()
        print(f"   market_cap æœ‰æ•ˆå€¼: {valid_mc}/{len(market_cap)}")
        if valid_mc == 0:
            print(f"   âš ï¸  market_cap å…¨ä¸º NaNï¼Œå°†ä»…ä½¿ç”¨è¡Œä¸šä¸­æ€§åŒ–")
            market_cap = None
    
    # æ£€æŸ¥ industry æ˜¯å¦æœ‰æ•ˆ
    if industry is not None:
        valid_ind = industry['industry'].notna().sum()
        print(f"   industry æœ‰æ•ˆå€¼: {valid_ind}/{len(industry)}")
        if valid_ind == 0:
            print(f"   âš ï¸  industry å…¨ä¸º NaNï¼Œæ— æ³•è¿›è¡Œè¡Œä¸šä¸­æ€§åŒ–")
            industry = None
    
    if preprocess_config.get('neutralize', False) and (market_cap is not None or industry is not None):
        print(f"\nğŸ’¾ ä¿å­˜ä¸­æ€§åŒ–å› å­...")
        
        from evaluation.factor_preprocessing import preprocess_factor_pipeline
        
        # å¯¹æ‰€æœ‰åˆæ ¼å› å­è¿›è¡Œä¸­æ€§åŒ–
        neutralized_factors_df = preprocess_factor_pipeline(
            factors=qualified_factors_df,
            market_cap=market_cap,
            industry=industry,
            winsorize=True,
            standardize=True,
            neutralize=True
        )
        
        # ä¿å­˜ä¸­æ€§åŒ–å› å­ (Parquetæ ¼å¼)
        neutral_output_path = os.path.join(datasets_dir, f"qualified_factors_neutralized_{datetime.now().strftime('%Y%m%d')}.parquet")
        neutralized_factors_df.to_parquet(neutral_output_path)
        print(f"   âœ… ä¸­æ€§åŒ–å› å­ (Parquet): {neutral_output_path}")
        
        # CSVæ ¼å¼
        neutral_csv_path = os.path.join(datasets_dir, f"qualified_factors_neutralized_{datetime.now().strftime('%Y%m%d')}.csv")
        neutralized_factors_df.to_csv(neutral_csv_path)
        print(f"   âœ… ä¸­æ€§åŒ–å› å­ (CSV): {neutral_csv_path}")
    
    # ä¿å­˜final_feature_list.txt
    feature_list_path = os.path.join(ml_root, "ML output/final_feature_list.txt")
    with open(feature_list_path, 'w', encoding='utf-8') as f:
        f.write("# å› å­å·¥å‚ v1 - åˆæ ¼å› å­æ¸…å•\n")
        f.write(f"# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"# æ•°æ®åŒºé—´: {start_date} ~ {end_date}\n")
        f.write(f"# é€šè¿‡å› å­æ•°: {len(qualified_factors)}\n")
        f.write("\n")
        for factor_name in qualified_factors:
            report = quality_reports[factor_name]
            f.write(f"{factor_name}\t")
            f.write(f"IC={report['ic_mean']:.4f}\t")
            f.write(f"ICIR={report['icir_annual']:.2f}\t")
            f.write(f"Spread={report['spread']:.4f}\n")
    
    print(f"   âœ… å› å­æ¸…å•: {feature_list_path}")
    
    # 7. éªŒæ”¶æ£€æŸ¥
    print("\n" + "=" * 80)
    print("éªŒæ”¶æ£€æŸ¥")
    print("=" * 80)
    
    acceptance_passed = True
    
    # æ£€æŸ¥1: è‡³å°‘10ä¸ªç¨³å®šå› å­è¿‡æ£€
    print(f"\nâœ“ æ£€æŸ¥1: ç¨³å®šå› å­æ•°é‡")
    print(f"   è¦æ±‚: â‰¥10 ä¸ª")
    print(f"   å®é™…: {len(qualified_factors)} ä¸ª")
    
    if len(qualified_factors) < 10:
        print(f"   âŒ æœªé€šè¿‡")
        acceptance_passed = False
    else:
        print(f"   âœ… é€šè¿‡")
    
    # æ£€æŸ¥2: æ¨ªæˆªé¢ Rank IC æ˜¾è‘—
    print(f"\nâœ“ æ£€æŸ¥2: Rank IC æ˜¾è‘—æ€§")
    
    significant_factors = []
    for factor_name in qualified_factors:
        report = quality_reports[factor_name]
        # å…¼å®¹æ‰å¹³æ ¼å¼ï¼ˆpass_icï¼‰å’ŒåµŒå¥—æ ¼å¼ï¼ˆic_metrics.pass_icï¼‰
        if 'ic_metrics' in report:
            pass_ic = report['ic_metrics']['pass_ic']
        else:
            pass_ic = report.get('pass_ic', False)
        
        if pass_ic:
            significant_factors.append(factor_name)
    
    print(f"   è¦æ±‚: |IC| â‰¥ {IC_THRESHOLD_STRICT} ä¸”ç»Ÿè®¡æ˜¾è‘— (p < {IC_PVALUE_STRICT})")
    print(f"   å®é™…: {len(significant_factors)} / {len(qualified_factors)} ä¸ªå› å­æ˜¾è‘—")
    
    if len(significant_factors) < len(qualified_factors) * 0.8:
        print(f"   âŒ æœªé€šè¿‡ (æ˜¾è‘—å› å­æ¯”ä¾‹è¿‡ä½)")
        acceptance_passed = False
    else:
        print(f"   âœ… é€šè¿‡")
    
    # æ£€æŸ¥3: åˆå…¥åç»„åˆ IC æœ‰å®è´¨æå‡
    print(f"\nâœ“ æ£€æŸ¥3: ç»„åˆ IC æå‡")
    print(f"   åŸºå‡†ç‰¹å¾æ•°: {features_df.shape[1]}")
    print(f"   æ–°å¢å› å­æ•°: {len(qualified_factors)}")
    
    # ç®€å•ç»„åˆæµ‹è¯•ï¼šæ‰€æœ‰å› å­ç­‰æƒå¹³å‡
    combined_factor = qualified_factors_df.mean(axis=1)
    # ä½¿ç”¨ 5æ—¥æ”¶ç›Šä½œä¸ºç›®æ ‡ï¼ˆä¸å› å­è¯„ä¼°ä¸€è‡´ï¼‰
    target_col = f"ret_{target_config['horizon']}d"
    if target_col in targets_df.columns:
        target_values = targets_df[target_col]
    else:
        # å›é€€åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨åˆ—
        target_values = targets_df.iloc[:, 0]
    
    # è®¡ç®—ç»„åˆIC
    aligned_df = pd.DataFrame({
        'factor': combined_factor,
        'target': target_values
    }).dropna()
    
    grouped = aligned_df.groupby(level='date')
    ic_series = grouped.apply(lambda x: x['factor'].corr(x['target'], method='spearman'))
    
    # ç¡®ä¿ ic_series æ˜¯ä¸€ç»´çš„ï¼Œå–æ ‡é‡å€¼
    if hasattr(ic_series, 'values'):
        ic_values = ic_series.values.flatten()
        combined_ic = float(np.nanmean(ic_values))
        combined_icir = float(np.nanmean(ic_values) / np.nanstd(ic_values) * np.sqrt(252)) if np.nanstd(ic_values) > 0 else 0.0
    else:
        combined_ic = float(ic_series) if not pd.isna(ic_series) else 0.0
        combined_icir = 0.0
    
    print(f"   ç»„åˆIC: {combined_ic:.4f}")
    print(f"   ç»„åˆICIR: {combined_icir:.2f}")
    
    if combined_ic < 0.03:
        print(f"   âš ï¸  ç»„åˆICåä½ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–")
    else:
        print(f"   âœ… ç»„åˆICæ˜¾è‘—")
    
    # æœ€ç»ˆéªŒæ”¶ç»“æœ
    print("\n" + "=" * 80)
    if acceptance_passed:
        print("ğŸ‰ éªŒæ”¶é€šè¿‡ï¼å› å­å·¥å‚ v1 æ„å»ºæˆåŠŸ")
    else:
        print("âš ï¸  éƒ¨åˆ†éªŒæ”¶æŒ‡æ ‡æœªè¾¾æ ‡ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    print("=" * 80)
    
    print(f"\nå®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    return qualified_factors_df, quality_reports, manager


if __name__ == "__main__":
    """è¿è¡Œå› å­å‡†å¤‡æµç¨‹"""
    
    # ç›´æ¥è¿è¡Œï¼Œæ‰€æœ‰å‚æ•°ä»é…ç½®æ–‡ä»¶è¯»å–ï¼ˆè‚¡ç¥¨æ± ã€æ—¥æœŸèŒƒå›´ç­‰ï¼‰
    qualified_factors_df, quality_reports, manager = prepare_factors(
        config_path="configs/ml_baseline.yml"
    )
    
    print("\nâœ… æµç¨‹æ‰§è¡Œå®Œæˆï¼")
