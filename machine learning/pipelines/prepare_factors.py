#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å› å­å‡†å¤‡ç®¡é“ - å› å­å·¥å‚å®Œæ•´æµç¨‹

æœ¬ç®¡é“å®Œå…¨åŸºäºå·²æœ‰çš„æ¨ªæˆªé¢è¯„ä¼°æ¡†æ¶ (evaluation/) æ„å»ºï¼Œ
å®ç°ä»å› å­ç”Ÿæˆåˆ°è´¨é‡æ£€æŸ¥ã€æŠ¥å‘Šè¾“å‡ºçš„å…¨æµç¨‹è‡ªåŠ¨åŒ–ã€‚

æµç¨‹ï¼š
1. åŠ è½½å¸‚åœºæ•°æ®ï¼ˆDataLoaderï¼‰
   - ä»InfluxDBåŠ è½½OHLCVæ•°æ®
   - åº”ç”¨å¯äº¤æ˜“æ€§è¿‡æ»¤
   
2. ç”Ÿæˆå› å­ï¼ˆFactorFactoryï¼‰
   - åŠ¨é‡æ—: ROC_N, Price_to_SMA, RankMomentum
   - æ³¢åŠ¨ç‡æ—: RealizedVol, Parkinson, Skewness
   - é‡ä»·æ—: Turnover, VolumePriceCorr, VWAP_Dev
   
3. æ¨ªæˆªé¢è´¨é‡æ£€æŸ¥ â­æ ¸å¿ƒæ­¥éª¤
   ä½¿ç”¨ evaluation/CrossSectionAnalyzer è¿›è¡Œå®Œæ•´è¯„ä¼°ï¼š
   - calculate_forward_returns: è®¡ç®—è¿œæœŸæ”¶ç›Š
   - preprocess: Winsorize + æ ‡å‡†åŒ– + ä¸­æ€§åŒ–
   - analyze: è®¡ç®—IC/ICIR/Spread/å•è°ƒæ€§
   - è¯„ä¼°æ ‡å‡†: ICâ‰¥0.02, ICIRâ‰¥0.5, Spread>0
   
4. å› å­å…¥åº“ï¼ˆFactorLibraryManagerï¼‰
   - åªæœ‰é€šè¿‡æ¨ªæˆªé¢æ£€æŸ¥çš„å› å­æ‰å…¥åº“
   - ä¿å­˜è´¨é‡æŠ¥å‘Šå’Œå…ƒæ•°æ®
   
5. ç”ŸæˆæŠ¥å‘Š â­è¾“å‡ºæ ‡å‡†åŒ–
   ä½¿ç”¨ evaluation/tearsheet ç”ŸæˆæŠ¥å‘Šï¼š
   - HTML tearsheet: tearsheet_{factor}_{period}.html
   - ICåºåˆ—CSV: ic_{factor}_{period}.csv
   - åˆ†ä½æ•°æ”¶ç›ŠCSV: quantile_returns_{factor}_{period}.csv
   - è‡ªåŠ¨ç”Ÿæˆæ‰€æœ‰å›¾è¡¨ (ICèµ°å»Šå›¾ã€ç´¯è®¡æ”¶ç›Šã€Spreadç­‰)

è¾“å‡ºç›®å½•ç»“æ„:
/ML output/reports/baseline_v1/factors/
  â”œâ”€â”€ tearsheet_ROC_20_5d.html
  â”œâ”€â”€ ic_ROC_20_5d.csv
  â””â”€â”€ quantile_returns_ROC_20_5d.csv
/ML output/figures/baseline_v1/factors/
  â”œâ”€â”€ ic_series_ROC_20_5d.png
  â”œâ”€â”€ quantile_cumret_ROC_20_5d.png
  â””â”€â”€ spread_cumret_ROC_20_5d.png
/ML output/datasets/baseline_v1/
  â””â”€â”€ qualified_factors_YYYYMMDD.parquet

éªŒæ”¶æ ‡å‡†:
- â‰¥10ä¸ªç¨³å®šå› å­é€šè¿‡æ£€æŸ¥
- æ¨ªæˆªé¢Rank ICæ˜¾è‘— (IC>0.02, p<0.05)
- åˆå…¥åç»„åˆICæœ‰å®è´¨æå‡ (>0.03)

è¯¦ç»†æ–‡æ¡£: pipelines/README_PREPARE_FACTORS.md
"""

import os
import sys
import yaml
import pandas as pd
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
ml_root = os.path.dirname(current_dir)
if ml_root not in sys.path:
    sys.path.insert(0, ml_root)

from features.factor_factory import FactorFactory
from features.factor_library_manager import FactorLibraryManager
from data.data_loader import DataLoader
from data.tradability_filter import TradabilityFilter
from data.financial_data_loader import FinancialDataLoader
# ä½¿ç”¨ä½ å·²æœ‰çš„æ¨ªæˆªé¢è¯„ä¼°æ¡†æ¶ï¼
from evaluation.cross_section_analyzer import CrossSectionAnalyzer
from evaluation.cross_section_metrics import calculate_forward_returns
from evaluation.factor_preprocessing import preprocess_factor_pipeline
from evaluation.tearsheet import generate_html_tearsheet


def load_config(config_path: str = "configs/ml_baseline.yml") -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if not os.path.isabs(config_path):
        config_path = os.path.join(ml_root, config_path)
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    return config


def prepare_factors(config_path: str = "configs/ml_baseline.yml",
                   start_date: str = None,
                   end_date: str = None,
                   tickers: list = None):
    """
    å› å­å‡†å¤‡å®Œæ•´æµç¨‹
    
    Parameters:
    -----------
    config_path : str
        é…ç½®æ–‡ä»¶è·¯å¾„
    start_date : str
        å¼€å§‹æ—¥æœŸ
    end_date : str
        ç»“æŸæ—¥æœŸ
    tickers : list, optional
        è‚¡ç¥¨åˆ—è¡¨ï¼ˆNoneè¡¨ç¤ºå…¨å¸‚åœºï¼‰
    """
    print("=" * 80)
    print("å› å­å·¥å‚ v1 - å®Œæ•´æµç¨‹")
    print("=" * 80)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"æ•°æ®åŒºé—´: {start_date} ~ {end_date}")
    print(f"è‚¡ç¥¨èŒƒå›´: {tickers if tickers else 'å…¨å¸‚åœº'}")
    print()
    
    # 1. åŠ è½½é…ç½®
    print("\n" + "=" * 80)
    print("æ­¥éª¤ 1: åŠ è½½é…ç½®")
    print("=" * 80)
    config = load_config(config_path)
    
    # ä»é…ç½®ä¸­æå–å‚æ•°
    influxdb_config = config['data']['influxdb']
    target_config = config.get('target', {})  # æ³¨æ„ï¼šé…ç½®æ–‡ä»¶ç”¨çš„æ˜¯ 'target' å•æ•°
    tradability_config = config['data'].get('universe', {})  # ä½¿ç”¨ 'universe' è€Œä¸æ˜¯ 'tradability_filter'
    
    # å¦‚æœæ²¡æœ‰ä¼ å…¥æ—¥æœŸå‚æ•°ï¼Œä»é…ç½®æ–‡ä»¶è¯»å–
    if start_date is None:
        start_date = config['data'].get('start_date', '2018-01-01')
    if end_date is None:
        end_date = config['data'].get('end_date', '2024-12-31')
    
    # å¦‚æœæ²¡æœ‰ä¼ å…¥è‚¡ç¥¨åˆ—è¡¨ï¼Œä»é…ç½®æ–‡ä»¶è¯»å–
    # æ³¨æ„ï¼šInfluxDB ä¸­å­˜å‚¨çš„è‚¡ç¥¨ä»£ç æ˜¯çº¯æ•°å­—æ ¼å¼ï¼ˆå¦‚ '000001'ï¼‰ï¼Œä¸å¸¦åç¼€
    if tickers is None:
        tickers = config['data'].get('symbol', None)
        if isinstance(tickers, str):
            tickers = [tickers]
    
    # è®¾ç½®é»˜è®¤å€¼
    if 'type' not in target_config:
        target_config['type'] = 'forward_return'
    if 'horizon' not in target_config:
        target_config['horizon'] = target_config.get('forward_periods', 5)
    
    print(f"âœ… é…ç½®åŠ è½½å®Œæˆ")
    print(f"   InfluxDB: {influxdb_config['url']}")
    print(f"   é¢„æµ‹ç›®æ ‡: {target_config.get('name', 'future_return_5d')} ({target_config['horizon']}æ—¥)")
    print(f"   æ—¥æœŸèŒƒå›´: {start_date} ~ {end_date}")
    print(f"   è‚¡ç¥¨ä»£ç : {tickers}")
    
    # 2. åŠ è½½æ•°æ®ï¼ˆä½¿ç”¨MarketDataLoaderæ‰¹é‡åŠ è½½ï¼‰
    print("\n" + "=" * 80)
    print("æ­¥éª¤ 2: åŠ è½½å¸‚åœºæ•°æ®")
    print("=" * 80)
    
    # ä½¿ç”¨MarketDataLoaderæ‰¹é‡åŠ è½½å¤šè‚¡ç¥¨æ•°æ®
    from data.market_data_loader import MarketDataLoader
    
    market_loader = MarketDataLoader(
        url=influxdb_config['url'],
        token=influxdb_config['token'],
        org=influxdb_config['org'],
        bucket=influxdb_config['bucket']
    )
    
    # å¦‚æœæœªæŒ‡å®štickersï¼Œå¯ä»¥ä»é…ç½®æˆ–æ•°æ®åº“è·å–è‚¡ç¥¨æ± 
    if not tickers:
        # TODO: ä»é…ç½®æ–‡ä»¶æˆ–æ•°æ®åº“è·å–è‚¡ç¥¨æ± 
        print(f"\nâš ï¸  æœªæŒ‡å®šè‚¡ç¥¨åˆ—è¡¨ï¼Œè¯·åœ¨é…ç½®ä¸­è®¾ç½®æˆ–ä¼ å…¥tickerså‚æ•°")
        raise ValueError("å¿…é¡»æä¾›tickerså‚æ•°")
    
    # æ‰¹é‡åŠ è½½å¸‚åœºæ•°æ®ï¼ˆè¿”å›MultiIndex[date, ticker]æ ¼å¼ï¼‰
    features_df = market_loader.load_market_data_batch(
        symbols=tickers,
        start_date=start_date,
        end_date=end_date
    )
    
    if features_df.empty:
        raise ValueError(f"æœªåŠ è½½åˆ°ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥InfluxDBè¿æ¥å’Œè‚¡ç¥¨ä»£ç ")
    
    # è®¡ç®—ç›®æ ‡å˜é‡ï¼ˆè¿œæœŸæ”¶ç›Šï¼‰
    from evaluation.cross_section_metrics import calculate_forward_returns
    
    prices_df = features_df[['close']]
    targets_df = calculate_forward_returns(
        prices=prices_df,
        periods=[1, 5, 10, 20],
        method='simple'
    )
    
    print(f"\nâœ… æ•°æ®åŠ è½½å®Œæˆ")
    print(f"   ç‰¹å¾æ•°æ®å½¢çŠ¶: {features_df.shape}")
    print(f"   ç›®æ ‡æ•°æ®å½¢çŠ¶: {targets_df.shape}")
    print(f"   æ—¥æœŸèŒƒå›´: {features_df.index.get_level_values('date').min()} ~ {features_df.index.get_level_values('date').max()}")
    print(f"   è‚¡ç¥¨æ•°é‡: {features_df.index.get_level_values('ticker').nunique()}")
    print(f"   è‚¡ç¥¨åˆ—è¡¨: {', '.join(features_df.index.get_level_values('ticker').unique()[:5])}..." if len(tickers) > 5 else f"   è‚¡ç¥¨åˆ—è¡¨: {', '.join(tickers)}")
    
    # 2.5 äº¤æ˜“å¯è¡Œæ€§è¿‡æ»¤
    print("\n" + "=" * 80)
    print("æ­¥éª¤ 2.5: äº¤æ˜“å¯è¡Œæ€§è¿‡æ»¤")
    print("=" * 80)
    
    tradability_filter = TradabilityFilter(
        min_volume=tradability_config.get('min_volume', 2000),
        min_amount=tradability_config.get('min_amount', 10000000),
        min_price=tradability_config.get('min_price', 1.0),
        min_turnover=tradability_config.get('min_turnover', 0.1),
        min_listing_days=tradability_config.get('min_listing_days', 60),
        exclude_st=tradability_config.get('exclude_st', True),
        exclude_limit_moves=tradability_config.get('exclude_limit_moves', False),
        limit_threshold=tradability_config.get('limit_threshold', 0.098)
    )
    
    # ç”Ÿæˆå¯äº¤æ˜“æ€§æ©ç 
    tradable_mask = tradability_filter.filter(features_df)
    tradable_ratio = tradable_mask.sum() / len(tradable_mask) * 100
    
    print(f"âœ… äº¤æ˜“å¯è¡Œæ€§è¿‡æ»¤å®Œæˆ")
    print(f"   æ€»æ ·æœ¬æ•°: {len(tradable_mask)}")
    print(f"   å¯äº¤æ˜“æ ·æœ¬: {tradable_mask.sum()} ({tradable_ratio:.1f}%)")
    print(f"   è¢«è¿‡æ»¤æ ·æœ¬: {(~tradable_mask).sum()} ({100-tradable_ratio:.1f}%)")
    
    # 2.6 åŠ è½½è´¢åŠ¡æ•°æ®ï¼ˆå¦‚æœé…ç½®å¯ç”¨ï¼‰
    financial_features = None
    pit_config = config['data'].get('pit', {})
    
    if pit_config.get('enabled', False):
        print("\n" + "=" * 80)
        print("æ­¥éª¤ 2.6: åŠ è½½è´¢åŠ¡æ•°æ® (PITå¯¹é½)")
        print("=" * 80)
        
        try:
            financial_loader = FinancialDataLoader(
                announce_lag_days=pit_config.get('financial_lag_days', 90),
                ffill_limit=pit_config.get('financial_ffill_limit', 95)
            )
            
            financial_dfs = []
            for ticker in tickers:
                try:
                    fin_df = financial_loader.load_financial_data(
                        symbol=ticker,
                        start_date=start_date,
                        end_date=end_date
                    )
                    if fin_df is not None and not fin_df.empty:
                        financial_dfs.append(fin_df)
                except Exception as e:
                    print(f"   âš ï¸  {ticker} è´¢åŠ¡æ•°æ®åŠ è½½å¤±è´¥: {e}")
            
            if financial_dfs:
                financial_features = pd.concat(financial_dfs)
                print(f"âœ… è´¢åŠ¡æ•°æ®åŠ è½½å®Œæˆ")
                print(f"   è´¢åŠ¡ç‰¹å¾æ•°: {financial_features.shape[1]}")
                print(f"   æ ·æœ¬æ•°: {len(financial_features)}")
            else:
                print(f"âš ï¸  æœªåŠ è½½åˆ°è´¢åŠ¡æ•°æ®ï¼Œè·³è¿‡è´¢åŠ¡å› å­")
        except Exception as e:
            print(f"âš ï¸  è´¢åŠ¡æ•°æ®åŠ è½½å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            print(f"   å°†è·³è¿‡è´¢åŠ¡å› å­ï¼Œä»…ä½¿ç”¨å¸‚åœºæ•°æ®å› å­")
    else:
        print("\nğŸ“‹ è´¢åŠ¡æ•°æ®æœªå¯ç”¨ (pit.enabled=False)")
    
    # 3. ç”Ÿæˆå› å­
    print("\n" + "=" * 80)
    print("æ­¥éª¤ 3: ç”Ÿæˆå› å­")
    print("=" * 80)
    
    factory = FactorFactory()
    
    # ç”Ÿæˆæ‰€æœ‰å› å­æ—
    print("\nğŸ­ ç”Ÿæˆå› å­...")
    all_factors_df = factory.generate_all_factors(features_df)
    
    # å¦‚æœæœ‰è´¢åŠ¡æ•°æ®ï¼Œå¯ä»¥ç”Ÿæˆè´¢åŠ¡å› å­ï¼ˆéœ€è¦FactorFactoryæ”¯æŒï¼‰
    if financial_features is not None:
        print(f"\nğŸ“Š è´¢åŠ¡æ•°æ®å¯ç”¨ï¼Œå¯ç”Ÿæˆè´¢åŠ¡ç›¸å…³å› å­")
        # TODO: åœ¨FactorFactoryä¸­æ·»åŠ è´¢åŠ¡å› å­ç”Ÿæˆæ–¹æ³•
        # factory.generate_financial_factors(financial_features)
    
    print(f"\nâœ… å› å­ç”Ÿæˆå®Œæˆ")
    print(f"   ç”Ÿæˆå› å­æ•°: {all_factors_df.shape[1]}")
    print(f"   å› å­æ—ç»Ÿè®¡:")
    
    # ç»Ÿè®¡å„æ—å› å­
    factor_families = factory.get_factor_registry()
    family_counts = {}
    for factor_info in factor_families.values():
        family = factor_info['family']
        family_counts[family] = family_counts.get(family, 0) + 1
    
    for family, count in family_counts.items():
        print(f"   - {family}: {count} ä¸ª")
    
    # 4. æ¨ªæˆªé¢è´¨é‡æ£€æŸ¥ï¼ˆä½¿ç”¨ä½ å·²æœ‰çš„è¯„ä¼°æ¡†æ¶ï¼ï¼‰
    print("\n" + "=" * 80)
    print("æ­¥éª¤ 4: æ¨ªæˆªé¢å› å­è¯„ä¼°ï¼ˆAlphalensé£æ ¼ï¼‰")
    print("=" * 80)
    
    # å‡†å¤‡ä»·æ ¼æ•°æ®ç”¨äºè®¡ç®—forward returns
    prices_df = features_df[['close']] if 'close' in features_df.columns else None
    
    # è®¡ç®—è¿œæœŸæ”¶ç›Šï¼ˆä½¿ç”¨ä½ çš„cross_section_metricsï¼‰
    print(f"\nğŸ“Š è®¡ç®—è¿œæœŸæ”¶ç›Š...")
    forward_horizons = [1, 5, 10, 20]
    forward_returns_df = calculate_forward_returns(
        prices=prices_df,
        periods=forward_horizons,
        method='simple'
    )
    print(f"   âœ… è¿œæœŸæ”¶ç›Šè®¡ç®—å®Œæˆ: {forward_returns_df.shape}")
    
    # ä½¿ç”¨æ­¥éª¤2.5ç”Ÿæˆçš„å¯äº¤æ˜“æ€§mask
    # tradable_mask å·²ç»åœ¨å‰é¢çš„ TradabilityFilter ä¸­ç”Ÿæˆ
    if tradable_mask is not None and tradable_mask.sum() > 0:
        print(f"   âœ… ä½¿ç”¨å¯äº¤æ˜“æ€§mask (å¯äº¤æ˜“æ ·æœ¬: {tradable_mask.sum()})")
        # è½¬æ¢ä¸ºDataFrameæ ¼å¼ä»¥åŒ¹é…CrossSectionAnalyzerçš„è¦æ±‚
        tradable_mask_df = pd.DataFrame({'tradable': tradable_mask}, index=features_df.index)
    else:
        tradable_mask_df = None
        print(f"   âš ï¸  æœªç”Ÿæˆå¯äº¤æ˜“æ€§maskï¼Œå°†ä½¿ç”¨å…¨éƒ¨æ ·æœ¬")
    
    # å‡†å¤‡å¸‚å€¼å’Œè¡Œä¸šæ•°æ®ï¼ˆç”¨äºä¸­æ€§åŒ–ï¼‰
    market_cap = features_df[['market_cap']] if 'market_cap' in features_df.columns else None
    industry = features_df[['industry']] if 'industry' in features_df.columns else None
    
    # è´¨é‡æ£€æŸ¥é˜ˆå€¼
    IC_THRESHOLD = 0.02
    ICIR_THRESHOLD = 0.5
    SPREAD_THRESHOLD = 0.0
    CORR_THRESHOLD = 0.7
    
    # é€ä¸ªå› å­è¯„ä¼°
    qualified_factors = []
    quality_reports = {}
    
    print(f"\nğŸ” å¼€å§‹æ¨ªæˆªé¢è¯„ä¼° (å…± {all_factors_df.shape[1]} ä¸ªå› å­)...\n")
    
    # é¢„å¤„ç†é…ç½® - ä½¿ç”¨é»˜è®¤å€¼å³å¯
    preprocess_config = {
        'winsorize': True,
        'standardize': True,
        'neutralize': False  # å¯é€‰: True (éœ€è¦market_cap/industry)
    }
    
    for i, factor_name in enumerate(all_factors_df.columns, 1):
        print(f"[{i}/{all_factors_df.shape[1]}] è¯„ä¼°å› å­: {factor_name}")
        
        try:
            # æ„å»ºå•å› å­DataFrame
            single_factor_df = all_factors_df[[factor_name]]
            
            # ä½¿ç”¨ä½ çš„CrossSectionAnalyzerï¼ï¼ˆä¸€æ¬¡æ€§å®Œæˆæ‰€æœ‰åˆ†æï¼ŒåŒ…æ‹¬æ·±åº¦è´¨é‡æ£€æŸ¥ï¼‰
            analyzer = CrossSectionAnalyzer(
                factors=single_factor_df,
                forward_returns=forward_returns_df,
                prices=prices_df if 'close' in features_df.columns else None,
                tradable_mask=tradable_mask_df,  # ä½¿ç”¨æ­¥éª¤2.5ç”Ÿæˆçš„å¯äº¤æ˜“æ€§mask
                market_cap=market_cap,
                industry=industry
            )
            
            # é¢„å¤„ç†ï¼ˆä½¿ç”¨é»˜è®¤å‚æ•°ï¼‰
            analyzer.preprocess(
                winsorize=preprocess_config.get('winsorize', True),
                standardize=preprocess_config.get('standardize', True),
                neutralize=preprocess_config.get('neutralize', False)
            )
            
            # è¿è¡Œå®Œæ•´åˆ†æï¼ˆä¸€æ¬¡åˆ°ä½ï¼ŒåŒ…å«æ·±åº¦è´¨é‡æ£€æŸ¥ï¼‰
            analyzer.analyze(
                n_quantiles=5,
                ic_method='spearman',
                spread_method='top_minus_mean',  # å®ç›˜æ›´ç¨³å¥
                periods_per_year=252,
                check_quality=True  # å¼€å¯æ·±åº¦æ£€æŸ¥ï¼ˆPSI/KS/ICè¡°å‡ï¼‰
            )
            
            # è·å–ç»“æœ
            results = analyzer.get_results()
            
            # æå–å…³é”®æŒ‡æ ‡ï¼ˆkeyä¸º(factor_name, 'ret_5d')ï¼‰
            key_5d = (factor_name, 'ret_5d')
            
            # å®‰å…¨è·å–å„é¡¹æŒ‡æ ‡ï¼ˆè‚¡ç¥¨æ•°å¤ªå°‘æ—¶å¯èƒ½ç¼ºå¤±ï¼‰
            ic_summary = results.get('ic_summary', {}).get(key_5d, {})
            spread_summary = results.get('spread_summary', {}).get(key_5d, {})
            monotonicity = results.get('monotonicity', {}).get(key_5d, {})
            quality_report = results.get('quality_reports', {}).get(factor_name, {})
            
            # å¦‚æœç¼ºå°‘å…³é”®æŒ‡æ ‡ï¼Œè·³è¿‡æ­¤å› å­
            if not ic_summary:
                print(f"   âš ï¸  ICæ•°æ®ä¸è¶³ï¼Œè·³è¿‡")
                continue
            
            # åˆ¤æ–­æ˜¯å¦é€šè¿‡ï¼ˆæ¨ªæˆªé¢è¯„ä¼°çš„æ ¸å¿ƒæŒ‡æ ‡ï¼‰
            ic_mean = ic_summary.get('mean', 0)
            ic_pvalue = ic_summary.get('p_value', 1)
            icir_annual = ic_summary.get('icir_annual', 0)
            spread_mean = spread_summary.get('mean', np.nan)
            kendall_tau = monotonicity.get('kendall_tau', np.nan)
            mono_pvalue = monotonicity.get('p_value', 1)
            
            pass_ic = ic_mean >= IC_THRESHOLD and ic_pvalue < 0.05
            pass_icir = icir_annual >= ICIR_THRESHOLD
            pass_spread = spread_mean > SPREAD_THRESHOLD if not np.isnan(spread_mean) else False
            pass_mono = kendall_tau > 0 and mono_pvalue < 0.05 if not np.isnan(kendall_tau) else False
            
            # æ·±åº¦è´¨é‡æ£€æŸ¥ç»“æœ
            pass_psi = quality_report.get('psi', 1.0) < 0.25
            pass_ks = quality_report.get('ks_p', 0) > 0.05
            
            # ç›¸å…³æ€§æ£€æŸ¥ï¼ˆä¸å·²æœ‰å› å­ï¼‰
            pass_corr = True
            max_corr = 0.0
            if qualified_factors:
                existing_factors = all_factors_df[qualified_factors]
                corrs = existing_factors.corrwith(single_factor_df[factor_name]).abs()
                max_corr = corrs.max()
                pass_corr = max_corr < CORR_THRESHOLD
            
            # æ ¸å¿ƒæŒ‡æ ‡ï¼šICå¿…é¡»é€šè¿‡ï¼Œå…¶ä»–æŒ‡æ ‡åœ¨æ•°æ®å……è¶³æ—¶æ‰æ£€æŸ¥
            # è‚¡ç¥¨æ•°å¤ªå°‘æ—¶ï¼ŒSpreadå’Œå•è°ƒæ€§å¯èƒ½ä¸ºNaNï¼Œæ”¾å®½æ¡ä»¶
            overall_pass = pass_ic and pass_icir and pass_corr
            if not np.isnan(spread_mean):
                overall_pass = overall_pass and pass_spread
            
            # ä¿å­˜æŠ¥å‘Š
            quality_reports[factor_name] = {
                'ic_mean': ic_mean,
                'icir_annual': icir_annual,
                'ic_pvalue': ic_pvalue,
                'spread': spread_mean,
                'monotonicity_tau': kendall_tau,
                'max_correlation': max_corr,
                'ic_half_life': quality_report.get('ic_half_life', np.nan),
                'psi': quality_report.get('psi', np.nan),
                'ks_stat': quality_report.get('ks_stat', np.nan),
                'ks_p': quality_report.get('ks_p', np.nan),
                'pass_ic': pass_ic,
                'pass_icir': pass_icir,
                'pass_spread': pass_spread,
                'pass_correlation': pass_corr,
                'pass_psi': pass_psi,
                'pass_ks': pass_ks,
                'overall_pass': overall_pass,
                'full_results': results  # æ¨ªæˆªé¢å®Œæ•´ç»“æœ
            }
            
            if overall_pass:
                qualified_factors.append(factor_name)
                print(f"   âœ… é€šè¿‡")
                print(f"      IC={ic_mean:.4f} (ICIR={icir_annual:.2f})")
                spread_str = f"{spread_mean:.4f}" if not np.isnan(spread_mean) else "N/A"
                tau_str = f"{kendall_tau:.3f}" if not np.isnan(kendall_tau) else "N/A"
                print(f"      Spread={spread_str}, Ï„={tau_str}")
            else:
                fail_reasons = []
                if not pass_ic: fail_reasons.append("ICä¸æ˜¾è‘—")
                if not pass_icir: fail_reasons.append("ICIRè¿‡ä½")
                if not pass_spread and not np.isnan(spread_mean): fail_reasons.append("Spreadâ‰¤0")
                if not pass_corr: fail_reasons.append("ä¸å·²æœ‰å› å­é«˜åº¦ç›¸å…³")
                
                print(f"   âŒ æ‹’ç» | {', '.join(fail_reasons) if fail_reasons else 'ICæ¡ä»¶æœªæ»¡è¶³'}")
        
        except Exception as e:
            print(f"   âš ï¸  è¯„ä¼°å¤±è´¥: {str(e)}")
            quality_reports[factor_name] = {
                'overall_pass': False,
                'error': str(e)
            }
        
        print()
    
    print(f"âœ… æ¨ªæˆªé¢è¯„ä¼°å®Œæˆ")
    print(f"   é€šè¿‡å› å­æ•°: {len(qualified_factors)} / {all_factors_df.shape[1]}")
    print(f"   é€šè¿‡ç‡: {len(qualified_factors) / all_factors_df.shape[1] * 100:.1f}%")
    
    # 5. å› å­å…¥åº“
    print("\n" + "=" * 80)
    print("æ­¥éª¤ 5: å› å­å…¥åº“ç®¡ç†")
    print("=" * 80)
    
    manager = FactorLibraryManager()
    
    # æ·»åŠ é€šè¿‡çš„å› å­
    print(f"\nğŸ“¥ å°† {len(qualified_factors)} ä¸ªé€šè¿‡çš„å› å­åŠ å…¥åº“ä¸­...\n")
    
    factor_registry = factory.get_factor_registry()
    
    for factor_name in qualified_factors:
        factor_info = factor_registry.get(factor_name, {})
        quality_report = quality_reports[factor_name]
        
        manager.add_factor(
            factor_name=factor_name,
            quality_report=quality_report,
            formula=factor_info.get('formula', ''),
            family=factor_info.get('family', ''),
            reference=factor_info.get('reference', '')
        )
    
    print(f"\nâœ… å› å­å…¥åº“å®Œæˆ")
    
    # 6. ç”Ÿæˆå®Œæ•´æŠ¥å‘Šï¼ˆä½¿ç”¨ä½ çš„tearsheetï¼ï¼‰
    print("\n" + "=" * 80)
    print("æ­¥éª¤ 6: ç”ŸæˆTearsheetæŠ¥å‘Š")
    print("=" * 80)
    
    # ä¸ºæ¯ä¸ªé€šè¿‡çš„å› å­ç”Ÿæˆå®Œæ•´çš„tearsheetæŠ¥å‘Š
    reports_dir = os.path.join(ml_root, "ML output/reports/baseline_v1/factors")
    figures_dir = os.path.join(ml_root, "ML output/figures/baseline_v1/factors")
    os.makedirs(reports_dir, exist_ok=True)
    os.makedirs(figures_dir, exist_ok=True)
    
    print(f"\nğŸ“ ç”Ÿæˆ {len(qualified_factors)} ä¸ªå› å­çš„è¯¦ç»†æŠ¥å‘Š...\n")
    
    for i, factor_name in enumerate(qualified_factors, 1):
        print(f"[{i}/{len(qualified_factors)}] ç”ŸæˆæŠ¥å‘Š: {factor_name}")
        
        try:
            report = quality_reports[factor_name]
            full_results = report['full_results']
            
            # ç”ŸæˆHTML tearsheetï¼ˆä½¿ç”¨ä½ çš„evaluationæ¨¡å—ï¼ï¼‰
            tearsheet_path = os.path.join(reports_dir, f"tearsheet_{factor_name}_5d.html")
            
            # ä½¿ç”¨æ­£ç¡®çš„tearsheetå‡½æ•°
            from evaluation.tearsheet import generate_html_tearsheet
            generate_html_tearsheet(
                analyzer_results=full_results,
                factor_name=factor_name,
                return_period='ret_5d',
                output_path=tearsheet_path,
                plot_paths=None  # å›¾è¡¨ä¼šè‡ªåŠ¨ç”Ÿæˆåœ¨figuresç›®å½•
            )
            
            # ä¿å­˜ICæ—¶é—´åºåˆ—CSV
            ic_series_path = os.path.join(reports_dir, f"ic_{factor_name}_5d.csv")
            full_results['ic_series'][5].to_csv(ic_series_path)
            
            # ä¿å­˜åˆ†ä½æ•°æ”¶ç›ŠCSV
            quantile_returns_path = os.path.join(reports_dir, f"quantile_returns_{factor_name}_5d.csv")
            full_results['quantile_returns'][5].to_csv(quantile_returns_path)
            
            print(f"   âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            print(f"      HTML: {tearsheet_path}")
        
        except Exception as e:
            print(f"   âš ï¸  æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
        
        print()
    
    # å› å­æ¸…å•æŠ¥å‘Š
    print(f"\nğŸ“Š ç”Ÿæˆå› å­åº“æ±‡æ€»æŠ¥å‘Š...")
    report_df = manager.generate_factor_report()
    
    # æ—åˆ«è¡¨ç°æŠ¥å‘Š
    family_df = manager.analyze_factor_family_performance()
    
    print(f"\næ—åˆ«è¡¨ç°æ±‡æ€»:")
    print(family_df.to_string(index=False))
    
    # ä¿å­˜é€šè¿‡çš„å› å­æ•°æ®
    print(f"\nğŸ’¾ ä¿å­˜å› å­æ•°æ®...")
    
    qualified_factors_df = all_factors_df[qualified_factors]
    
    # ä¿å­˜è·¯å¾„
    datasets_dir = os.path.join(ml_root, "ML output/datasets/baseline_v1")
    os.makedirs(datasets_dir, exist_ok=True)
    
    # ä¿å­˜Parquetæ ¼å¼
    output_path = os.path.join(datasets_dir, f"qualified_factors_{datetime.now().strftime('%Y%m%d')}.parquet")
    qualified_factors_df.to_parquet(output_path)
    print(f"   âœ… å› å­æ•°æ® (Parquet): {output_path}")
    
    # åŒæ—¶ä¿å­˜CSVæ ¼å¼ï¼ˆå…¼å®¹æ€§ï¼‰
    csv_path = os.path.join(datasets_dir, f"qualified_factors_{datetime.now().strftime('%Y%m%d')}.csv")
    qualified_factors_df.to_csv(csv_path)
    print(f"   âœ… å› å­æ•°æ® (CSV): {csv_path}")
    
    # ä¿å­˜final_feature_list.txt
    feature_list_path = os.path.join(ml_root, "ML output/final_feature_list.txt")
    with open(feature_list_path, 'w', encoding='utf-8') as f:
        f.write("# å› å­å·¥å‚ v1 - åˆæ ¼å› å­æ¸…å•\n")
        f.write(f"# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"# æ•°æ®åŒºé—´: {start_date} ~ {end_date}\n")
        f.write(f"# é€šè¿‡å› å­æ•°: {len(qualified_factors)}\n")
        f.write("\n")
        for factor_name in qualified_factors:
            report = quality_reports[factor_name]
            f.write(f"{factor_name}\t")
            f.write(f"IC={report['ic_mean']:.4f}\t")
            f.write(f"ICIR={report['icir_annual']:.2f}\t")
            f.write(f"Spread={report['spread']:.4f}\n")
    
    print(f"   âœ… å› å­æ¸…å•: {feature_list_path}")
    
    # 7. éªŒæ”¶æ£€æŸ¥
    print("\n" + "=" * 80)
    print("éªŒæ”¶æ£€æŸ¥")
    print("=" * 80)
    
    acceptance_passed = True
    
    # æ£€æŸ¥1: è‡³å°‘10ä¸ªç¨³å®šå› å­è¿‡æ£€
    print(f"\nâœ“ æ£€æŸ¥1: ç¨³å®šå› å­æ•°é‡")
    print(f"   è¦æ±‚: â‰¥10 ä¸ª")
    print(f"   å®é™…: {len(qualified_factors)} ä¸ª")
    
    if len(qualified_factors) < 10:
        print(f"   âŒ æœªé€šè¿‡")
        acceptance_passed = False
    else:
        print(f"   âœ… é€šè¿‡")
    
    # æ£€æŸ¥2: æ¨ªæˆªé¢ Rank IC æ˜¾è‘—
    print(f"\nâœ“ æ£€æŸ¥2: Rank IC æ˜¾è‘—æ€§")
    
    significant_factors = []
    for factor_name in qualified_factors:
        report = quality_reports[factor_name]
        # å…¼å®¹æ‰å¹³æ ¼å¼ï¼ˆpass_icï¼‰å’ŒåµŒå¥—æ ¼å¼ï¼ˆic_metrics.pass_icï¼‰
        if 'ic_metrics' in report:
            pass_ic = report['ic_metrics']['pass_ic']
        else:
            pass_ic = report.get('pass_ic', False)
        
        if pass_ic:
            significant_factors.append(factor_name)
    
    print(f"   è¦æ±‚: IC > 0.02 ä¸”ç»Ÿè®¡æ˜¾è‘—")
    print(f"   å®é™…: {len(significant_factors)} / {len(qualified_factors)} ä¸ªå› å­æ˜¾è‘—")
    
    if len(significant_factors) < len(qualified_factors) * 0.8:
        print(f"   âŒ æœªé€šè¿‡ (æ˜¾è‘—å› å­æ¯”ä¾‹è¿‡ä½)")
        acceptance_passed = False
    else:
        print(f"   âœ… é€šè¿‡")
    
    # æ£€æŸ¥3: åˆå…¥åç»„åˆ IC æœ‰å®è´¨æå‡
    print(f"\nâœ“ æ£€æŸ¥3: ç»„åˆ IC æå‡")
    print(f"   åŸºå‡†ç‰¹å¾æ•°: {features_df.shape[1]}")
    print(f"   æ–°å¢å› å­æ•°: {len(qualified_factors)}")
    
    # ç®€å•ç»„åˆæµ‹è¯•ï¼šæ‰€æœ‰å› å­ç­‰æƒå¹³å‡
    combined_factor = qualified_factors_df.mean(axis=1)
    # ä½¿ç”¨ 5æ—¥æ”¶ç›Šä½œä¸ºç›®æ ‡ï¼ˆä¸å› å­è¯„ä¼°ä¸€è‡´ï¼‰
    target_col = f"ret_{target_config['horizon']}d"
    if target_col in targets_df.columns:
        target_values = targets_df[target_col]
    else:
        # å›é€€åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨åˆ—
        target_values = targets_df.iloc[:, 0]
    
    # è®¡ç®—ç»„åˆIC
    aligned_df = pd.DataFrame({
        'factor': combined_factor,
        'target': target_values
    }).dropna()
    
    grouped = aligned_df.groupby(level='date')
    ic_series = grouped.apply(lambda x: x['factor'].corr(x['target'], method='spearman'))
    
    # ç¡®ä¿ ic_series æ˜¯ä¸€ç»´çš„ï¼Œå–æ ‡é‡å€¼
    if hasattr(ic_series, 'values'):
        ic_values = ic_series.values.flatten()
        combined_ic = float(np.nanmean(ic_values))
        combined_icir = float(np.nanmean(ic_values) / np.nanstd(ic_values) * np.sqrt(252)) if np.nanstd(ic_values) > 0 else 0.0
    else:
        combined_ic = float(ic_series) if not pd.isna(ic_series) else 0.0
        combined_icir = 0.0
    
    print(f"   ç»„åˆIC: {combined_ic:.4f}")
    print(f"   ç»„åˆICIR: {combined_icir:.2f}")
    
    if combined_ic < 0.03:
        print(f"   âš ï¸  ç»„åˆICåä½ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–")
    else:
        print(f"   âœ… ç»„åˆICæ˜¾è‘—")
    
    # æœ€ç»ˆéªŒæ”¶ç»“æœ
    print("\n" + "=" * 80)
    if acceptance_passed:
        print("ğŸ‰ éªŒæ”¶é€šè¿‡ï¼å› å­å·¥å‚ v1 æ„å»ºæˆåŠŸ")
    else:
        print("âš ï¸  éƒ¨åˆ†éªŒæ”¶æŒ‡æ ‡æœªè¾¾æ ‡ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    print("=" * 80)
    
    print(f"\nå®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    return qualified_factors_df, quality_reports, manager


if __name__ == "__main__":
    """è¿è¡Œå› å­å‡†å¤‡æµç¨‹"""
    
    # åŠ è½½é…ç½®æ–‡ä»¶è·å–å‚æ•°
    config = load_config("configs/ml_baseline.yml")
    
    # ä»é…ç½®æ–‡ä»¶è¯»å–è‚¡ç¥¨ä»£ç ï¼ˆçº¯æ•°å­—æ ¼å¼ï¼Œå¦‚ '000001'ï¼Œä¸æ˜¯ '000001.SZ'ï¼‰
    # å› ä¸º InfluxDB ä¸­å­˜å‚¨çš„è‚¡ç¥¨ä»£ç æ˜¯çº¯æ•°å­—æ ¼å¼
    tickers = config['data'].get('symbol', ['000001', '000002', '000063'])
    if isinstance(tickers, str):
        tickers = [tickers]
    
    # ä»é…ç½®æ–‡ä»¶è¯»å–æ—¥æœŸèŒƒå›´
    start_date = config['data'].get('start_date', '2018-01-01')
    end_date = config['data'].get('end_date', '2024-12-31')
    
    print(f"\nğŸ“‹ ä»é…ç½®æ–‡ä»¶è¯»å–å‚æ•°:")
    print(f"   è‚¡ç¥¨ä»£ç : {tickers}")
    print(f"   æ—¥æœŸèŒƒå›´: {start_date} ~ {end_date}")
    
    qualified_factors_df, quality_reports, manager = prepare_factors(
        config_path="configs/ml_baseline.yml",
        start_date=start_date,
        end_date=end_date,
        tickers=tickers
    )
    
    print("\nâœ… æµç¨‹æ‰§è¡Œå®Œæˆï¼")
