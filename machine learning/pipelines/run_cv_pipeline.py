#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—¶åºäº¤å‰éªŒè¯ç®¡é“ - ç»Ÿä¸€çš„ Purged+Embargo CV æµç¨‹

åŠŸèƒ½ï¼š
1. åŠ è½½é…ç½®å’Œæ•°æ®
2. åº”ç”¨ Purged + Embargo æ—¶é—´åˆ‡åˆ†
3. å¯é€‰ï¼šWalk-Forward å¤šæŠ˜éªŒè¯
4. å› å­æ¨ªæˆªé¢è¯„ä¼°ï¼ˆå„åˆ†å‰²ç‹¬ç«‹ï¼‰
5. æ¼‚ç§»æ£€æµ‹ä¸æŠ¥å‘Šç”Ÿæˆ

ä½¿ç”¨æ–¹æ³•ï¼š
    python run_cv_pipeline.py
    python run_cv_pipeline.py --config configs/ml_baseline.yml
    python run_cv_pipeline.py --wfa  # å¼ºåˆ¶ä½¿ç”¨ Walk-Forward

è¾“å‡ºï¼š
    /ML output/reports/baseline_v1/cv/
    â”œâ”€â”€ drift_report.json
    â”œâ”€â”€ drift_tearsheet.html
    â”œâ”€â”€ split_comparison.csv
    â””â”€â”€ fold_X_results.json (WFA æ¨¡å¼)

åˆ›å»º: 2025-12-02 | ç‰ˆæœ¬: v1.0
"""

import os
import pandas as pd
import sys
import yaml
import argparse
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
ml_root = os.path.dirname(current_dir)
if ml_root not in sys.path:
    sys.path.insert(0, ml_root)

from data.data_loader import DataLoader
from data.time_series_cv import TimeSeriesCV, create_cv_from_config
from data.market_data_loader import MarketDataLoader
from targets.label_transformer import create_forward_returns_with_transform
from evaluation.cross_section_analyzer import CrossSectionAnalyzer
from evaluation.cross_section_metrics import calculate_forward_returns
from evaluation.drift_detector import DriftDetector, compare_splits_with_analyzer
from features.factor_factory import FactorFactory


def load_config(config_path: str = "configs/ml_baseline.yml") -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if not os.path.isabs(config_path):
        config_path = os.path.join(ml_root, config_path)
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    return config


def run_single_split_cv(config: dict,
                        factors: 'pd.DataFrame',
                        forward_returns: 'pd.DataFrame',
                        output_dir: str) -> dict:
    """
    å•æ¬¡æ—¶é—´åˆ‡åˆ† + æ¼‚ç§»æ£€æµ‹
    
    Parameters:
    -----------
    config : dict
        é…ç½®å­—å…¸
    factors : pd.DataFrame
        å› å­æ•°æ®
    forward_returns : pd.DataFrame
        è¿œæœŸæ”¶ç›Š
    output_dir : str
        è¾“å‡ºç›®å½•
        
    Returns:
    --------
    dict
        ç»“æœæ±‡æ€»
    """
    print("\n" + "=" * 80)
    print("å•æ¬¡æ—¶é—´åˆ‡åˆ† CVï¼ˆPurged + Embargoï¼‰")
    print("=" * 80)
    
    # åˆ›å»º CV å®ä¾‹
    cv = TimeSeriesCV.from_config(config)
    
    # è·å–åˆ‡åˆ†ç´¢å¼•
    train_idx, valid_idx, test_idx = cv.single_split(factors)
    
    # éªŒè¯æ— æ³„æ¼
    target_horizon = config.get('target', {}).get('forward_periods', 5)
    cv.validate_no_leakage(train_idx, valid_idx, test_idx, target_horizon)
    
    # æ¼‚ç§»æ£€æµ‹ä¸åˆ†æ
    results = compare_splits_with_analyzer(
        factors=factors,
        forward_returns=forward_returns,
        train_idx=train_idx,
        valid_idx=valid_idx,
        test_idx=test_idx,
        output_dir=output_dir,
        drift_threshold=config.get('split', {}).get('drift_threshold', 0.2)
    )
    
    # ä¿å­˜åˆ‡åˆ†å…ƒæ•°æ®
    meta_path = os.path.join(output_dir, 'cv_meta.json')
    meta = cv.get_split_meta()
    meta['train_samples'] = len(train_idx)
    meta['valid_samples'] = len(valid_idx)
    meta['test_samples'] = len(test_idx)
    
    with open(meta_path, 'w', encoding='utf-8') as f:
        json.dump(meta, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“Š åˆ‡åˆ†å…ƒæ•°æ®å·²ä¿å­˜: {meta_path}")
    
    return results


def run_walk_forward_cv(config: dict,
                        factors: 'pd.DataFrame',
                        forward_returns: 'pd.DataFrame',
                        output_dir: str) -> dict:
    """
    Walk-Forward éªŒè¯
    
    Parameters:
    -----------
    config : dict
        é…ç½®å­—å…¸
    factors : pd.DataFrame
        å› å­æ•°æ®
    forward_returns : pd.DataFrame
        è¿œæœŸæ”¶ç›Š
    output_dir : str
        è¾“å‡ºç›®å½•
        
    Returns:
    --------
    dict
        ç»“æœæ±‡æ€»
    """
    print("\n" + "=" * 80)
    print("Walk-Forward éªŒè¯ï¼ˆPurged + Embargoï¼‰")
    print("=" * 80)
    
    # åˆ›å»º CV å®ä¾‹
    cv = TimeSeriesCV.from_config(config)
    
    # WFA é…ç½®
    wfa_config = config.get('split', {}).get('walk_forward', {})
    n_splits = wfa_config.get('n_splits', 5)
    min_train_days = wfa_config.get('min_train_days', 252)
    expanding = wfa_config.get('expanding', True)
    
    # æ”¶é›†æ‰€æœ‰æŠ˜çš„ç»“æœ
    all_fold_results = []
    all_oos_ic = []
    all_oos_spread = []
    
    detector = DriftDetector(
        drift_threshold=config.get('split', {}).get('drift_threshold', 0.2)
    )
    
    for fold, train_idx, valid_idx, test_idx in cv.walk_forward_split(
        factors, n_splits=n_splits, min_train_days=min_train_days, expanding=expanding
    ):
        fold_dir = os.path.join(output_dir, f'fold_{fold+1}')
        os.makedirs(fold_dir, exist_ok=True)
        
        # å„åˆ†å‰²åˆ†æ
        try:
            # Train
            train_analyzer = CrossSectionAnalyzer(
                factors=factors.loc[train_idx],
                forward_returns=forward_returns.loc[train_idx]
            )
            train_analyzer.analyze()
            
            # Valid
            valid_analyzer = CrossSectionAnalyzer(
                factors=factors.loc[valid_idx],
                forward_returns=forward_returns.loc[valid_idx]
            )
            valid_analyzer.analyze()
            
            # Test
            test_analyzer = CrossSectionAnalyzer(
                factors=factors.loc[test_idx],
                forward_returns=forward_returns.loc[test_idx]
            )
            test_analyzer.analyze()
            
            # æ”¶é›† OOS ç»“æœï¼ˆValid + Testï¼‰
            oos_ic = valid_analyzer.results.get('daily_ic', None)
            if oos_ic is not None:
                all_oos_ic.append(oos_ic)
            
            oos_ic_test = test_analyzer.results.get('daily_ic', None)
            if oos_ic_test is not None:
                all_oos_ic.append(oos_ic_test)
            
            # æ”¶é›† OOS Spread ç»“æœ
            oos_spread = valid_analyzer.results.get('spreads', None)
            if oos_spread is not None:
                all_oos_spread.append(oos_spread)
            
            oos_spread_test = test_analyzer.results.get('spreads', None)
            if oos_spread_test is not None:
                all_oos_spread.append(oos_spread_test)
            
            fold_results = {
                'fold': fold + 1,
                'train_samples': len(train_idx),
                'valid_samples': len(valid_idx),
                'test_samples': len(test_idx),
                'train_ic_summary': train_analyzer.results.get('ic_summary', {}),
                'valid_ic_summary': valid_analyzer.results.get('ic_summary', {}),
                'test_ic_summary': test_analyzer.results.get('ic_summary', {})
            }
            
            all_fold_results.append(fold_results)
            
        except Exception as e:
            print(f"   âš ï¸  Fold {fold+1} åˆ†æå¤±è´¥: {e}")
            continue
    
    # åˆå¹¶ OOS ç»“æœ
    print("\n" + "=" * 70)
    print("åˆå¹¶ OOS ç»“æœ")
    print("=" * 70)
    
    if all_oos_ic:
        import pandas as pd
        combined_oos_ic = pd.concat(all_oos_ic).groupby(level=0).mean()
        
        # è®¡ç®—åˆå¹¶åçš„ IC ç»Ÿè®¡
        from evaluation.cross_section_metrics import calculate_ic_summary
        
        combined_summary = {}
        for col in combined_oos_ic.columns:
            combined_summary[col] = calculate_ic_summary(combined_oos_ic[col])
        
        print(f"\nğŸ“Š åˆå¹¶ OOS IC ç»Ÿè®¡:")
        for key, summary in list(combined_summary.items())[:3]:
            print(f"   {key}:")
            print(f"      Mean IC: {summary['mean']:.4f}")
            print(f"      ICIR: {summary['icir']:.4f}")
            print(f"      ICIR(å¹´åŒ–): {summary['icir_annual']:.4f}")
    
    # åˆå¹¶ OOS Spread ç»“æœ
    if all_oos_spread:
        # spreads æ˜¯å­—å…¸ {(factor, period): Series}ï¼Œéœ€è¦æŒ‰ key åˆå¹¶
        combined_spread_stats = {}
        for spread_dict in all_oos_spread:
            for key, spread_series in spread_dict.items():
                if key not in combined_spread_stats:
                    combined_spread_stats[key] = []
                combined_spread_stats[key].append(spread_series)
        
        print(f"\nğŸ“Š åˆå¹¶ OOS Spread ç»Ÿè®¡:")
        for key, series_list in list(combined_spread_stats.items())[:3]:
            combined = pd.concat(series_list)
            mean_spread = combined.mean()
            std_spread = combined.std()
            sharpe = mean_spread / std_spread if std_spread != 0 else 0
            print(f"   {key}: Mean={mean_spread:.4f}, Std={std_spread:.4f}, Sharpe={sharpe:.4f}")
    
    # ä¿å­˜ WFA ç»“æœ
    wfa_results = {
        'mode': 'walk_forward',
        'n_folds': len(all_fold_results),
        'config': {
            'n_splits': n_splits,
            'min_train_days': min_train_days,
            'expanding': expanding
        },
        'folds': all_fold_results
    }
    
    results_path = os.path.join(output_dir, 'wfa_results.json')
    
    # è½¬æ¢ numpy ç±»å‹
    def convert_to_native(obj):
        import numpy as np
        if isinstance(obj, (np.integer, np.floating)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_to_native(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_native(i) for i in obj]
        elif isinstance(obj, tuple):
            return tuple(convert_to_native(i) for i in obj)
        return obj
    
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(convert_to_native(wfa_results), f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“Š WFA ç»“æœå·²ä¿å­˜: {results_path}")
    
    return wfa_results


def main(config_path: str = "configs/ml_baseline.yml",
         force_wfa: bool = False):
    """
    ä¸»å‡½æ•°
    
    Parameters:
    -----------
    config_path : str
        é…ç½®æ–‡ä»¶è·¯å¾„
    force_wfa : bool
        å¼ºåˆ¶ä½¿ç”¨ Walk-Forward æ¨¡å¼
    """
    print("=" * 80)
    print("æ—¶åºäº¤å‰éªŒè¯ç®¡é“")
    print("=" * 80)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # 1. åŠ è½½é…ç½®
    print("\n" + "=" * 80)
    print("æ­¥éª¤ 1: åŠ è½½é…ç½®")
    print("=" * 80)
    
    config = load_config(config_path)
    
    project_name = config.get('project', {}).get('name', 'baseline_v1')
    cv_mode = config.get('split', {}).get('cv_mode', 'single_split')
    
    if force_wfa:
        cv_mode = 'walk_forward'
    
    print(f"   é¡¹ç›®: {project_name}")
    print(f"   CV æ¨¡å¼: {cv_mode}")
    
    # 2. åŠ è½½æ•°æ®
    print("\n" + "=" * 80)
    print("æ­¥éª¤ 2: åŠ è½½æ•°æ®")
    print("=" * 80)
    
    influxdb_config = config['data']['influxdb']
    tickers = config['data'].get('symbol', ['000001'])
    if isinstance(tickers, str):
        tickers = [tickers]
    
    start_date = config['data'].get('start_date', '2018-01-01')
    end_date = config['data'].get('end_date', '2024-12-31')
    
    market_loader = MarketDataLoader(
        url=influxdb_config['url'],
        token=influxdb_config['token'],
        org=influxdb_config['org'],
        bucket=influxdb_config['bucket']
    )
    
    market_data = market_loader.load_market_data_batch(
        symbols=tickers,
        start_date=start_date,
        end_date=end_date
    )
    
    if market_data.empty:
        raise ValueError("æœªåŠ è½½åˆ°å¸‚åœºæ•°æ®")
    
    print(f"   æ•°æ®å½¢çŠ¶: {market_data.shape}")
    print(f"   æ—¥æœŸèŒƒå›´: {market_data.index.get_level_values('date').min()} ~ {market_data.index.get_level_values('date').max()}")
    
    # 3. ç”Ÿæˆå› å­
    print("\n" + "=" * 80)
    print("æ­¥éª¤ 3: ç”Ÿæˆå› å­")
    print("=" * 80)
    
    factory = FactorFactory()
    
    # ç”Ÿæˆä¸€äº›åŸºç¡€å› å­
    factors_list = []
    
    # åŠ¨é‡å› å­
    roc_factors = factory.calc_roc_family(market_data, periods=[5, 20, 60])
    factors_list.append(roc_factors)
    
    # æ³¢åŠ¨ç‡å› å­
    vol_factors = factory.calc_realized_volatility(market_data, periods=[20])
    factors_list.append(vol_factors)
    
    import pandas as pd
    factors = pd.concat(factors_list, axis=1).dropna()
    
    print(f"   å› å­æ•°é‡: {factors.shape[1]}")
    print(f"   æ ·æœ¬æ•°é‡: {len(factors)}")
    
    # 4. è®¡ç®—è¿œæœŸæ”¶ç›Š
    print("\n" + "=" * 80)
    print("æ­¥éª¤ 4: è®¡ç®—è¿œæœŸæ”¶ç›Š")
    print("=" * 80)
    
    target_config = config.get('target', {})
    periods = [target_config.get('forward_periods', 5)]
    method = target_config.get('return_type', 'simple')
    transform = target_config.get('transform', 'none')
    
    forward_returns = calculate_forward_returns(
        market_data[['close']],
        periods=periods,
        method=method
    )
    
    # å¯¹é½å› å­å’Œæ”¶ç›Š
    common_idx = factors.index.intersection(forward_returns.index)
    factors = factors.loc[common_idx]
    forward_returns = forward_returns.loc[common_idx]
    
    print(f"   æ”¶ç›Šå‘¨æœŸ: {periods}")
    print(f"   æ”¶ç›Šç±»å‹: {method}")
    print(f"   å¯¹é½åæ ·æœ¬: {len(factors)}")
    
    # 5. è¿è¡Œ CV
    output_dir = os.path.join(ml_root, 'ML output', 'reports', project_name, 'cv')
    os.makedirs(output_dir, exist_ok=True)
    
    if cv_mode == 'walk_forward':
        results = run_walk_forward_cv(config, factors, forward_returns, output_dir)
    else:
        results = run_single_split_cv(config, factors, forward_returns, output_dir)
    
    # 6. å®Œæˆ
    print("\n" + "=" * 80)
    print("å®Œæˆ")
    print("=" * 80)
    print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    
    return results


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='æ—¶åºäº¤å‰éªŒè¯ç®¡é“')
    parser.add_argument('--config', type=str, default='configs/ml_baseline.yml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--wfa', action='store_true',
                       help='å¼ºåˆ¶ä½¿ç”¨ Walk-Forward æ¨¡å¼')
    
    args = parser.parse_args()
    
    try:
        main(config_path=args.config, force_wfa=args.wfa)
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
