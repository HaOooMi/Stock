#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®æ¸…æ´—ä¸å¿«ç…§å±‚ - é›†æˆç¤ºä¾‹

å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ–°çš„æ•°æ®æ¸…æ´—å’Œå¿«ç…§åŠŸèƒ½

åŠŸèƒ½æµç¨‹ï¼š
1. åŠ è½½åŸå§‹æ•°æ®
2. åº”ç”¨äº¤æ˜“å¯è¡Œæ€§è¿‡æ»¤ï¼ˆ7å±‚ï¼‰
3. PITå¯¹é½éªŒè¯
4. åˆ›å»ºæ•°æ®å¿«ç…§
5. ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š
"""

import os
import sys
import pandas as pd
import yaml
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
current_dir = os.path.dirname(os.path.abspath(__file__))
ml_root = os.path.dirname(current_dir)
project_root = os.path.dirname(ml_root)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# æ·»åŠ  machine learning ç›®å½•åˆ°è·¯å¾„
if ml_root not in sys.path:
    sys.path.insert(0, ml_root)

from data.data_loader import DataLoader
from features.feature_engineering import FeatureEngineer
from targets.target_engineering import TargetEngineer


def load_config(config_path: str = "configs/ml_baseline.yml"):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_file = os.path.join(ml_root, config_path)
    with open(config_file, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("æ•°æ®æ¸…æ´—ä¸å¿«ç…§å±‚ - æ‰¹é‡å¤„ç†")
    print("=" * 70)
    print(f"è¿è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    
    # 1. åŠ è½½é…ç½®
    print("\n[æ­¥éª¤1] åŠ è½½é…ç½®")
    config = load_config()
    
    # æ”¯æŒå•æ ‡çš„å’Œå¤šæ ‡çš„
    config_symbol = config['data']['symbol']
    if isinstance(config_symbol, list):
        symbols = config_symbol
        is_multi = True
    else:
        symbols = [config_symbol]
        is_multi = False
    
    start_date = config['data']['start_date']
    end_date = config['data']['end_date']
    target_col = config['target']['name']
    random_seed = config['runtime']['random_seed']
    
    mode_name = "å¤šæ ‡çš„" if is_multi else "å•æ ‡çš„"
    print(f"   æ¨¡å¼: {mode_name}")
    print(f"   è‚¡ç¥¨ä»£ç : {symbols}")
    print(f"   è‚¡ç¥¨æ•°é‡: {len(symbols)}")
    print(f"   æ—¶é—´èŒƒå›´: {start_date} ~ {end_date}")
    print(f"   ç›®æ ‡å˜é‡: {target_col}")
    print(f"   éšæœºç§å­: {random_seed}")
    
    # 2. åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨ï¼ˆå¯ç”¨æ‰€æœ‰åŠŸèƒ½ï¼‰
    print("\n[æ­¥éª¤2] åˆå§‹åŒ–å¢å¼ºç‰ˆæ•°æ®åŠ è½½å™¨")
    
    # æå–è¿‡æ»¤å™¨é…ç½®
    filter_config = {
        'min_volume': config['data']['universe']['min_volume'],
        'min_amount': config['data']['universe']['min_amount'],
        'min_price': config['data']['universe']['min_price'],
        'min_turnover': config['data']['universe']['min_turnover'],
        'min_listing_days': config['data']['universe']['min_listing_days'],
        'exclude_st': config['data']['universe']['exclude_st'],
        'exclude_limit_moves': config['data']['universe']['exclude_limit_moves'],
        'limit_threshold': config['data']['universe']['limit_threshold']
    }
    
    # ä»é…ç½®æ–‡ä»¶è¯»å– InfluxDB é…ç½®
    influxdb_enabled = config['data'].get('influxdb', {}).get('enabled', True)
    influxdb_config = None
    if influxdb_enabled:
        influxdb_config = {
            'url': config['data']['influxdb']['url'],
            'org': config['data']['influxdb']['org'],
            'bucket': config['data']['influxdb']['bucket'],
            'token': config['data']['influxdb']['token']
        }
    
    loader = DataLoader(
        data_root=os.path.join(ml_root, "ML output/datasets/baseline_v1"),
        enable_snapshot=config['data']['snapshot']['enabled'],
        enable_filtering=True,
        enable_pit_alignment=config['data']['pit']['enabled'],
        enable_influxdb=influxdb_enabled,
        influxdb_config=influxdb_config,
        filter_config=filter_config
    )
    
    # 3. æ‰¹é‡å¤„ç†æ‰€æœ‰è‚¡ç¥¨
    print(f"\n[æ­¥éª¤3] æ‰¹é‡å¤„ç† {len(symbols)} åªè‚¡ç¥¨")
    print("=" * 70)
    
    success_count = 0
    failed_symbols = []
    
    for idx, symbol in enumerate(symbols, 1):
        print(f"\n{'='*70}")
        print(f"å¤„ç†è‚¡ç¥¨ [{idx}/{len(symbols)}]: {symbol}")
        print(f"{'='*70}")
        
        try:
            # æ­¥éª¤3.1: ç‰¹å¾å·¥ç¨‹
            print(f"\n[æ­¥éª¤3.1] ç‰¹å¾å·¥ç¨‹")
            feature_engineer = FeatureEngineer(use_talib=True, use_tsfresh=False)
            
            # åŠ è½½åŸå§‹æ•°æ®
            raw_data = feature_engineer.load_stock_data(symbol, start_date, end_date)
            if raw_data is None or len(raw_data) == 0:
                print(f"   âš ï¸ è·³è¿‡ {symbol}ï¼šæ— åŸå§‹æ•°æ®")
                failed_symbols.append(symbol)
                continue
            
            print(f"   åŸå§‹æ•°æ®: {raw_data.shape}")
            
            # ç”ŸæˆæŠ€æœ¯ç‰¹å¾
            features_df = feature_engineer.prepare_features(
                raw_data,
                use_auto_features=False,
                keep_base_columns=True
            )
            print(f"   âœ… ç‰¹å¾ç”Ÿæˆå®Œæˆ: {features_df.shape[1]} ä¸ªç‰¹å¾, {features_df.shape[0]} ä¸ªæ ·æœ¬")
            
            # æ­¥éª¤3.1.1: åº”ç”¨äº¤æ˜“å¯è¡Œæ€§è¿‡æ»¤ï¼ˆåœ¨æ ‡å‡†åŒ–ä¹‹å‰ï¼‰
            print(f"\n[æ­¥éª¤3.1.1] åº”ç”¨äº¤æ˜“å¯è¡Œæ€§è¿‡æ»¤å™¨ï¼ˆåŸºäºåŸå§‹å€¼ï¼‰")
            if loader.filter_engine:
                filtered_features, filter_log = loader.filter_engine.apply_filters(
                    features_df,
                    save_log=True,
                    log_path=os.path.join(
                        config['paths'].get('datasets_dir', 'ML output/datasets/baseline_v1'),
                        f"filter_log_{symbol}.csv"
                    ) if not os.path.isabs(config['paths'].get('datasets_dir', 'ML output/datasets/baseline_v1')) 
                      else os.path.join(ml_root, config['paths'].get('datasets_dir', 'ML output/datasets/baseline_v1'), f"filter_log_{symbol}.csv")
                )
                
                # åªä¿ç•™å¯äº¤æ˜“çš„æ ·æœ¬
                features_df = filtered_features[filtered_features['tradable_flag'] == 1].copy()
                features_df = features_df.drop(columns=['tradable_flag'], errors='ignore')
                
                print(f"\n   âœ… äº¤æ˜“è¿‡æ»¤å®Œæˆ:")
                print(f"      è¿‡æ»¤å‰: {len(filtered_features)} ä¸ªæ ·æœ¬")
                print(f"      è¿‡æ»¤å: {len(features_df)} ä¸ªå¯äº¤æ˜“æ ·æœ¬")
                print(f"      å‰”é™¤: {len(filtered_features) - len(features_df)} ä¸ªæ ·æœ¬ ({(len(filtered_features) - len(features_df))/len(filtered_features):.1%})")
                
                if len(features_df) == 0:
                    print(f"\n      âš ï¸  è­¦å‘Š: æ‰€æœ‰æ ·æœ¬å‡è¢«è¿‡æ»¤ï¼Œè·³è¿‡ {symbol}")
                    failed_symbols.append(symbol)
                    continue
            else:
                print(f"   â­ï¸  è·³è¿‡äº¤æ˜“è¿‡æ»¤ï¼ˆæœªå¯ç”¨ï¼‰")
            
            # æ­¥éª¤3.2: ç‰¹å¾é€‰æ‹©ï¼ˆå¯é€‰ï¼‰
            skip_selection = config.get('features', {}).get('skip_selection', False)
            
            if skip_selection:
                print(f"\n[æ­¥éª¤3.2] â­ï¸  è·³è¿‡ç‰¹å¾é€‰æ‹©ï¼ˆé…ç½®æ–‡ä»¶è®¾ç½®ï¼‰")
                selected_features = features_df
                final_feature_count = len([c for c in features_df.columns 
                                          if c not in ['close', 'volume', 'amount', 'pct_change', 'turnover']])
            else:
                print(f"\n[æ­¥éª¤3.2] ç‰¹å¾é€‰æ‹©")
                
                # ä»é…ç½®æ–‡ä»¶è¯»å–ç‰¹å¾é€‰æ‹©å‚æ•°
                final_k = config.get('features', {}).get('final_k', 20)
                variance_threshold = config.get('features', {}).get('variance_threshold', 0.01)
                correlation_threshold = config.get('features', {}).get('correlation_threshold', 0.9)
                
                selection_results = feature_engineer.select_features(
                    features_df,
                    final_k=final_k,
                    variance_threshold=variance_threshold,
                    correlation_threshold=correlation_threshold,
                    train_ratio=0.8
                )
                selected_features = selection_results['final_features_df']
                final_feature_count = len(selection_results['final_features'])
                print(f"   âœ… ç‰¹å¾é€‰æ‹©å®Œæˆ: {final_feature_count} ä¸ªç‰¹å¾")
            
            # æ­¥éª¤3.3: ç‰¹å¾æ ‡å‡†åŒ–
            print(f"\n[æ­¥éª¤3.3] ç‰¹å¾æ ‡å‡†åŒ–")
            scalers_dir = config['paths'].get('scalers_dir', 'ML output/scalers/baseline_v1')
            if not os.path.isabs(scalers_dir):
                scalers_dir = os.path.join(ml_root, scalers_dir)
            os.makedirs(scalers_dir, exist_ok=True)
            
            scaler_path = os.path.join(scalers_dir, f"scaler_{symbol}.pkl")
            scale_results = feature_engineer.scale_features(
                selected_features,
                scaler_type='robust',
                train_ratio=0.8,
                save_path=scaler_path
            )
            scaled_features = scale_results['scaled_df']
            print(f"   âœ… ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ")
            print(f"   ğŸ’¾ æ ‡å‡†åŒ–å™¨: {scaler_path}")
            
            # æ­¥éª¤3.4: ç›®æ ‡å·¥ç¨‹
            print(f"\n[æ­¥éª¤3.4] ç›®æ ‡å·¥ç¨‹")
            datasets_dir = config['paths'].get('datasets_dir', 'ML output/datasets/baseline_v1')
            if not os.path.isabs(datasets_dir):
                datasets_dir = os.path.join(ml_root, datasets_dir)
            os.makedirs(datasets_dir, exist_ok=True)
            
            target_engineer = TargetEngineer(data_dir=datasets_dir)
            complete_df = target_engineer.create_complete_dataset(
                features_df=scaled_features,
                periods=[1, 5, 10],
                price_col='close',
                include_labels=True,
                label_types=['binary']
            )
            
            print(f"   âœ… ç›®æ ‡å˜é‡ç”Ÿæˆå®Œæˆ: {complete_df.shape}")
            
            # ä¸´æ—¶ä¿å­˜å®Œæ•´æ•°æ®é›†ï¼ˆç”¨äºåç»­çš„æ•°æ®è´¨é‡æ£€æŸ¥ï¼‰
            temp_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            temp_csv = f"with_targets_{symbol}_complete_{temp_timestamp}.csv"
            temp_csv_path = os.path.join(datasets_dir, temp_csv)
            complete_df.to_csv(temp_csv_path)
            print(f"   ğŸ’¾ ä¸´æ—¶ä¿å­˜: {temp_csv}")
            
            # æ­¥éª¤3.4: æ•°æ®è´¨é‡æ£€æŸ¥å’Œå¿«ç…§
            print(f"\n[æ­¥éª¤3.4] æ•°æ®è´¨é‡æ£€æŸ¥å’Œå¿«ç…§")
            # æ³¨æ„ï¼šè¿‡æ»¤å·²åœ¨æ­¥éª¤3.1.1å®Œæˆï¼Œè¿™é‡Œä¸å†é‡å¤è¿‡æ»¤
            features, targets, snapshot_id = loader.load_with_snapshot(
                symbol=symbol,
                start_date=start_date,
                end_date=end_date,
                target_col=target_col,
                use_scaled=config['features']['use_scaled_features'],
                filters=None,  # ä¸å†åº”ç”¨è¿‡æ»¤å™¨ï¼ˆå·²åœ¨æ ‡å‡†åŒ–å‰å®Œæˆï¼‰
                random_seed=random_seed,
                save_parquet=config['data']['snapshot']['save_parquet']
            )
            
            print(f"\nâœ… æ•°æ®åŠ è½½æˆåŠŸ!")
            print(f"   å¿«ç…§ID: {snapshot_id}")
            print(f"   ç‰¹å¾å½¢çŠ¶: {features.shape}")
            print(f"   ç›®æ ‡å½¢çŠ¶: {targets.shape}")
            
            # 4. å±•ç¤ºæ•°æ®è´¨é‡ç»Ÿè®¡
            print("\n[æ­¥éª¤4] æ•°æ®è´¨é‡ç»Ÿè®¡")
            print(f"   ç‰¹å¾ç¼ºå¤±ç‡: {features.isna().sum().sum() / features.size:.2%}")
            print(f"   ç›®æ ‡ç¼ºå¤±ç‡: {targets.isna().sum() / len(targets):.2%}")
            print(f"   æ—¶é—´èŒƒå›´: {features.index.get_level_values('date').min().date()} ~ "
                  f"{features.index.get_level_values('date').max().date()}")
            
            # 5. å¿«ç…§ä¿¡æ¯
            if snapshot_id and loader.snapshot_mgr:
                # è´¨é‡æŠ¥å‘Šä½ç½®
                quality_report_path = os.path.join(
                    loader.snapshot_mgr.quality_reports_dir,
                    f"{snapshot_id}.json"
                )
            
            # 6. éªŒæ”¶æ£€æŸ¥
            print("\n[æ­¥éª¤5] æ•°æ®éªŒæ”¶æ£€æŸ¥")
            
            # æ£€æŸ¥1: å¯äº¤æ˜“æ ·æœ¬è§„æ¨¡
            n_samples = len(features)
            min_samples = 200  # æ ¹æ®å®ªç« è¦æ±‚
            sample_check = n_samples >= min_samples
            print(f"   {'âœ…' if sample_check else 'âŒ'} æ ·æœ¬è§„æ¨¡: {n_samples} (æœ€ä½ {min_samples})")
            
            # æ£€æŸ¥2: PITå¯¹é½
            if loader.pit_aligner:
                combined = features.copy()
                combined[target_col] = targets
                pit_results = loader.pit_aligner.validate_pit_alignment(combined, target_col)
                pit_check = pit_results.get('overall_pass', False)
                print(f"   {'âœ…' if pit_check else 'âŒ'} PITå¯¹é½éªŒè¯")
            else:
                pit_check = True
                print(f"   âš ï¸  PITå¯¹é½éªŒè¯ï¼ˆæœªå¯ç”¨ï¼‰")
            
            # æ£€æŸ¥3: æ•°æ®è´¨é‡
            if snapshot_id and loader.snapshot_mgr:
                import json
                with open(quality_report_path, 'r', encoding='utf-8') as f:
                    quality_report = json.load(f)
                quality_check = quality_report.get('overall_quality') == 'PASS'
                red_flags = quality_report.get('red_flags_count', 0)
                print(f"   {'âœ…' if quality_check else 'âŒ'} æ•°æ®è´¨é‡: {quality_report.get('overall_quality')} ({red_flags} ä¸ªçº¢ç¯)")
            else:
                quality_check = True
                print(f"   âš ï¸  æ•°æ®è´¨é‡æ£€æŸ¥ï¼ˆæœªå¯ç”¨å¿«ç…§ï¼‰")
            
            # æ€»ä½“éªŒæ”¶
            all_passed = sample_check and pit_check and quality_check
            
            if all_passed:
                # 7. ä¿å­˜CSVæ ¼å¼æ•°æ®é›†ï¼ˆç”¨äºåç»­ train_models.pyï¼‰
                print("\n[æ­¥éª¤6] ä¿å­˜CSVæ ¼å¼æ•°æ®é›†ï¼ˆç”¨äºæ¨¡å‹è®­ç»ƒï¼‰")
                datasets_dir = config['paths'].get('datasets_dir', 'ML output/datasets/baseline_v1')
                if not os.path.isabs(datasets_dir):
                    datasets_dir = os.path.join(ml_root, datasets_dir)
                os.makedirs(datasets_dir, exist_ok=True)
                
                # åˆå¹¶ç‰¹å¾å’Œç›®æ ‡
                complete_df = features.copy()
                complete_df[target_col] = targets
                
                # ä¿å­˜ä¸ºCSV
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                csv_filename = f"with_targets_{symbol}_complete_{timestamp}.csv"
                csv_path = os.path.join(datasets_dir, csv_filename)
                complete_df.to_csv(csv_path)
                
                print(f"   âœ… CSVæ•°æ®é›†å·²ä¿å­˜: {csv_path}")
                print(f"   ğŸ“Š å½¢çŠ¶: {complete_df.shape}")
                
                success_count += 1
                print(f"\nâœ… {symbol} å¤„ç†æˆåŠŸï¼")
            else:
                print(f"\nâš ï¸ {symbol} éªŒæ”¶å¤±è´¥ï¼Œè·³è¿‡ä¿å­˜")
                failed_symbols.append(symbol)
            
        except Exception as e:
            print(f"\nâŒ {symbol} å¤„ç†å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            failed_symbols.append(symbol)
            continue
    
    # 4. æœ€ç»ˆç»Ÿè®¡
    print("\n" + "=" * 70)
    print("æ‰¹é‡å¤„ç†å®Œæˆ")
    print("=" * 70)
    print(f"âœ… æˆåŠŸ: {success_count}/{len(symbols)}")
    if failed_symbols:
        print(f"âŒ å¤±è´¥: {len(failed_symbols)}/{len(symbols)}")
        print(f"   å¤±è´¥è‚¡ç¥¨: {failed_symbols}")
    print("=" * 70)
    
    return 0 if success_count == len(symbols) else 1


if __name__ == "__main__":
    exit(main())
