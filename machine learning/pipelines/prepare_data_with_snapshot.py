#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®æ¸…æ´—ä¸å¿«ç…§å±‚ - é›†æˆç¤ºä¾‹

å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ–°çš„æ•°æ®æ¸…æ´—å’Œå¿«ç…§åŠŸèƒ½

åŠŸèƒ½æµç¨‹ï¼š
1. åŠ è½½åŸå§‹æ•°æ®
2. åº”ç”¨äº¤æ˜“å¯è¡Œæ€§è¿‡æ»¤ï¼ˆ7å±‚ï¼‰
3. PITå¯¹é½éªŒè¯
4. åˆ›å»ºæ•°æ®å¿«ç…§
5. ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š
"""

import os
import sys
import pandas as pd
import yaml
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
current_dir = os.path.dirname(os.path.abspath(__file__))
ml_root = os.path.dirname(current_dir)
project_root = os.path.dirname(ml_root)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# æ·»åŠ  machine learning ç›®å½•åˆ°è·¯å¾„
if ml_root not in sys.path:
    sys.path.insert(0, ml_root)

from data.data_loader import DataLoader
from features.feature_engineering import FeatureEngineer
from targets.target_engineering import TargetEngineer


def load_config(config_path: str = "configs/ml_baseline.yml"):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_file = os.path.join(ml_root, config_path)
    with open(config_file, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("æ•°æ®æ¸…æ´—ä¸å¿«ç…§å±‚ - æ‰¹é‡å¤„ç†")
    print("=" * 70)
    print(f"è¿è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    
    # 1. åŠ è½½é…ç½®
    print("\n[æ­¥éª¤1] åŠ è½½é…ç½®")
    config = load_config()
    
    # æ”¯æŒå•æ ‡çš„å’Œå¤šæ ‡çš„
    config_symbol = config['data']['symbol']
    if isinstance(config_symbol, list):
        symbols = config_symbol
        is_multi = True
    else:
        symbols = [config_symbol]
        is_multi = False
    
    start_date = config['data']['start_date']
    end_date = config['data']['end_date']
    target_col = config['target']['name']
    random_seed = config['runtime']['random_seed']
    
    mode_name = "å¤šæ ‡çš„" if is_multi else "å•æ ‡çš„"
    print(f"   æ¨¡å¼: {mode_name}")
    print(f"   è‚¡ç¥¨ä»£ç : {symbols}")
    print(f"   è‚¡ç¥¨æ•°é‡: {len(symbols)}")
    print(f"   æ—¶é—´èŒƒå›´: {start_date} ~ {end_date}")
    print(f"   ç›®æ ‡å˜é‡: {target_col}")
    print(f"   éšæœºç§å­: {random_seed}")
    
    # 2. åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨ï¼ˆå¯ç”¨æ‰€æœ‰åŠŸèƒ½ï¼‰
    print("\n[æ­¥éª¤2] åˆå§‹åŒ–å¢å¼ºç‰ˆæ•°æ®åŠ è½½å™¨")
    
    # æå–è¿‡æ»¤å™¨é…ç½®
    filter_config = {
        'min_volume': config['data']['universe']['min_volume'],
        'min_amount': config['data']['universe']['min_amount'],
        'min_price': config['data']['universe']['min_price'],
        'min_turnover': config['data']['universe']['min_turnover'],
        'min_listing_days': config['data']['universe']['min_listing_days'],
        'exclude_st': config['data']['universe']['exclude_st'],
        'exclude_limit_moves': config['data']['universe']['exclude_limit_moves'],
        'limit_threshold': config['data']['universe']['limit_threshold']
    }
    
    # ä»é…ç½®æ–‡ä»¶è¯»å– InfluxDB é…ç½®
    influxdb_enabled = config['data'].get('influxdb', {}).get('enabled', True)
    influxdb_config = None
    if influxdb_enabled:
        influxdb_config = {
            'url': config['data']['influxdb']['url'],
            'org': config['data']['influxdb']['org'],
            'bucket': config['data']['influxdb']['bucket'],
            'token': config['data']['influxdb']['token']
        }
    
    # æå– PIT é…ç½®
    pit_config = {
        'financial_lag_days': config['data']['pit'].get('financial_lag_days', 90),
        'financial_ffill_limit': config['data']['pit'].get('financial_ffill_limit', 95)
    }
    
    loader = DataLoader(
        data_root=os.path.join(ml_root, "ML output/datasets/baseline_v1"),
        enable_snapshot=config['data']['snapshot']['enabled'],
        enable_filtering=True,
        enable_pit_alignment=config['data']['pit']['enabled'],
        enable_influxdb=influxdb_enabled,
        influxdb_config=influxdb_config,
        filter_config=filter_config,
        pit_config=pit_config
    )
    
    # 3. æ‰¹é‡å¤„ç†æ‰€æœ‰è‚¡ç¥¨
    print(f"\n[æ­¥éª¤3] æ‰¹é‡å¤„ç† {len(symbols)} åªè‚¡ç¥¨")
    print("=" * 70)
    
    success_count = 0
    failed_symbols = []
    
    for idx, symbol in enumerate(symbols, 1):
        print(f"\n{'='*70}")
        print(f"å¤„ç†è‚¡ç¥¨ [{idx}/{len(symbols)}]: {symbol}")
        print(f"{'='*70}")
        
        try:
            # æ­¥éª¤3.1: ç‰¹å¾å·¥ç¨‹
            print(f"\n[æ­¥éª¤3.1] ç‰¹å¾å·¥ç¨‹")
            feature_engineer = FeatureEngineer(use_talib=True, use_tsfresh=False)
            
            # åŠ è½½åŸå§‹æ•°æ®
            raw_data = feature_engineer.load_stock_data(symbol, start_date, end_date)
            if raw_data is None or len(raw_data) == 0:
                print(f"   âš ï¸ è·³è¿‡ {symbol}ï¼šæ— åŸå§‹æ•°æ®")
                failed_symbols.append(symbol)
                continue
            
            print(f"   åŸå§‹æ•°æ®: {raw_data.shape}")
            
            # ç”ŸæˆæŠ€æœ¯ç‰¹å¾
            features_df = feature_engineer.prepare_features(
                raw_data,
                use_auto_features=False,
                keep_base_columns=True
            )
            print(f"   âœ… ç‰¹å¾ç”Ÿæˆå®Œæˆ: {features_df.shape[1]} ä¸ªç‰¹å¾, {features_df.shape[0]} ä¸ªæ ·æœ¬")
            
            # æ­¥éª¤3.1.1: åº”ç”¨äº¤æ˜“å¯è¡Œæ€§è¿‡æ»¤ï¼ˆåœ¨æ ‡å‡†åŒ–ä¹‹å‰ï¼‰
            print(f"\n[æ­¥éª¤3.1.1] åº”ç”¨äº¤æ˜“å¯è¡Œæ€§è¿‡æ»¤å™¨ï¼ˆåŸºäºåŸå§‹å€¼ï¼‰")
            if loader.filter_engine:
                # æ„é€  filter_log è·¯å¾„
                datasets_dir = config['paths'].get('datasets_dir', 'ML output/datasets/baseline_v1')
                if not os.path.isabs(datasets_dir):
                    datasets_dir = os.path.join(ml_root, datasets_dir)
                
                filter_log_path = os.path.join(datasets_dir, f"filter_log_{symbol}.csv")
                
                filtered_features, filter_log = loader.filter_engine.apply_filters(
                    features_df,
                    save_log=True,
                    log_path=filter_log_path
                )
                
                # åªä¿ç•™å¯äº¤æ˜“çš„æ ·æœ¬
                features_df = filtered_features[filtered_features['tradable_flag'] == 1].copy()
                features_df = features_df.drop(columns=['tradable_flag'], errors='ignore')
                
                print(f"\n   âœ… äº¤æ˜“è¿‡æ»¤å®Œæˆ:")
                print(f"      è¿‡æ»¤å‰: {len(filtered_features)} ä¸ªæ ·æœ¬")
                print(f"      è¿‡æ»¤å: {len(features_df)} ä¸ªå¯äº¤æ˜“æ ·æœ¬")
                print(f"      å‰”é™¤: {len(filtered_features) - len(features_df)} ä¸ªæ ·æœ¬ ({(len(filtered_features) - len(features_df))/len(filtered_features):.1%})")
                
                if len(features_df) == 0:
                    print(f"\n      âš ï¸  è­¦å‘Š: æ‰€æœ‰æ ·æœ¬å‡è¢«è¿‡æ»¤ï¼Œè·³è¿‡ {symbol}")
                    failed_symbols.append(symbol)
                    continue
            else:
                print(f"   â­ï¸  è·³è¿‡äº¤æ˜“è¿‡æ»¤ï¼ˆæœªå¯ç”¨ï¼‰")
            
            # æ­¥éª¤3.2: ç‰¹å¾é€‰æ‹©ï¼ˆå¯é€‰ï¼‰
            skip_selection = config.get('features', {}).get('skip_selection', False)
            
            if skip_selection:
                print(f"\n[æ­¥éª¤3.2] â­ï¸  è·³è¿‡ç‰¹å¾é€‰æ‹©ï¼ˆé…ç½®æ–‡ä»¶è®¾ç½®ï¼‰")
                selected_features = features_df
                final_feature_count = len([c for c in features_df.columns 
                                          if c not in ['close', 'volume', 'amount', 'pct_change', 'turnover']])
            else:
                print(f"\n[æ­¥éª¤3.2] ç‰¹å¾é€‰æ‹©")
                
                # ä»é…ç½®æ–‡ä»¶è¯»å–ç‰¹å¾é€‰æ‹©å‚æ•°
                final_k = config.get('features', {}).get('final_k', 20)
                variance_threshold = config.get('features', {}).get('variance_threshold', 0.01)
                correlation_threshold = config.get('features', {}).get('correlation_threshold', 0.9)
                
                selection_results = feature_engineer.select_features(
                    features_df,
                    final_k=final_k,
                    variance_threshold=variance_threshold,
                    correlation_threshold=correlation_threshold,
                    train_ratio=0.8
                )
                selected_features = selection_results['final_features_df']
                final_feature_count = len(selection_results['final_features'])
                print(f"   âœ… ç‰¹å¾é€‰æ‹©å®Œæˆ: {final_feature_count} ä¸ªç‰¹å¾")
            
            # æ­¥éª¤3.3: ç‰¹å¾æ ‡å‡†åŒ–
            print(f"\n[æ­¥éª¤3.3] ç‰¹å¾æ ‡å‡†åŒ–")
            scalers_dir = config['paths'].get('scalers_dir', 'ML output/scalers/baseline_v1')
            if not os.path.isabs(scalers_dir):
                scalers_dir = os.path.join(ml_root, scalers_dir)
            os.makedirs(scalers_dir, exist_ok=True)
            
            scaler_path = os.path.join(scalers_dir, f"scaler_{symbol}.pkl")
            scale_results = feature_engineer.scale_features(
                selected_features,
                scaler_type='robust',
                train_ratio=0.8,
                save_path=scaler_path
            )
            scaled_features = scale_results['scaled_df']
            print(f"   âœ… ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ")
            print(f"   ğŸ’¾ æ ‡å‡†åŒ–å™¨: {scaler_path}")
            
            # æ­¥éª¤3.4: ç›®æ ‡å·¥ç¨‹
            print(f"\n[æ­¥éª¤3.4] ç›®æ ‡å·¥ç¨‹")
            datasets_dir = config['paths'].get('datasets_dir', 'ML output/datasets/baseline_v1')
            if not os.path.isabs(datasets_dir):
                datasets_dir = os.path.join(ml_root, datasets_dir)
            os.makedirs(datasets_dir, exist_ok=True)
            
            target_engineer = TargetEngineer(data_dir=datasets_dir)
            complete_df = target_engineer.create_complete_dataset(
                features_df=scaled_features,
                periods=[1, 5, 10],
                price_col='close',
                include_labels=True,
                label_types=['binary']
            )
            
            print(f"   âœ… ç›®æ ‡å˜é‡ç”Ÿæˆå®Œæˆ: {complete_df.shape}")
            
            # æ­¥éª¤3.5: å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡
            print(f"\n[æ­¥éª¤3.5] å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡")
            
            # æå–ç‰¹å¾åˆ—ï¼ˆæ’é™¤closeå’Œç›®æ ‡åˆ—ï¼‰
            exclude_cols = ['close'] + [col for col in complete_df.columns 
                                        if col.startswith('future_return_') or col.startswith('label_')]
            feature_cols = [col for col in complete_df.columns if col not in exclude_cols]
            
            # è½¬æ¢ä¸ºMultiIndexæ ¼å¼ [date, ticker]
            dates = complete_df.index
            tickers = [symbol] * len(dates)
            multi_index = pd.MultiIndex.from_arrays([dates, tickers], names=['date', 'ticker'])
            
            features = complete_df[feature_cols].copy()
            features.index = multi_index
            
            targets = complete_df[target_col].copy()
            targets.index = multi_index
            
            print(f"   âœ… ç‰¹å¾æå–å®Œæˆ: {features.shape}")
            print(f"   âœ… ç›®æ ‡æå–å®Œæˆ: {targets.shape}")
            
            # æ­¥éª¤3.6: åŠ è½½å¹¶åˆå¹¶è´¢åŠ¡æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if loader.enable_pit_alignment:
                print(f"\n[æ­¥éª¤3.6] åŠ è½½å¹¶åˆå¹¶è´¢åŠ¡æ•°æ®")
                features, targets = loader._load_and_merge_financial_data(
                    features, targets, symbol, start_date, end_date
                )
            
            # æ­¥éª¤3.7: åˆ›å»ºæ•°æ®å¿«ç…§
            if loader.enable_snapshot and loader.snapshot_mgr:
                print(f"\n[æ­¥éª¤3.7] åˆ›å»ºæ•°æ®å¿«ç…§")
                snapshot_data = features.copy()
                snapshot_data[target_col] = targets
                
                snapshot_id = loader.snapshot_mgr.create_snapshot(
                    data=snapshot_data,
                    symbol=symbol,
                    start_date=start_date,
                    end_date=end_date,
                    filters=filter_config,
                    random_seed=random_seed,
                    save_parquet=config['data']['snapshot']['save_parquet']
                )
                print(f"\n   âœ… æ•°æ®å¿«ç…§åˆ›å»ºå®Œæˆ: {snapshot_id}")
            else:
                snapshot_id = None
            
            print(f"\n{'='*60}")
            print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
            print(f"{'='*60}")
            print(f"   ç‰¹å¾æ•°é‡: {len(features.columns)}")
            print(f"   æ ·æœ¬æ•°é‡: {len(features)}")
            if snapshot_id:
                print(f"   å¿«ç…§ID: {snapshot_id}")
            print(f"{'='*60}")
            
            # 4. å±•ç¤ºæ•°æ®è´¨é‡ç»Ÿè®¡
            print("\n[æ­¥éª¤4] æ•°æ®è´¨é‡ç»Ÿè®¡")
            print(f"   ç‰¹å¾ç¼ºå¤±ç‡: {features.isna().sum().sum() / features.size:.2%}")
            print(f"   ç›®æ ‡ç¼ºå¤±ç‡: {targets.isna().sum() / len(targets):.2%}")
            print(f"   æ—¶é—´èŒƒå›´: {features.index.get_level_values('date').min().date()} ~ "
                  f"{features.index.get_level_values('date').max().date()}")
            
            # 6. éªŒæ”¶æ£€æŸ¥
            print("\n[æ­¥éª¤5] æ•°æ®éªŒæ”¶æ£€æŸ¥")
            
            # æ£€æŸ¥1: å¯äº¤æ˜“æ ·æœ¬è§„æ¨¡
            n_samples = len(features)
            min_samples = 200  # æ ¹æ®å®ªç« è¦æ±‚
            sample_check = n_samples >= min_samples
            print(f"   {'âœ…' if sample_check else 'âŒ'} æ ·æœ¬è§„æ¨¡: {n_samples} (æœ€ä½ {min_samples})")
            
            # æ£€æŸ¥2: PITå¯¹é½
            if loader.pit_aligner:
                combined = features.copy()
                combined[target_col] = targets
                pit_results = loader.pit_aligner.validate_pit_alignment(combined, target_col)
                pit_check = pit_results.get('overall_pass', False)
                print(f"   {'âœ…' if pit_check else 'âŒ'} PITå¯¹é½éªŒè¯")
            else:
                pit_check = True
                print(f"   âš ï¸  PITå¯¹é½éªŒè¯ï¼ˆæœªå¯ç”¨ï¼‰")
            
            # æ£€æŸ¥3: æ•°æ®è´¨é‡
            if snapshot_id and loader.snapshot_mgr:
                # è´¨é‡æŠ¥å‘Šè·¯å¾„
                quality_report_path = os.path.join(
                    loader.snapshot_mgr.quality_reports_dir,
                    f"{snapshot_id}.json"
                )
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if not os.path.exists(quality_report_path):
                    print(f"   âš ï¸  è´¨é‡æŠ¥å‘Šæœªæ‰¾åˆ°: {quality_report_path}")
                    quality_check = True  # å¦‚æœæ²¡æœ‰è´¨é‡æŠ¥å‘Šï¼Œé»˜è®¤é€šè¿‡
                else:
                    with open(quality_report_path, 'r', encoding='utf-8') as f:
                        quality_report = json.load(f)
                    
                    overall_quality = quality_report.get('overall_quality')
                    red_flags = quality_report.get('red_flags_count', 0)
                    
                    # åˆ†æWARNINGç±»å‹ï¼šæ¥å—"æ—¶é—´é—´éš”"å’Œ"ç¼ºå¤±ç‡"ï¼ˆè´¢åŠ¡æ•°æ®ï¼‰ç±»å‹çš„WARNING
                    checks = quality_report.get('checks', {})
                    time_continuity_warning = checks.get('time_continuity', {}).get('red_flag', False)
                    missing_ratio_warning = checks.get('missing_ratio', {}).get('red_flag', False)
                    
                    # å¯æ¥å—çš„WARNINGç±»å‹
                    acceptable_warnings = time_continuity_warning or missing_ratio_warning
                    unacceptable_warnings = red_flags > 0 and not acceptable_warnings
                    
                    # åªæ¥å—PASS æˆ– ä»…æœ‰å¯æ¥å—WARNINGçš„æƒ…å†µ
                    if overall_quality == 'PASS':
                        quality_check = True
                        print(f"   âœ… æ•°æ®è´¨é‡: PASS")
                    elif overall_quality == 'WARNING' and not unacceptable_warnings:
                        # è¯¦ç»†è¯´æ˜æ˜¯å“ªç§å¯æ¥å—çš„WARNING
                        warning_types = []
                        if time_continuity_warning:
                            warning_types.append("æ—¶é—´é—´éš”")
                        if missing_ratio_warning:
                            warning_types.append("ç¼ºå¤±ç‡(è´¢åŠ¡æ•°æ®æ­£å¸¸)")
                        quality_check = True
                        print(f"   âš ï¸  æ•°æ®è´¨é‡: WARNING ({', '.join(warning_types)}ï¼Œå¯æ¥å—)")
                    else:
                        quality_check = False
                        print(f"   âŒ æ•°æ®è´¨é‡: {overall_quality} ({red_flags} ä¸ªçº¢ç¯)")
            else:
                quality_check = True
                print(f"   âš ï¸  æ•°æ®è´¨é‡æ£€æŸ¥ï¼ˆæœªå¯ç”¨å¿«ç…§ï¼‰")
            
            # æ€»ä½“éªŒæ”¶ï¼ˆä¿æŒä¸¥æ ¼æ ‡å‡†ï¼‰
            # æ ¸å¿ƒè¦æ±‚ï¼šæ ·æœ¬æ•°è¶³å¤Ÿ + PITéªŒè¯é€šè¿‡ + æ•°æ®è´¨é‡å¯æ¥å—
            all_passed = sample_check and pit_check and quality_check
            
            if all_passed:
                # 7. ä¿å­˜CSVæ ¼å¼æ•°æ®é›†ï¼ˆç”¨äºåç»­ train_models.pyï¼‰
                print("\n[æ­¥éª¤6] ä¿å­˜CSVæ ¼å¼æ•°æ®é›†ï¼ˆç”¨äºæ¨¡å‹è®­ç»ƒï¼‰")
                datasets_dir = config['paths'].get('datasets_dir', 'ML output/datasets/baseline_v1')
                if not os.path.isabs(datasets_dir):
                    datasets_dir = os.path.join(ml_root, datasets_dir)
                os.makedirs(datasets_dir, exist_ok=True)
                
                # åˆå¹¶ç‰¹å¾å’Œç›®æ ‡
                complete_df = features.copy()
                complete_df[target_col] = targets
                
                # ä¿å­˜ä¸ºCSV
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                csv_filename = f"with_targets_{symbol}_complete_{timestamp}.csv"
                csv_path = os.path.join(datasets_dir, csv_filename)
                complete_df.to_csv(csv_path)
                
                print(f"   âœ… CSVæ•°æ®é›†å·²ä¿å­˜: {csv_path}")
                print(f"   ğŸ“Š å½¢çŠ¶: {complete_df.shape}")
                
                success_count += 1
                print(f"\nâœ… {symbol} å¤„ç†æˆåŠŸï¼")
            else:
                print(f"\nâš ï¸ {symbol} éªŒæ”¶å¤±è´¥ï¼Œè·³è¿‡ä¿å­˜")
                failed_symbols.append(symbol)
            
        except Exception as e:
            print(f"\nâŒ {symbol} å¤„ç†å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            failed_symbols.append(symbol)
            continue
    
    # 4. æœ€ç»ˆç»Ÿè®¡
    print("\n" + "=" * 70)
    print("æ‰¹é‡å¤„ç†å®Œæˆ")
    print("=" * 70)
    print(f"âœ… æˆåŠŸ: {success_count}/{len(symbols)}")
    if failed_symbols:
        print(f"âŒ å¤±è´¥: {len(failed_symbols)}/{len(symbols)}")
        print(f"   å¤±è´¥è‚¡ç¥¨: {failed_symbols}")
    print("=" * 70)
    
    return 0 if success_count == len(symbols) else 1


if __name__ == "__main__":
    exit(main())
