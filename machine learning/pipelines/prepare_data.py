#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´çš„ç‰¹å¾+ç›®æ ‡ç”Ÿæˆæµç¨‹ï¼ˆç»Ÿä¸€æ”¯æŒå•æ ‡çš„å’Œå¤šæ ‡çš„ï¼‰

åŠŸèƒ½ï¼š
1. æ™ºèƒ½æ£€æµ‹å•æ ‡çš„/å¤šæ ‡çš„æ¨¡å¼ï¼ˆæ ¹æ®é…ç½®æ–‡ä»¶ä¸­çš„ symbol ç±»å‹ï¼‰
2. åŠ è½½åŸå§‹æ•°æ®ï¼ˆå•æ ‡çš„æˆ–æ‰¹é‡ï¼‰
3. ç”ŸæˆæŠ€æœ¯ç‰¹å¾
4. ç‰¹å¾é€‰æ‹©å’Œæ ‡å‡†åŒ–
5. ç”Ÿæˆç›®æ ‡å˜é‡
6. ä¿å­˜å®Œæ•´æ•°æ®é›†

ä½¿ç”¨æ–¹å¼ï¼š
- å•æ ‡çš„ï¼šé…ç½®æ–‡ä»¶ä¸­ symbol: "000001"
- å¤šæ ‡çš„ï¼šé…ç½®æ–‡ä»¶ä¸­ symbol: ["000001", "600000", "000858"]
  æˆ–å‘½ä»¤è¡Œï¼špython prepare_data.py --symbols 000001 600000 000858
"""

import os
import sys
import yaml
import argparse
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
ml_root = os.path.dirname(current_dir)
if ml_root not in sys.path:
    sys.path.insert(0, ml_root)

# å¯¼å…¥ç‰¹å¾å’Œç›®æ ‡å·¥ç¨‹æ¨¡å—
from features.feature_engineering import FeatureEngineer
from targets.target_engineering import TargetEngineer


def load_config(config_path: str) -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºåŸºäºml_rootçš„ç»å¯¹è·¯å¾„
    if not os.path.isabs(config_path):
        config_path = os.path.join(ml_root, config_path.replace("machine learning/", ""))
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def main(config_path: str = "configs/ml_baseline.yml", symbols: list = None):
    """
    å®Œæ•´çš„æ•°æ®å‡†å¤‡æµç¨‹ï¼ˆç»Ÿä¸€æ”¯æŒå•æ ‡çš„å’Œå¤šæ ‡çš„ï¼‰
    
    Parameters:
    -----------
    config_path : str
        é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ä½¿ç”¨ ml_baseline.ymlï¼‰
    symbols : list, optional
        è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ï¼‰
    """
    print("=" * 70)
    print("ğŸ”¨ æ•°æ®å‡†å¤‡æµç¨‹")
    print("=" * 70)
    
    # 1. åŠ è½½é…ç½®
    print("\nğŸ“‹ åŠ è½½é…ç½®...")
    config = load_config(config_path)
    
    # æ˜¾ç¤ºé¡¹ç›®ä¿¡æ¯
    project_info = config.get('project', {})
    if project_info:
        print(f"   ğŸ“¦ é¡¹ç›®: {project_info.get('name', 'N/A')}")
        print(f"   ğŸ“ æè¿°: {project_info.get('description', 'N/A')}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆè½¬æ¢ä¸ºç»å¯¹è·¯å¾„ï¼‰
    datasets_dir_cfg = config['paths'].get('datasets_dir', 'ML output/datasets/baseline_v1')
    scalers_dir_cfg = config['paths'].get('scalers_dir', 'ML output/scalers/baseline_v1')

    datasets_dir = datasets_dir_cfg if os.path.isabs(datasets_dir_cfg) else os.path.join(ml_root, datasets_dir_cfg)
    scalers_dir = scalers_dir_cfg if os.path.isabs(scalers_dir_cfg) else os.path.join(ml_root, scalers_dir_cfg)
    os.makedirs(datasets_dir, exist_ok=True)
    os.makedirs(scalers_dir, exist_ok=True)
    
    # 2. æ™ºèƒ½æ£€æµ‹å•æ ‡çš„/å¤šæ ‡çš„æ¨¡å¼
    if symbols is None:
        config_symbol = config['data']['symbol']
        # æ™ºèƒ½æ£€æµ‹ï¼šå¦‚æœæ˜¯åˆ—è¡¨åˆ™ç”¨åˆ—è¡¨ï¼Œå¦‚æœæ˜¯å­—ç¬¦ä¸²åˆ™è½¬ä¸ºå•å…ƒç´ åˆ—è¡¨
        if isinstance(config_symbol, list):
            symbols = config_symbol
            is_multi = True
        else:
            symbols = [config_symbol]
            is_multi = False
    else:
        is_multi = len(symbols) > 1
    
    start_date = config['data']['start_date']
    end_date = config['data']['end_date']
    
    mode_name = "å¤šæ ‡çš„" if is_multi else "å•æ ‡çš„"
    print(f"   æ¨¡å¼: {mode_name}")
    print(f"   è‚¡ç¥¨ä»£ç : {symbols if is_multi else symbols[0]}")
    print(f"   è‚¡ç¥¨æ•°é‡: {len(symbols)}")
    print(f"   æ—¶é—´èŒƒå›´: {start_date} ~ {end_date}")
    
    # 3. ç‰¹å¾å·¥ç¨‹
    print(f"\nğŸ”§ ç‰¹å¾å·¥ç¨‹ï¼ˆ{mode_name}æ¨¡å¼ï¼‰...")
    feature_engineer = FeatureEngineer(use_talib=True, use_tsfresh=False)
    
    if is_multi:
        # å¤šæ ‡çš„æ¨¡å¼ï¼šæ‰¹é‡åŠ è½½å’Œç”Ÿæˆç‰¹å¾
        features_df = feature_engineer.prepare_features_batch(
            symbols=symbols,
            start_date=start_date,
            end_date=end_date,
            use_auto_features=False,
            keep_base_columns=True
        )
        # ç»Ÿè®¡ä¿¡æ¯
        feature_cols = [c for c in features_df.columns if c not in ['close', 'volume', 'amount', 'pct_change', 'turnover']]
        print(f"   âœ… æ‰¹é‡ç‰¹å¾ç”Ÿæˆå®Œæˆ")
        print(f"      æ€»æ ·æœ¬: {len(features_df):,}")
        print(f"      ç‰¹å¾æ•°: {len(feature_cols)}")
        print(f"      è‚¡ç¥¨æ•°: {features_df.index.get_level_values('ticker').nunique()}")
    else:
        # å•æ ‡çš„æ¨¡å¼ï¼šä¼ ç»Ÿæµç¨‹
        raw_data = feature_engineer.load_stock_data(symbols[0], start_date, end_date)
        features_df = feature_engineer.prepare_features(
            raw_data,
            use_auto_features=False,
            keep_base_columns=True
        )
        print(f"   âœ… ç‰¹å¾ç”Ÿæˆå®Œæˆ: {features_df.shape[1]-1} ä¸ªç‰¹å¾")
    
    # 4. ç‰¹å¾é€‰æ‹©ï¼ˆå¯é€‰ï¼Œé…ç½®æ–‡ä»¶å¯æ§åˆ¶æ˜¯å¦è·³è¿‡ï¼‰
    skip_selection = config.get('features', {}).get('skip_selection', False)  # é»˜è®¤ä¸è·³è¿‡
    
    if skip_selection:
        print(f"\nâ­ï¸  è·³è¿‡ç‰¹å¾é€‰æ‹©ï¼ˆé…ç½®æ–‡ä»¶è®¾ç½®ï¼‰")
        selected_features = features_df
        final_feature_count = len([c for c in features_df.columns if c not in ['close', 'volume', 'amount', 'pct_change', 'turnover']])
    else:
        print(f"\nğŸ¯ ç‰¹å¾é€‰æ‹©...")
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–ç‰¹å¾é€‰æ‹©å‚æ•°
        final_k = config.get('features', {}).get('final_k', 20)
        variance_threshold = config.get('features', {}).get('variance_threshold', 0.01)
        correlation_threshold = config.get('features', {}).get('correlation_threshold', 0.9)
        
        selection_results = feature_engineer.select_features(
            features_df,
            final_k=final_k,
            variance_threshold=variance_threshold,
            correlation_threshold=correlation_threshold,
            train_ratio=0.8
        )
        selected_features = selection_results['final_features_df']
        final_feature_count = len(selection_results['final_features'])
        print(f"   âœ… ç‰¹å¾é€‰æ‹©å®Œæˆ: {final_feature_count} ä¸ªç‰¹å¾")
    
    # 5. ç‰¹å¾æ ‡å‡†åŒ–
    print(f"\nğŸ“ ç‰¹å¾æ ‡å‡†åŒ–...")
    scaler_suffix = "multi" if is_multi else symbols[0]
    scaler_path = os.path.join(scalers_dir, f"scaler_{scaler_suffix}.pkl")
    
    scale_results = feature_engineer.scale_features(
        selected_features,
        scaler_type='robust',
        train_ratio=0.8,
        save_path=scaler_path
    )
    
    scaled_features = scale_results['scaled_df']
    print(f"   âœ… ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ")
    print(f"   ğŸ’¾ æ ‡å‡†åŒ–å™¨: {scaler_path}")
    
    # 6. ç›®æ ‡å·¥ç¨‹
    print(f"\nğŸ¯ ç›®æ ‡å·¥ç¨‹ï¼ˆ{mode_name}æ¨¡å¼ï¼‰...")
    target_engineer = TargetEngineer(data_dir=datasets_dir)
    
    # ç”Ÿæˆç›®æ ‡å˜é‡
    complete_df = target_engineer.create_complete_dataset(
        features_df=scaled_features,
        periods=[1, 5, 10],
        price_col='close',
        include_labels=True,
        label_types=['binary']
    )
    
    # 7. ä¿å­˜æ•°æ®é›†
    print(f"\nğŸ’¾ ä¿å­˜æ•°æ®é›†...")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    if is_multi:
        # å¤šæ ‡çš„ï¼šä¿å­˜ä¸º complete_multi_timestamp.csv
        filepath = target_engineer.save_dataset(
            complete_df,
            symbol="multi",
            suffix=f"complete_{timestamp}"
        )
    else:
        # å•æ ‡çš„ï¼šä¿å­˜ä¸º complete_SYMBOL_timestamp.csv
        filepath = target_engineer.save_dataset(
            complete_df,
            symbol=symbols[0],
            suffix=f"complete_{timestamp}"
        )
    
    print(f"   âœ… æ•°æ®é›†å·²ä¿å­˜: {filepath}")
    
    # 8. æ€»ç»“
    print("\n" + "=" * 70)
    print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼ï¼ˆ{mode_name}æ¨¡å¼ï¼‰")
    print("=" * 70)
    print(f"\nğŸ“Š è¾“å‡ºæ–‡ä»¶:")
    print(f"   ç‰¹å¾æ ‡å‡†åŒ–å™¨: {scale_results['scaler_path']}")
    print(f"   å®Œæ•´æ•°æ®é›†: {filepath}")
    print(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
    print(f"   æ¨¡å¼: {mode_name}")
    print(f"   è‚¡ç¥¨æ•°é‡: {len(symbols)}")
    print(f"   ç‰¹å¾æ•°é‡: {final_feature_count}")
    print(f"   æ ·æœ¬æ•°é‡: {len(complete_df):,}")
    if is_multi:
        print(f"   ç´¢å¼•æ ¼å¼: MultiIndex [date, ticker]")
        print(f"   å”¯ä¸€è‚¡ç¥¨æ•°: {complete_df.index.get_level_values('ticker').nunique()}")
    else:
        print(f"   ç´¢å¼•æ ¼å¼: DatetimeIndex")
    print(f"   ç›®æ ‡å˜é‡: future_return_1d, future_return_5d, future_return_10d")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='æ•°æ®å‡†å¤‡æµç¨‹ï¼ˆç»Ÿä¸€æ”¯æŒå•æ ‡çš„å’Œå¤šæ ‡çš„ï¼‰')
    parser.add_argument('--config', type=str, 
                       default='configs/ml_baseline.yml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--symbols', type=str, nargs='+',
                       help='è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰ï¼Œä¾‹å¦‚ï¼š--symbols 000001 600000 000858')
    
    args = parser.parse_args()
    
    try:
        main(args.config, symbols=args.symbols)
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
