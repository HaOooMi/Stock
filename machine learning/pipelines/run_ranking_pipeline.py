#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ’åºæ¨¡å‹è®­ç»ƒç®¡é“ - ä¸‰æ¡çº¿å¯¹æ¯”

åŠŸèƒ½ï¼š
1. Baseline Aï¼šå›å½’åŸå§‹æ”¶ç›Šï¼ˆLGBMRegressorï¼‰
2. Baseline Bï¼šReg-on-Rankï¼ˆLGBMRegressor + æ’åºæ ‡ç­¾ï¼‰
3. Sortingï¼šLambdaRankï¼ˆLGBMRankerï¼‰

ç»Ÿä¸€ä½¿ç”¨ CrossSectionAnalyzer è¯„ä¼°ï¼Œå¯¹æ¯”ä¸‰æ¡çº¿çš„ï¼š
- Rank IC / ICIR
- Top-Mean / Top-Bottom Spread
- ç¨³å®šæ€§ / æ¼‚ç§»

ä½¿ç”¨æ–¹æ³•ï¼š
    python run_ranking_pipeline.py
    python run_ranking_pipeline.py --task_type lambdarank
    python run_ranking_pipeline.py --compare_all  # è¿è¡Œä¸‰æ¡çº¿å¯¹æ¯”

è¾“å‡ºï¼š
    /ML output/reports/baseline_v1/ranking/
    â”œâ”€â”€ model_comparison.json
    â”œâ”€â”€ regression_results.json
    â”œâ”€â”€ regression_rank_results.json
    â”œâ”€â”€ lambdarank_results.json
    â””â”€â”€ comparison_tearsheet.html

åˆ›å»º: 2025-12-04 | ç‰ˆæœ¬: v1.0
"""

import os
import sys
import yaml
import json
import argparse
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Optional, Tuple

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
ml_root = os.path.dirname(current_dir)
if ml_root not in sys.path:
    sys.path.insert(0, ml_root)

# å¯¼å…¥æ¨¡å—
from data.data_loader import DataLoader
from data.time_series_cv import TimeSeriesCV
from targets.ranking_labels import RankingLabelFactory, create_ranking_labels
from models.lgbm_model import LightGBMModel
from models.lgbm_ranker import LightGBMRanker, prepare_ranking_data
from evaluation.cross_section_analyzer import CrossSectionAnalyzer
from evaluation.cross_section_metrics import calculate_forward_returns


def load_config(config_path: str = "configs/ml_baseline.yml") -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if not os.path.isabs(config_path):
        config_path = os.path.join(ml_root, config_path)
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    return config


def prepare_data(config: dict) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    å‡†å¤‡æ•°æ®ï¼šç‰¹å¾ã€è¿œæœŸæ”¶ç›Šã€ä»·æ ¼
    
    Returns:
    --------
    Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]
        (features, forward_returns, prices)
    """
    print("\n" + "=" * 70)
    print("å‡†å¤‡æ•°æ®")
    print("=" * 70)
    
    # è·å–é…ç½®
    data_config = config['data']
    target_config = config['target']
    symbols = data_config['symbol']
    if isinstance(symbols, str):
        symbols = [symbols]
    
    start_date = data_config['start_date']
    end_date = data_config['end_date']
    forward_periods = target_config['forward_periods']
    target_col = f"future_return_{forward_periods}d"
    
    # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
    # æ³¨æ„ï¼šinfluxdb_config éœ€è¦ç§»é™¤ 'enabled' å­—æ®µ
    influxdb_config = data_config.get('influxdb', {}).copy()
    influxdb_config.pop('enabled', None)  # ç§»é™¤ enabled å­—æ®µ
    
    loader = DataLoader(
        data_root=config['paths']['data_root'],
        enable_snapshot=data_config['snapshot']['enabled'],
        enable_filtering=True,
        enable_influxdb=data_config['influxdb']['enabled'],
        influxdb_config=influxdb_config,
        filter_config=data_config['universe']
    )
    
    # åŠ è½½å¤šä¸ªè‚¡ç¥¨çš„æ•°æ®
    all_features = []
    all_targets = []
    
    for symbol in symbols:
        try:
            features, targets = loader.load_features_and_targets(
                symbol=symbol,
                target_col=target_col,
                use_scaled=config['features']['use_scaled_features']
            )
            all_features.append(features)
            all_targets.append(targets)
            print(f"   âœ… {symbol}: {len(features)} æ ·æœ¬")
        except Exception as e:
            print(f"   âš ï¸ {symbol} åŠ è½½å¤±è´¥: {e}")
            continue
    
    if not all_features:
        raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•è‚¡ç¥¨æ•°æ®")
    
    # åˆå¹¶æ‰€æœ‰è‚¡ç¥¨
    features = pd.concat(all_features, axis=0)
    targets = pd.concat(all_targets, axis=0)
    
    print(f"âœ… ç‰¹å¾åŠ è½½å®Œæˆ: {features.shape}")
    print(f"âœ… ç›®æ ‡åŠ è½½å®Œæˆ: {len(targets)}")
    
    # æ„é€  forward_returns DataFrameï¼ˆè¯„ä¼°éœ€è¦ï¼‰
    forward_returns = targets.to_frame(f'ret_{forward_periods}d')
    
    # prices æš‚æ—¶è®¾ä¸º Noneï¼ˆå¦‚æœéœ€è¦å¯ä»¥ä» InfluxDB åŠ è½½ï¼‰
    prices = None
    
    print(f"âœ… æ ·æœ¬æ€»æ•°: {len(features):,}")
    
    return features, forward_returns, prices


def run_single_task(task_type: str,
                    config: dict,
                    features: pd.DataFrame,
                    forward_returns: pd.DataFrame,
                    train_idx: pd.Index,
                    valid_idx: pd.Index,
                    test_idx: pd.Index,
                    output_dir: str) -> Dict:
    """
    è¿è¡Œå•ä¸ªä»»åŠ¡ç±»å‹
    
    Parameters:
    -----------
    task_type : str
        ä»»åŠ¡ç±»å‹ï¼š'regression', 'regression_rank', 'lambdarank'
    config : dict
        é…ç½®å­—å…¸
    features : pd.DataFrame
        ç‰¹å¾æ•°æ®
    forward_returns : pd.DataFrame
        è¿œæœŸæ”¶ç›Š
    train_idx, valid_idx, test_idx : pd.Index
        åˆ‡åˆ†ç´¢å¼•
    output_dir : str
        è¾“å‡ºç›®å½•
        
    Returns:
    --------
    dict
        ç»“æœæ±‡æ€»
    """
    print(f"\n{'='*70}")
    print(f"ä»»åŠ¡ç±»å‹: {task_type}")
    print(f"{'='*70}")
    
    # è·å–ç›®æ ‡åˆ—å
    target_col = f"ret_{config['target']['forward_periods']}d"
    
    # æ’åºé…ç½®
    ranking_config = config.get('ranking', {})
    
    # åˆ›å»ºæ ‡ç­¾
    label_factory = RankingLabelFactory(
        n_bins=ranking_config.get('lambdarank', {}).get('n_bins', 5),
        rank_method=ranking_config.get('regression_rank', {}).get('rank_method', 'zscore')
    )
    
    min_samples = ranking_config.get('regression_rank', {}).get('min_samples_per_day', 30)
    label_result = label_factory.create_labels(
        forward_returns, task_type, target_col, min_samples
    )
    
    labels = label_result['labels']
    groups = label_result['groups']
    
    # å¯¹é½ç‰¹å¾ä¸æ ‡ç­¾
    X_aligned, y_aligned = label_factory.align_features_with_labels(features, labels)
    
    # æŒ‰åˆ‡åˆ†ç´¢å¼•è·å–è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
    train_common = train_idx.intersection(X_aligned.index)
    valid_common = valid_idx.intersection(X_aligned.index)
    test_common = test_idx.intersection(X_aligned.index)
    
    X_train = X_aligned.loc[train_common].sort_index(level='date')
    y_train = y_aligned.loc[train_common].sort_index(level='date')
    X_valid = X_aligned.loc[valid_common].sort_index(level='date')
    y_valid = y_aligned.loc[valid_common].sort_index(level='date')
    X_test = X_aligned.loc[test_common].sort_index(level='date')
    y_test = y_aligned.loc[test_common].sort_index(level='date')
    
    print(f"è®­ç»ƒé›†: {len(X_train):,} æ ·æœ¬")
    print(f"éªŒè¯é›†: {len(X_valid):,} æ ·æœ¬")
    print(f"æµ‹è¯•é›†: {len(X_test):,} æ ·æœ¬")
    
    # æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©æ¨¡å‹
    if task_type == 'lambdarank':
        # LambdaRank éœ€è¦ group
        train_groups = X_train.groupby(level='date').size().tolist()
        valid_groups = X_valid.groupby(level='date').size().tolist()
        
        model_config = config['models'].get('lightgbm_ranker', {}).get('params', {})
        model = LightGBMRanker(params=model_config)
        
        train_result = model.fit(
            X_train, y_train,
            X_valid, y_valid,
            groups=train_groups,
            valid_groups=valid_groups
        )
    else:
        # å›å½’æ¨¡å‹ï¼ˆregression æˆ– regression_rankï¼‰
        model_config = config['models'].get('lightgbm', {}).get('params', {})
        model = LightGBMModel(params=model_config)
        
        train_result = model.fit(X_train, y_train, X_valid, y_valid)
    
    # é¢„æµ‹
    pred_train = model.predict(X_train)
    pred_valid = model.predict(X_valid)
    pred_test = model.predict(X_test)
    
    # å°†é¢„æµ‹å€¼è½¬ä¸º Seriesï¼ˆä¿æŒ MultiIndexï¼‰
    pred_train_series = pd.Series(pred_train, index=X_train.index, name='score')
    pred_valid_series = pd.Series(pred_valid, index=X_valid.index, name='score')
    pred_test_series = pd.Series(pred_test, index=X_test.index, name='score')
    
    # åˆå¹¶æ‰€æœ‰é¢„æµ‹
    all_predictions = pd.concat([pred_train_series, pred_valid_series, pred_test_series])
    all_predictions = all_predictions.to_frame('score')
    
    # ä½¿ç”¨ CrossSectionAnalyzer è¯„ä¼°
    # æ³¨æ„ï¼šè¯„ä¼°æ—¶ç»Ÿä¸€ä½¿ç”¨åŸå§‹æ”¶ç›Šä½œä¸º forward_returns
    test_forward_returns = forward_returns.loc[test_common]
    
    print("\nğŸ“Š æµ‹è¯•é›†æ¨ªæˆªé¢è¯„ä¼°...")
    
    analyzer = CrossSectionAnalyzer(
        factors=pred_test_series.to_frame('model_score'),
        forward_returns=test_forward_returns
    )
    analyzer.analyze()
    
    results = analyzer.get_results()
    
    # æå–å…³é”®æŒ‡æ ‡
    ic_summary = results.get('ic_summary', {})
    spreads = results.get('spreads', {})
    
    # æ„å»ºç»“æœæ±‡æ€»
    summary = {
        'task_type': task_type,
        'train_samples': len(X_train),
        'valid_samples': len(X_valid),
        'test_samples': len(X_test),
        'training_result': train_result,
        'ic_summary': {},
        'spreads': {}
    }
    
    # è½¬æ¢ IC ç»Ÿè®¡
    for key, value in ic_summary.items():
        if isinstance(value, dict):
            summary['ic_summary'][str(key)] = {
                k: float(v) if isinstance(v, (np.floating, np.integer)) else v
                for k, v in value.items()
            }
    
    # è½¬æ¢ Spread
    for key, value in spreads.items():
        if hasattr(value, 'mean'):
            summary['spreads'][str(key)] = {
                'mean': float(value.mean()),
                'std': float(value.std()),
                'sharpe': float(value.mean() / value.std()) if value.std() != 0 else 0
            }
    
    # ä¿å­˜ç»“æœ
    result_path = os.path.join(output_dir, f'{task_type}_results.json')
    with open(result_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"âœ… ç»“æœå·²ä¿å­˜: {result_path}")
    
    # ä¿å­˜é¢„æµ‹
    pred_path = os.path.join(output_dir, f'{task_type}_predictions.parquet')
    all_predictions.to_parquet(pred_path)
    print(f"âœ… é¢„æµ‹å·²ä¿å­˜: {pred_path}")
    
    # ä¿å­˜æ¨¡å‹
    model_path = os.path.join(output_dir, f'{task_type}_model.pkl')
    model.save(model_path)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    return summary


def compare_results(results: Dict[str, Dict], output_dir: str) -> Dict:
    """
    å¯¹æ¯”ä¸‰æ¡çº¿çš„ç»“æœ
    
    Parameters:
    -----------
    results : Dict[str, Dict]
        å„ä»»åŠ¡ç±»å‹çš„ç»“æœ
    output_dir : str
        è¾“å‡ºç›®å½•
        
    Returns:
    --------
    dict
        å¯¹æ¯”æ±‡æ€»
    """
    print("\n" + "=" * 70)
    print("ä¸‰æ¡çº¿å¯¹æ¯”")
    print("=" * 70)
    
    comparison = {
        'timestamp': datetime.now().isoformat(),
        'tasks': list(results.keys()),
        'metrics': {}
    }
    
    # æ”¶é›†å„ä»»åŠ¡çš„å…³é”®æŒ‡æ ‡
    for task_type, result in results.items():
        ic_summary = result.get('ic_summary', {})
        spreads = result.get('spreads', {})
        
        # æå–ç¬¬ä¸€ä¸ªå› å­çš„ IC
        first_ic_key = list(ic_summary.keys())[0] if ic_summary else None
        if first_ic_key:
            ic_stats = ic_summary[first_ic_key]
            comparison['metrics'][task_type] = {
                'mean_ic': ic_stats.get('mean', 0),
                'icir': ic_stats.get('icir', 0),
                'icir_annual': ic_stats.get('icir_annual', 0),
                't_stat': ic_stats.get('t_stat', 0),
                'ic_positive_ratio': ic_stats.get('positive_ratio', 0)
            }
        
        # æå– Spread
        first_spread_key = list(spreads.keys())[0] if spreads else None
        if first_spread_key:
            spread_stats = spreads[first_spread_key]
            comparison['metrics'][task_type]['spread_mean'] = spread_stats.get('mean', 0)
            comparison['metrics'][task_type]['spread_sharpe'] = spread_stats.get('sharpe', 0)
    
    # æ‰“å°å¯¹æ¯”è¡¨æ ¼
    print("\nğŸ“Š å…³é”®æŒ‡æ ‡å¯¹æ¯”:")
    print("-" * 80)
    print(f"{'ä»»åŠ¡ç±»å‹':<20} {'Mean IC':>12} {'ICIR':>12} {'ICIR(å¹´åŒ–)':>12} {'Spread':>12}")
    print("-" * 80)
    
    for task_type, metrics in comparison['metrics'].items():
        print(f"{task_type:<20} "
              f"{metrics.get('mean_ic', 0):>12.4f} "
              f"{metrics.get('icir', 0):>12.4f} "
              f"{metrics.get('icir_annual', 0):>12.4f} "
              f"{metrics.get('spread_mean', 0):>12.4f}")
    
    print("-" * 80)
    
    # è®¡ç®—æå‡æ¯”ä¾‹
    if 'regression' in comparison['metrics'] and len(comparison['metrics']) > 1:
        baseline_ic = comparison['metrics']['regression'].get('mean_ic', 0)
        baseline_icir = comparison['metrics']['regression'].get('icir', 0)
        
        print("\nğŸ“ˆ ç›¸å¯¹å›å½’åŸºçº¿çš„æå‡:")
        for task_type, metrics in comparison['metrics'].items():
            if task_type == 'regression':
                continue
            
            ic_improvement = (abs(metrics.get('mean_ic', 0)) - abs(baseline_ic)) / abs(baseline_ic) * 100 if baseline_ic != 0 else 0
            icir_improvement = (abs(metrics.get('icir', 0)) - abs(baseline_icir)) / abs(baseline_icir) * 100 if baseline_icir != 0 else 0
            
            print(f"  {task_type}: IC æå‡ {ic_improvement:+.1f}%, ICIR æå‡ {icir_improvement:+.1f}%")
            
            comparison['metrics'][task_type]['ic_improvement_vs_baseline'] = ic_improvement
            comparison['metrics'][task_type]['icir_improvement_vs_baseline'] = icir_improvement
    
    # ä¿å­˜å¯¹æ¯”ç»“æœ
    comparison_path = os.path.join(output_dir, 'model_comparison.json')
    with open(comparison_path, 'w', encoding='utf-8') as f:
        json.dump(comparison, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… å¯¹æ¯”ç»“æœå·²ä¿å­˜: {comparison_path}")
    
    return comparison


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ’åºæ¨¡å‹è®­ç»ƒç®¡é“')
    parser.add_argument('--config', type=str, default='configs/ml_baseline.yml',
                        help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--task_type', type=str, default=None,
                        choices=['regression', 'regression_rank', 'lambdarank'],
                        help='ä»»åŠ¡ç±»å‹ï¼ˆé»˜è®¤ä»é…ç½®è¯»å–ï¼‰')
    parser.add_argument('--compare_all', action='store_true',
                        help='è¿è¡Œä¸‰æ¡çº¿å¯¹æ¯”')
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("æ’åºæ¨¡å‹è®­ç»ƒç®¡é“")
    print("=" * 70)
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    print(f"âœ… é…ç½®åŠ è½½å®Œæˆ: {args.config}")
    
    # ç¡®å®šä»»åŠ¡ç±»å‹
    if args.compare_all:
        task_types = ['regression', 'regression_rank', 'lambdarank']
    elif args.task_type:
        task_types = [args.task_type]
    else:
        task_types = [config.get('ranking', {}).get('task_type', 'regression')]
    
    print(f"ğŸ“‹ ä»»åŠ¡ç±»å‹: {task_types}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = os.path.join(ml_root, config['paths']['reports_dir'], 'ranking')
    os.makedirs(output_dir, exist_ok=True)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # å‡†å¤‡æ•°æ®
    try:
        features, forward_returns, prices = prepare_data(config)
    except Exception as e:
        print(f"âŒ æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
        print("å°è¯•ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•...")
        
        # æ¨¡æ‹Ÿæ•°æ®ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        np.random.seed(42)
        dates = pd.date_range('2020-01-01', periods=500, freq='D')
        tickers = [f'{i:06d}' for i in range(1, 51)]
        index = pd.MultiIndex.from_product([dates, tickers], names=['date', 'ticker'])
        
        features = pd.DataFrame(
            np.random.randn(len(index), 20),
            columns=[f'feature_{i}' for i in range(20)],
            index=index
        )
        
        forward_returns = pd.DataFrame({
            'ret_5d': np.random.randn(len(index)) * 0.05
        }, index=index)
        
        prices = None
        print(f"âœ… æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå®Œæˆ: {features.shape}")
    
    # æ—¶åºåˆ‡åˆ†
    cv = TimeSeriesCV.from_config(config)
    train_idx, valid_idx, test_idx = cv.single_split(features)
    
    print(f"\nğŸ“Š æ—¶åºåˆ‡åˆ†:")
    print(f"   è®­ç»ƒé›†: {len(train_idx):,}")
    print(f"   éªŒè¯é›†: {len(valid_idx):,}")
    print(f"   æµ‹è¯•é›†: {len(test_idx):,}")
    
    # è¿è¡Œå„ä»»åŠ¡
    all_results = {}
    
    for task_type in task_types:
        try:
            result = run_single_task(
                task_type=task_type,
                config=config,
                features=features,
                forward_returns=forward_returns,
                train_idx=train_idx,
                valid_idx=valid_idx,
                test_idx=test_idx,
                output_dir=output_dir
            )
            all_results[task_type] = result
        except Exception as e:
            print(f"âŒ ä»»åŠ¡ {task_type} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # å¯¹æ¯”ç»“æœ
    if len(all_results) > 1:
        compare_results(all_results, output_dir)
    
    print("\n" + "=" * 70)
    print("âœ… æ’åºæ¨¡å‹è®­ç»ƒå®Œæˆ")
    print("=" * 70)


if __name__ == "__main__":
    main()
