#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨ç‰¹å¾å·¥ç¨‹æ¨¡å— - ä½¿ç”¨çœŸå®InfluxDBå†å²æ•°æ®

åŠŸèƒ½:
1. ä»InfluxDBåŠ è½½çœŸå®è‚¡ç¥¨å†å²æ•°æ®
2. ç”ŸæˆæŠ€æœ¯åˆ†æç‰¹å¾ 
3. ç‰¹å¾é€‰æ‹©å’Œä¼˜åŒ–
4. ç‰¹å¾è´¨é‡åˆ†æ

ä½œè€…: Your Name
æ—¥æœŸ: 2024
"""

import pandas as pd
import numpy as np
import sys
import os
import pickle
import json
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')

# InfluxDBç›¸å…³
sys.path.append(r'd:\vscode projects\stock\stock_info')
from utils import get_influxdb_client
from stock_market_data_akshare import get_history_data

# æœºå™¨å­¦ä¹ ç›¸å…³ 
from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_regression
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

# å°è¯•å¯¼å…¥å¯é€‰åº“
try:
    import talib
    HAS_TALIB = True
except ImportError:
    HAS_TALIB = False
    print("âš ï¸ talib æœªå®‰è£…ï¼Œéƒ¨åˆ†æŠ€æœ¯æŒ‡æ ‡å°†ä½¿ç”¨pandaså®ç°")

try:
    from tsfresh import extract_features
    HAS_TSFRESH = True
except ImportError:
    HAS_TSFRESH = False
    print("âš ï¸ tsfresh æœªå®‰è£…ï¼Œè‡ªåŠ¨ç‰¹å¾ç”Ÿæˆä¸å¯ç”¨")

try:
    from xgboost import XGBRegressor
    HAS_XGBOOST = True
except ImportError:
    HAS_XGBOOST = False
    print("âš ï¸ xgboost æœªå®‰è£…ï¼Œå°†ä½¿ç”¨RandomForestè¿›è¡Œç‰¹å¾é‡è¦æ€§è¯„ä¼°")




class FeatureEngineer:
    """
    è‚¡ç¥¨ç‰¹å¾å·¥ç¨‹ç±» - ä½¿ç”¨çœŸå®InfluxDBæ•°æ®
    
    åŠŸèƒ½:
    1. ä»InfluxDBåŠ è½½çœŸå®å†å²è‚¡ç¥¨æ•°æ®
    2. ç”ŸæˆæŠ€æœ¯åˆ†æç‰¹å¾
    3. ç‰¹å¾é€‰æ‹©å’Œä¼˜åŒ– 
    4. ç‰¹å¾è´¨é‡åˆ†æ
    """
    
    def __init__(self, use_talib: bool = True, use_tsfresh: bool = False):
        """
        åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹å™¨
        
        Parameters:
        -----------
        use_talib : bool, default=True
            æ˜¯å¦ä½¿ç”¨TA-Libè¿›è¡ŒæŠ€æœ¯æŒ‡æ ‡è®¡ç®—
        use_tsfresh : bool, default=False  
            æ˜¯å¦ä½¿ç”¨TSFreshè¿›è¡Œè‡ªåŠ¨ç‰¹å¾ç”Ÿæˆ
        """
        self.use_talib = use_talib and HAS_TALIB
        self.use_tsfresh = use_tsfresh and HAS_TSFRESH
        self.use_xgboost = HAS_XGBOOST
        
        # åˆå§‹åŒ–InfluxDBå®¢æˆ·ç«¯
        self.influx_client = get_influxdb_client()
        if not self.influx_client:
            raise ConnectionError("æ— æ³•è¿æ¥åˆ°InfluxDBï¼Œè¯·æ£€æŸ¥æœåŠ¡çŠ¶æ€å’Œé…ç½®")
        self.query_api = self.influx_client.query_api()
        
        print("ğŸ”§ ç‰¹å¾å·¥ç¨‹å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ“Š TA-Lib: {'âœ…' if self.use_talib else 'âŒ'}")
        print(f"   ğŸ¤– TSFresh: {'âœ…' if self.use_tsfresh else 'âŒ'}")
        print(f"   ğŸ’¾ InfluxDB: âœ…")
    
    def load_stock_data(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """
        ä»InfluxDBåŠ è½½çœŸå®è‚¡ç¥¨å†å²æ•°æ®
        
        Parameters:
        -----------
        symbol : str
            è‚¡ç¥¨ä»£ç ï¼Œå¦‚ '000001' 
        start_date : str
            å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ 'YYYY-MM-DD'
        end_date : str  
            ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ 'YYYY-MM-DD'
        
        Returns:
        --------
        pd.DataFrame
            åŒ…å«OHLCVæ•°æ®çš„DataFrame
        """
        print(f"ğŸ“Š ä»InfluxDBåŠ è½½è‚¡ç¥¨æ•°æ®: {symbol} ({start_date} åˆ° {end_date})")
        
        try:
            # è½¬æ¢æ—¥æœŸæ ¼å¼ä¸ºInfluxDB FluxæŸ¥è¯¢æ ¼å¼
            start_flux = pd.to_datetime(start_date).strftime('%Y-%m-%dT00:00:00Z')
            end_flux = pd.to_datetime(end_date).strftime('%Y-%m-%dT23:59:59Z')
            
            # ä»InfluxDBè·å–æ•°æ®
            df = get_history_data(self.query_api, symbol, start_flux, end_flux)
            
            if df.empty:
                raise ValueError(f"æœªæ‰¾åˆ°è‚¡ç¥¨ {symbol} åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ•°æ®")
            
            # æ ‡å‡†åŒ–åˆ—å 
            column_mapping = {
                'æ—¥æœŸ': 'timestamp',
                'å¼€ç›˜': 'open', 
                'æ”¶ç›˜': 'close',
                'æœ€é«˜': 'high',
                'æœ€ä½': 'low', 
                'æˆäº¤é‡': 'volume',
                'æˆäº¤é¢': 'turnover'
            }
            
            # é‡å‘½åå­˜åœ¨çš„åˆ—
            existing_cols = {k: v for k, v in column_mapping.items() if k in df.columns}
            df = df.rename(columns=existing_cols)
            
            # ç¡®ä¿æ—¶é—´åˆ—æ­£ç¡®å¤„ç†
            if 'timestamp' in df.columns:
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                df = df.set_index('timestamp')
            elif 'æ—¥æœŸ' in df.columns:
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
                df = df.set_index('æ—¥æœŸ')
            
            # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            for col in ['open', 'high', 'low', 'close', 'volume']:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # åˆ é™¤ç¼ºå¤±å€¼
            df = df.dropna()
            
            # æŒ‰æ—¶é—´æ’åº
            df = df.sort_index()
            
            print(f"âœ… æˆåŠŸåŠ è½½ {len(df)} æ¡æ•°æ®è®°å½•")
            print(f"ğŸ“… æ•°æ®æ—¶é—´èŒƒå›´: {df.index.min().date()} åˆ° {df.index.max().date()}")
            
            return df
            
        except Exception as e:
            print(f"âŒ åŠ è½½è‚¡ç¥¨æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            raise

    def prepare_features(self, data: pd.DataFrame, use_auto_features: bool = True, 
                        window_size: int = 20, max_auto_features: int = 50) -> pd.DataFrame:
        """
        ç»Ÿä¸€ç‰¹å¾ç”Ÿæˆæ–¹æ³• - æ”¯æŒæ‰‹å·¥ç‰¹å¾å’Œå¯é€‰çš„è‡ªåŠ¨ç‰¹å¾
        
        Parameters:
        -----------
        data : pd.DataFrame
            åŸå§‹è‚¡ç¥¨æ•°æ®ï¼ŒåŒ…å«OHLCVåˆ—
        use_auto_features : bool, default=False
            æ˜¯å¦ä½¿ç”¨TSFreshè‡ªåŠ¨ç”Ÿæˆç‰¹å¾
        window_size : int, default=20
            è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆçš„æ»‘åŠ¨çª—å£å¤§å°
        max_auto_features : int, default=50
            è‡ªåŠ¨ç‰¹å¾çš„æœ€å¤§æ•°é‡
        
        Returns:
        --------
        pd.DataFrame
            åŒ…å«æ‰€æœ‰ç‰¹å¾çš„æ•°æ®æ¡†
        """
        print("ğŸ”¨ å¼€å§‹ç‰¹å¾ç”Ÿæˆ...")
        
        # === 1. æ‰‹å·¥ç‰¹å¾ç”Ÿæˆ ===
        print("ğŸ“Š ç”Ÿæˆæ‰‹å·¥ç‰¹å¾...")
        data = data.copy()
        
        # æ”¶ç›Šç‡ç‰¹å¾
        print("   ğŸ“ˆ è®¡ç®—æ”¶ç›Šç‡ç‰¹å¾...")
        data['return_1d'] = data['close'].pct_change()
        data['return_5d'] = data['close'].pct_change(5)
        data['return_10d'] = data['close'].pct_change(10)
        data['return_20d'] = data['close'].pct_change(20)
        
        # æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾
        print("   ğŸ“Š è®¡ç®—æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾...")
        for window in [5, 10, 20, 30]:
            data[f'rolling_mean_{window}d'] = data['close'].rolling(window).mean()
            data[f'rolling_std_{window}d'] = data['close'].rolling(window).std()
            data[f'rolling_median_{window}d'] = data['close'].rolling(window).median()
            data[f'price_position_{window}d'] = (data['close'] - data['close'].rolling(window).min()) / (
                data['close'].rolling(window).max() - data['close'].rolling(window).min() + 1e-8) * 2 - 1
        
        # åŠ¨é‡ç‰¹å¾
        print("   ğŸš€ è®¡ç®—åŠ¨é‡ç‰¹å¾...")
        data['momentum_3d'] = data['close'] / data['close'].shift(3) - 1
        data['momentum_5d'] = data['close'] / data['close'].shift(5) - 1
        data['momentum_10d'] = data['close'] / data['close'].shift(10) - 1
        data['momentum_20d'] = data['close'] / data['close'].shift(20) - 1
        
        # æ³¢åŠ¨ç‡ç‰¹å¾  
        print("   ğŸ“Š è®¡ç®—æ³¢åŠ¨ç‡ç‰¹å¾...")
        if self.use_talib:
            data['atr_14'] = talib.ATR(data['high'], data['low'], data['close'], timeperiod=14)
        else:
            data['high_low'] = data['high'] - data['low']
            data['high_close'] = np.abs(data['high'] - data['close'].shift())
            data['low_close'] = np.abs(data['low'] - data['close'].shift())
            data['atr_14'] = pd.concat([data['high_low'], data['high_close'], data['low_close']], axis=1).max(axis=1).rolling(14).mean()
            data.drop(['high_low', 'high_close', 'low_close'], axis=1, inplace=True)
        
        data['volatility_5d'] = data['return_1d'].rolling(5).std()
        data['volatility_20d'] = data['return_1d'].rolling(20).std()
        data['skewness_20d'] = data['return_1d'].rolling(20).skew()
        data['kurtosis_20d'] = data['return_1d'].rolling(20).kurt()
        
        # æˆäº¤é‡ç‰¹å¾
        print("   ğŸ’° è®¡ç®—æˆäº¤é‡ç‰¹å¾...")
        data['volume_change'] = data['volume'].pct_change()
        data['volume_ma_5'] = data['volume'].rolling(5).mean()
        data['volume_ma_20'] = data['volume'].rolling(20).mean()
        data['volume_ratio_5d'] = data['volume'] / data['volume_ma_5']
        data['volume_ratio_20d'] = data['volume'] / data['volume_ma_20']
        data['volume_roc_3d'] = data['volume'].pct_change(3)
        data['volume_roc_5d'] = data['volume'].pct_change(5)
        data['volume_roc_10d'] = data['volume'].pct_change(10)
        
        # ä»·æ ¼èŒƒå›´ç‰¹å¾
        print("   ğŸ“ è®¡ç®—ä»·æ ¼èŒƒå›´ç‰¹å¾...")
        data['high_low_ratio'] = data['high'] / data['low']
        data['high_close_ratio'] = data['high'] / data['close']
        data['low_close_ratio'] = data['low'] / data['close']
        data['open_close_ratio'] = data['open'] / data['close']
        data['price_range'] = data['high'] - data['low']
        data['price_range_pct'] = (data['high'] - data['low']) / data['close']
        data['open_close_range'] = np.abs(data['open'] - data['close'])
        data['open_close_range_pct'] = np.abs(data['open'] - data['close']) / data['close']
        
        # æŠ€æœ¯æŒ‡æ ‡
        print("   ğŸ” è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾...")
        if self.use_talib:
            data['rsi_14'] = talib.RSI(data['close'], timeperiod=14)
            data['bb_upper'], data['bb_middle'], data['bb_lower'] = talib.BBANDS(data['close'])
            data['macd'], data['macd_signal'], data['macd_hist'] = talib.MACD(data['close'])
        else:
            # RSIçš„pandaså®ç°
            delta = data['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
            rs = gain / loss
            data['rsi_14'] = 100 - (100 / (1 + rs))
            
            # å¸ƒæ—å¸¦çš„pandaså®ç°
            data['bb_middle'] = data['close'].rolling(20).mean()
            bb_std = data['close'].rolling(20).std()
            data['bb_upper'] = data['bb_middle'] + (bb_std * 2)
            data['bb_lower'] = data['bb_middle'] - (bb_std * 2)
            
            # MACDçš„pandaså®ç°
            ema_12 = data['close'].ewm(span=12).mean()
            ema_26 = data['close'].ewm(span=26).mean()
            data['macd'] = ema_12 - ema_26
            data['macd_signal'] = data['macd'].ewm(span=9).mean()
            data['macd_hist'] = data['macd'] - data['macd_signal']
        
        # å¸ƒæ—å¸¦ä½ç½®å’Œå®½åº¦
        data['bb_position'] = (data['close'] - data['bb_lower']) / (data['bb_upper'] - data['bb_lower'] + 1e-8)
        data['bb_width'] = (data['bb_upper'] - data['bb_lower']) / data['bb_middle']
        
        # æ¸…ç†æ— ç”¨åˆ—
        feature_columns = [col for col in data.columns 
                          if col not in ['datetime', 'open', 'high', 'low', 'close', 'volume', 'turnover', 'open_interest']]
        
        # åˆ›å»ºæ‰‹å·¥ç‰¹å¾ç»“æœï¼Œä¿æŒæ—¶é—´ç´¢å¼•ä¸€è‡´æ€§
        manual_result = data[['close'] + feature_columns].dropna()
        # ä¸ºäº†åç»­åŒ¹é…ï¼Œæ·»åŠ ä¸€ä¸ªæ•°å€¼ç´¢å¼•åˆ—ä½œä¸ºæ—¶é—´æˆ³
        manual_result['time_idx'] = range(len(manual_result))
        print(f"   âœ… æ‰‹å·¥ç‰¹å¾ç”Ÿæˆå®Œæˆï¼Œç‰¹å¾æ•°é‡: {len(feature_columns)}")
        
        # === 2. è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆï¼ˆå¯é€‰ï¼‰===
        if use_auto_features and self.use_tsfresh:
            print("\nğŸ¤– ç”Ÿæˆè‡ªåŠ¨ç‰¹å¾...")
            print(f"   ğŸ“Š çª—å£å¤§å°: {window_size}")
            print(f"   ğŸ”¢ æœ€å¤§ç‰¹å¾æ•°: {max_auto_features}")
            
            try:
                # å‡†å¤‡tsfreshæ ¼å¼çš„æ•°æ® - ä½¿ç”¨æ¸…ç†åçš„æ•°æ®
                clean_data = data.dropna()  # ä½¿ç”¨ä¸manual_resultç›¸åŒçš„æ¸…ç†é€»è¾‘
                tsfresh_data = []
                
                for i in range(window_size, len(clean_data)):
                    window_data = clean_data.iloc[i-window_size:i]
                    for col in ['close', 'volume']:
                        if col in window_data.columns:
                            for j, value in enumerate(window_data[col]):
                                tsfresh_data.append({
                                    'id': i,
                                    'time': j,
                                    'value': value,
                                    'variable': col
                                })
                
                if tsfresh_data:
                    tsfresh_df = pd.DataFrame(tsfresh_data)
                    
                    # æå–ç‰¹å¾ï¼ˆåŒºåˆ†ä¸åŒå˜é‡ç±»å‹ï¼‰
                    from tsfresh.feature_extraction import MinimalFCParameters
                    extracted_features = extract_features(
                        tsfresh_df,
                        column_id='id',
                        column_sort='time',
                        column_value='value',
                        column_kind='variable',  # å…³é”®ï¼šåŒºåˆ†closeå’Œvolumeå˜é‡
                        default_fc_parameters=MinimalFCParameters(),
                        disable_progressbar=True,  # ç¦ç”¨è¿›åº¦æ¡
                        n_jobs=1  # å•çº¿ç¨‹é¿å…æ½œåœ¨é—®é¢˜
                    )
                    
                    # é‡å‘½åç‰¹å¾ï¼Œä¿æŒè¯­ä¹‰æ¸…æ™°ï¼ˆclose__variance -> auto_close__varianceï¼‰
                    renamed_features = {}
                    for col in extracted_features.columns:
                        renamed_features[col] = f'auto_{col}'
                    extracted_features = extracted_features.rename(columns=renamed_features)
                    
                    # é€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾ï¼ˆåŸºäºæ–¹å·®ï¼‰
                    auto_cols_renamed = [col for col in extracted_features.columns if col.startswith('auto_')]
                    if len(auto_cols_renamed) > max_auto_features:
                        # åŸºäºæ–¹å·®é€‰æ‹©top-kç‰¹å¾
                        feature_vars = extracted_features[auto_cols_renamed].var()
                        selected_features = feature_vars.nlargest(max_auto_features).index.tolist()
                        extracted_features = extracted_features[selected_features]
                        print(f"   ğŸ¯ åŸºäºæ–¹å·®é€‰æ‹©äº† {len(selected_features)} ä¸ªæœ€ä¼˜ç‰¹å¾")
                    
                    # åˆ›å»ºä¸´æ—¶æ•°æ®æ¡†
                    temp_auto_df = pd.DataFrame(extracted_features)
                    
                    # åˆ›å»ºè‡ªåŠ¨ç‰¹å¾ç»“æœæ•°æ®æ¡†
                    result_indices = range(window_size, len(clean_data))
                    auto_result = pd.DataFrame({
                        'close': clean_data.iloc[result_indices]['close'].values,
                        'time_idx': result_indices  # ä½¿ç”¨ç›¸åŒçš„æ—¶é—´ç´¢å¼•
                    })
                    
                    # æ·»åŠ è‡ªåŠ¨ç‰¹å¾ï¼ˆæ¸…æ´—å°†åœ¨select_featuresä¸­è¿›è¡Œï¼‰
                    final_auto_cols = [col for col in temp_auto_df.columns if col.startswith('auto_')]
                    for col in final_auto_cols:
                        auto_result[col] = temp_auto_df[col].values
                    
                    auto_result = auto_result.dropna()
                    final_auto_count = len([col for col in auto_result.columns if col.startswith('auto_')])
                    print(f"   âœ… è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆå®Œæˆï¼Œæœ€ç»ˆç‰¹å¾æ•°é‡: {final_auto_count}")
                    
                    # åˆå¹¶æ‰‹å·¥ç‰¹å¾å’Œè‡ªåŠ¨ç‰¹å¾ - ä½¿ç”¨time_idxè¿›è¡ŒåŒ¹é…
                    manual_times = set(manual_result['time_idx'])
                    auto_times = set(auto_result['time_idx'])
                    common_times = manual_times.intersection(auto_times)
                    
                    if common_times and len(common_times) > 0:
                        # æŒ‰time_idxåŒ¹é…æ•°æ®
                        manual_filtered = manual_result[manual_result['time_idx'].isin(common_times)].copy()
                        auto_filtered = auto_result[auto_result['time_idx'].isin(common_times)].copy()
                        
                        # æŒ‰time_idxæ’åºç¡®ä¿å¯¹åº”å…³ç³»æ­£ç¡®
                        manual_filtered = manual_filtered.sort_values('time_idx').reset_index(drop=True)
                        auto_filtered = auto_filtered.sort_values('time_idx').reset_index(drop=True)
                        
                        # åˆå¹¶ç‰¹å¾ï¼ˆä¿ç•™æ‰‹å·¥ç‰¹å¾çš„æ‰€æœ‰åˆ—ï¼Œæ·»åŠ è‡ªåŠ¨ç‰¹å¾ï¼‰
                        auto_features_only = auto_filtered.drop(['close', 'time_idx'], axis=1, errors='ignore')
                        combined_result = pd.concat([manual_filtered, auto_features_only], axis=1)
                        
                        # æ·»åŠ çœŸå®çš„æ—¶é—´ç´¢å¼•
                        if not combined_result.empty:
                            time_indices = combined_result['time_idx'].values
                            combined_result.index = clean_data.index[time_indices]
                            combined_result = combined_result.drop('time_idx', axis=1)  # åˆ é™¤ä¸´æ—¶ç´¢å¼•åˆ—
                        
                        # è®¡ç®—åˆå¹¶åçš„ç‰¹å¾ç»Ÿè®¡
                        total_features = len(combined_result.columns) - 1  # æ’é™¤closeåˆ—
                        manual_features = len([col for col in combined_result.columns if not col.startswith('auto_') and col != 'close'])
                        auto_features = len([col for col in combined_result.columns if col.startswith('auto_')])
                        
                        print(f"âœ… ç‰¹å¾åˆå¹¶å®Œæˆ:")
                        print(f"   ğŸ“ˆ å…±äº«æ ·æœ¬: {len(common_times)}")
                        print(f"   ğŸ”¢ æ€»ç‰¹å¾æ•°: {total_features} (æ‰‹å·¥:{manual_features} + è‡ªåŠ¨:{auto_features})")
                        
                        return combined_result
                    else:
                        print(f"âš ï¸ æ‰‹å·¥ç‰¹å¾å’Œè‡ªåŠ¨ç‰¹å¾æ—¶é—´ä¸åŒ¹é…")
                        print(f"   æ‰‹å·¥ç‰¹å¾æ—¶é—´èŒƒå›´: {len(manual_times)} ä¸ªæ ·æœ¬")
                        print(f"   è‡ªåŠ¨ç‰¹å¾æ—¶é—´èŒƒå›´: {len(auto_times)} ä¸ªæ ·æœ¬")
                        print(f"   é‡å æ ·æœ¬: {len(common_times)} ä¸ª")
                        
            except Exception as e:
                print(f"âš ï¸ è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆå¤±è´¥: {e}")
                print("   ç»§ç»­ä½¿ç”¨æ‰‹å·¥ç‰¹å¾...")
        
        # æ¸…ç†ä¸´æ—¶ç´¢å¼•åˆ—å¹¶è¿”å›æ‰‹å·¥ç‰¹å¾
        if 'time_idx' in manual_result.columns:
            manual_result = manual_result.drop('time_idx', axis=1)
        
        feature_count = len(manual_result.columns) - 1  # æ’é™¤closeåˆ—
        print(f"âœ… ç‰¹å¾ç”Ÿæˆå®Œæˆï¼Œæ‰‹å·¥ç‰¹å¾æ•°é‡: {feature_count}")
        
        return manual_result

    def select_features(self, features_df: pd.DataFrame, final_k: int = 20,
                       variance_threshold: float = 0.01, correlation_threshold: float = 0.95,
                       importance_method: str = 'random_forest', train_ratio: float = 0.8) -> Dict:
        """
        ç»Ÿä¸€çš„ç‰¹å¾é€‰æ‹©ç®¡é“ - æ•´åˆäº†æ–¹å·®è¿‡æ»¤ã€ç›¸å…³æ€§å»é™¤å’Œé‡è¦æ€§é€‰æ‹©
        
        Parameters:
        -----------
        features_df : pd.DataFrame
            ç‰¹å¾æ•°æ®æ¡†ï¼Œåº”åŒ…å«ç›®æ ‡åˆ—'close'
        final_k : int, default=20
            æœ€ç»ˆä¿ç•™çš„ç‰¹å¾æ•°é‡
        variance_threshold : float, default=0.01
            æ–¹å·®é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„ç‰¹å¾å°†è¢«åˆ é™¤
        correlation_threshold : float, default=0.95
            ç›¸å…³æ€§é˜ˆå€¼ï¼Œé«˜äºæ­¤å€¼çš„ç‰¹å¾å¯¹ä¸­ä¿ç•™ä¸€ä¸ª
        importance_method : str, default='random_forest'
            é‡è¦æ€§è¯„ä¼°æ–¹æ³• ('random_forest' æˆ– 'xgboost')
            
        Returns:
        --------
        dict
            åŒ…å«å„æ­¥éª¤ç»“æœçš„ç»¼åˆä¿¡æ¯
        """
        print("ğŸš€ å¼€å§‹ç»¼åˆç‰¹å¾é€‰æ‹©ç®¡é“...")
        # åŠ¨æ€è®¡ç®—åŸå§‹ç‰¹å¾æ•°
        datetime_col = 'datetime' if 'datetime' in features_df.columns else None
        exclude_cols = ['close']
        if datetime_col:
            exclude_cols.append(datetime_col)
        original_feature_count = len(features_df.columns) - len(exclude_cols)
        
        print(f"   ğŸ¯ ç›®æ ‡: ä» {original_feature_count} ä¸ªç‰¹å¾ä¸­é€‰æ‹© {final_k} ä¸ª")
        print("=" * 60)
        
        results = {
            'original_features': original_feature_count,
            'final_k': final_k,
            'pipeline_steps': []
        }
        
        current_df = features_df.copy()
        
        # æ­¥éª¤0: è‡ªåŠ¨ç‰¹å¾æ¸…æ´—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        auto_cols = [col for col in current_df.columns if col.startswith('auto_')]
        if auto_cols:
            print("ğŸ§½ æ­¥éª¤0: è‡ªåŠ¨ç‰¹å¾æ¸…æ´—")
            original_auto_count = len(auto_cols)
            removed_features = []
            removal_reasons = {}
            
            for col in auto_cols:
                data = current_df[col]
                should_remove = False
                reason = ""
                
                # 1. æ£€æŸ¥æ— ç©·å€¼å’ŒNaN
                if not np.isfinite(data).all():
                    should_remove = True
                    reason = "åŒ…å«æ— ç©·å€¼æˆ–NaN"
                elif data.std() < 1e-10:
                    # 2. æ£€æŸ¥å¸¸æ•°ç‰¹å¾
                    should_remove = True
                    reason = "å¸¸æ•°ç‰¹å¾ï¼ˆæ–¹å·®æ¥è¿‘äº0ï¼‰"
                elif abs(data.mean()) > 1e10 or data.std() > 1e10:
                    # 3. æ£€æŸ¥æ•°å€¼èŒƒå›´å¼‚å¸¸
                    should_remove = True
                    reason = f"æ•°å€¼èŒƒå›´å¼‚å¸¸ï¼ˆå‡å€¼:{abs(data.mean()):.2e}, æ ‡å‡†å·®:{data.std():.2e}ï¼‰"
                elif col.endswith('__sum_values'):
                    # 4. ç§»é™¤ç»“æ„æ€§å†—ä½™ç‰¹å¾ï¼ˆçª—å£æ±‚å’Œï¼‰
                    should_remove = True
                    reason = "ç»“æ„æ€§å†—ä½™ï¼ˆçª—å£æ±‚å’Œï¼Œä¸å‡å€¼ç­‰ä»·ï¼‰"
                elif col.endswith('__variance') and any(c.endswith('__standard_deviation') and c.replace('__standard_deviation', '') == col.replace('__variance', '') for c in current_df.columns):
                    # 5. å¦‚æœåŒæ—¶å­˜åœ¨varianceå’Œstandard_deviationï¼Œä¿ç•™åè€…
                    should_remove = True
                    reason = "å·²å­˜åœ¨å¯¹åº”çš„standard_deviationç‰¹å¾"
                else:
                    # 6. æ£€æŸ¥æç«¯åˆ†å¸ƒ
                    try:
                        skew_val = data.skew()
                        kurt_val = data.kurtosis()
                        if abs(skew_val) > 15 or abs(kurt_val) > 200:
                            should_remove = True
                            reason = f"æç«¯åˆ†å¸ƒï¼ˆååº¦:{skew_val:.2f}, å³°åº¦:{kurt_val:.2f}ï¼‰"
                    except:
                        pass
                        
                if should_remove:
                    removed_features.append(col)
                    removal_reasons[col] = reason
            
            # æ‰§è¡Œç§»é™¤
            if removed_features:
                current_df = current_df.drop(columns=removed_features)
                remaining_auto_cols = [col for col in current_df.columns if col.startswith('auto_')]
                
                print(f"   âŒ ç§»é™¤å¼‚å¸¸è‡ªåŠ¨ç‰¹å¾: {len(removed_features)}/{original_auto_count}")
                # æ˜¾ç¤ºå‰3ä¸ªè¢«ç§»é™¤ç‰¹å¾çš„åŸå› 
                for i, feature in enumerate(removed_features[:3]):
                    reason = removal_reasons.get(feature, "æœªçŸ¥åŸå› ")
                    print(f"      {i+1}. {feature}: {reason}")
                if len(removed_features) > 3:
                    print(f"      ... è¿˜æœ‰ {len(removed_features) - 3} ä¸ª")
                    
                print(f"   âœ… ä¿ç•™æœ‰æ•ˆè‡ªåŠ¨ç‰¹å¾: {len(remaining_auto_cols)}ä¸ª")
                
                results['pipeline_steps'].append({
                    'step': 'auto_feature_cleaning',
                    'original_auto_features': original_auto_count,
                    'removed_features': removed_features,
                    'remaining_auto_features': len(remaining_auto_cols),
                    'removal_reasons': removal_reasons
                })
            else:
                print("   âœ… æ‰€æœ‰è‡ªåŠ¨ç‰¹å¾éƒ½é€šè¿‡äº†è´¨é‡æ£€æŸ¥")
                results['pipeline_steps'].append({
                    'step': 'auto_feature_cleaning',
                    'original_auto_features': original_auto_count,
                    'removed_features': [],
                    'remaining_auto_features': original_auto_count,
                    'removal_reasons': {}
                })
        
        # æ­¥éª¤1: æ–¹å·®é˜ˆå€¼è¿‡æ»¤
        print("\nğŸ”¸ æ­¥éª¤1: æ–¹å·®é˜ˆå€¼è¿‡æ»¤")
        # åŠ¨æ€æ£€æŸ¥datetimeåˆ—
        datetime_col = 'datetime' if 'datetime' in current_df.columns else None
        exclude_cols = ['close']
        if datetime_col:
            exclude_cols.append(datetime_col)
        feature_cols = [col for col in current_df.columns if col not in exclude_cols]
        
        if feature_cols:
            # æå–æ•°å€¼ç‰¹å¾
            features_only = current_df[feature_cols].select_dtypes(include=[np.number])
            
            if not features_only.empty:
                # åº”ç”¨æ–¹å·®é˜ˆå€¼è¿‡æ»¤
                selector = VarianceThreshold(threshold=variance_threshold)
                selector.fit(features_only.fillna(0))
                
                # è·å–ä¿ç•™çš„ç‰¹å¾
                selected_mask = selector.get_support()
                removed_features = [col for col, keep in zip(features_only.columns, selected_mask) if not keep]
                kept_features = [col for col, keep in zip(features_only.columns, selected_mask) if keep]
                
                # æ„å»ºç»“æœDataFrame
                result_columns = ['close'] + kept_features
                if datetime_col:
                    result_columns = [datetime_col] + result_columns
                current_df = current_df[result_columns].copy()
                
                print(f"   ğŸ“Š åŸå§‹ç‰¹å¾æ•°: {len(feature_cols)}")
                print(f"   âŒ åˆ é™¤ä½æ–¹å·®ç‰¹å¾: {len(removed_features)}")
                print(f"   âœ… ä¿ç•™ç‰¹å¾æ•°: {len(kept_features)}")
                
                results['pipeline_steps'].append({
                    'step': 'variance_filter',
                    'removed_features': removed_features,
                    'remaining_features': len(kept_features)
                })
            else:
                print("   âš ï¸ æ²¡æœ‰æ•°å€¼å‹ç‰¹å¾ï¼Œè·³è¿‡æ–¹å·®è¿‡æ»¤")
        else:
            print("   âš ï¸ æ²¡æœ‰ç‰¹å¾åˆ—ï¼Œè·³è¿‡æ–¹å·®è¿‡æ»¤")
        
        # æ­¥éª¤2: é«˜å…±çº¿æ€§å»é™¤
        print("\nğŸ”¸ æ­¥éª¤2: é«˜å…±çº¿æ€§ç‰¹å¾å»é™¤")
        # åŠ¨æ€æ£€æŸ¥datetimeåˆ—
        datetime_col = 'datetime' if 'datetime' in current_df.columns else None
        exclude_cols = ['close']
        if datetime_col:
            exclude_cols.append(datetime_col)
        feature_cols = [col for col in current_df.columns if col not in exclude_cols]
        
        if len(feature_cols) >= 2:
            # æå–æ•°å€¼ç‰¹å¾å¹¶è®¡ç®—ç›¸å…³çŸ©é˜µ
            features_only = current_df[feature_cols].select_dtypes(include=[np.number])
            
            if not features_only.empty and len(features_only.columns) >= 2:
                corr_matrix = features_only.corr().fillna(0)
                
                # æ‰¾å‡ºéœ€è¦åˆ é™¤çš„é«˜ç›¸å…³ç‰¹å¾
                removed_features = []
                remaining_features = list(features_only.columns)
                
                for i in range(len(corr_matrix.columns)):
                    for j in range(i+1, len(corr_matrix.columns)):
                        if (corr_matrix.columns[i] in remaining_features and 
                            corr_matrix.columns[j] in remaining_features):
                            if abs(corr_matrix.iloc[i, j]) > correlation_threshold:
                                # åˆ é™¤æ–¹å·®è¾ƒå°çš„ç‰¹å¾
                                var_i = features_only[corr_matrix.columns[i]].var()
                                var_j = features_only[corr_matrix.columns[j]].var()
                                
                                if var_i < var_j:
                                    removed_features.append(corr_matrix.columns[i])
                                    remaining_features.remove(corr_matrix.columns[i])
                                else:
                                    removed_features.append(corr_matrix.columns[j])
                                    remaining_features.remove(corr_matrix.columns[j])
                
                result_columns = ['close'] + remaining_features
                if datetime_col:
                    result_columns = [datetime_col] + result_columns
                current_df = current_df[result_columns].copy()
                
                print(f"   ğŸ“Š è¾“å…¥ç‰¹å¾æ•°: {len(feature_cols)}")
                print(f"   âŒ åˆ é™¤é«˜ç›¸å…³ç‰¹å¾: {len(removed_features)}")
                print(f"   âœ… ä¿ç•™ç‰¹å¾æ•°: {len(remaining_features)}")
                
                results['pipeline_steps'].append({
                    'step': 'correlation_filter',
                    'removed_features': removed_features,
                    'remaining_features': len(remaining_features)
                })
            else:
                print("   âš ï¸ æ•°å€¼ç‰¹å¾ä¸è¶³ï¼Œè·³è¿‡å…±çº¿æ€§æ£€æŸ¥")
        else:
            print("   âš ï¸ ç‰¹å¾æ•°ä¸è¶³2ä¸ªï¼Œè·³è¿‡å…±çº¿æ€§æ£€æŸ¥")
        
        # æ­¥éª¤3: åŸºäºé‡è¦æ€§çš„ç‰¹å¾é€‰æ‹©
        # è®¡ç®—å‰©ä½™ç‰¹å¾æ•°ï¼ˆæ’é™¤closeå’Œå¯èƒ½çš„datetimeåˆ—ï¼‰
        datetime_col = 'datetime' if 'datetime' in current_df.columns else None
        exclude_cols = ['close']
        if datetime_col:
            exclude_cols.append(datetime_col)
        remaining_features = len(current_df.columns) - len(exclude_cols)
        
        if remaining_features > final_k:
            print(f"\nğŸ”¸ æ­¥éª¤3: åŸºäºé‡è¦æ€§é€‰æ‹©Top-{final_k}ç‰¹å¾ (é˜²æ³„æ¼: train_ratio={train_ratio:.1%})")
            
            feature_cols = [col for col in current_df.columns if col not in exclude_cols]
            features_data = current_df[feature_cols].select_dtypes(include=[np.number]).fillna(0)
            
            if not features_data.empty:
                # ========== æ—¶é—´åºåˆ—åˆ‡åˆ†é˜²æ­¢æ•°æ®æ³„æ¼ ==========
                n_samples = len(features_data)
                split_idx = int(n_samples * train_ratio)
                
                # ç¡®ä¿è®­ç»ƒé›†æœ‰è¶³å¤Ÿæ ·æœ¬
                if split_idx < 50:
                    print(f"   âš ï¸ è®­ç»ƒæ ·æœ¬è¿‡å°‘({split_idx}<50)ï¼Œè·³è¿‡é‡è¦æ€§é€‰æ‹©")
                    feature_importance_dict = {}
                else:
                    print(f"   ğŸ“Š æ—¶é—´åˆ‡åˆ†: è®­ç»ƒé›† {split_idx}/{n_samples} ({train_ratio:.1%})")
                    
                    # åªä½¿ç”¨è®­ç»ƒé›†è®¡ç®—ç‰¹å¾é‡è¦æ€§
                    train_features = features_data.iloc[:split_idx].copy()
                    train_close = current_df['close'].iloc[:split_idx]
                    
                    # ç”Ÿæˆç›®æ ‡å˜é‡ï¼ˆåªåœ¨è®­ç»ƒé›†å†…ï¼‰
                    importance_results = {}
                    combined_importance = pd.Series(0.0, index=train_features.columns)
                    
                    targets = {
                        'return_1d': train_close.pct_change().shift(-1),
                        'return_5d': train_close.pct_change(5).shift(-5), 
                        'return_10d': train_close.pct_change(10).shift(-10)
                    }
                    
                    valid_targets = 0
                    for target_name, target_values in targets.items():
                        try:
                            # å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆç¡®ä¿ç›®æ ‡å€¼æœ‰æ•ˆä¸”ä¸ä½¿ç”¨æœªæ¥æ•°æ®ï¼‰
                            valid_mask = ~(target_values.isna() | train_features.isna().any(axis=1))
                            valid_count = valid_mask.sum()
                            
                            if valid_count < 30:  # æ¯ä¸ªç›®æ ‡è‡³å°‘30ä¸ªæ ·æœ¬
                                print(f"     âš ï¸ {target_name}: æœ‰æ•ˆæ ·æœ¬ä¸è¶³({valid_count}<30)")
                                continue
                                
                            X_train = train_features[valid_mask]
                            y_train = target_values[valid_mask]
                            
                            # é€‰æ‹©æ¨¡å‹
                            if importance_method == 'xgboost' and self.use_xgboost:
                                model = XGBRegressor(
                                    n_estimators=100, 
                                    max_depth=6,
                                    learning_rate=0.1,
                                    random_state=42, 
                                    verbosity=0
                                )
                            else:
                                model = RandomForestRegressor(
                                    n_estimators=100, 
                                    max_depth=10,
                                    min_samples_leaf=5,
                                    random_state=42, 
                                    n_jobs=-1
                                )
                            
                            # è®­ç»ƒæ¨¡å‹å¹¶è·å–ç‰¹å¾é‡è¦æ€§
                            model.fit(X_train, y_train)
                            feature_importance = pd.Series(model.feature_importances_, index=X_train.columns)
                            
                            # æ ‡å‡†åŒ–é‡è¦æ€§åˆ†æ•°é¿å…åç½®
                            if feature_importance.sum() > 0:
                                feature_importance = feature_importance / feature_importance.sum()
                                importance_results[target_name] = feature_importance
                                combined_importance += feature_importance
                                valid_targets += 1
                                print(f"     âœ… {target_name}: {valid_count}æ ·æœ¬, Topç‰¹å¾: {feature_importance.nlargest(3).index.tolist()}")
                            
                        except Exception as e:
                            print(f"     âŒ {target_name}: è®¡ç®—å¤±è´¥ - {str(e)}")
                            continue
                
                    if valid_targets > 0 and combined_importance.sum() > 0:
                        # é€‰æ‹©top-kç‰¹å¾ï¼ˆåŸºäºæ— æ³„æ¼çš„é‡è¦æ€§åˆ†æ•°ï¼‰
                        top_features = combined_importance.nlargest(final_k).index.tolist()
                        
                        # æ„å»ºç»“æœDataFrameï¼ˆåº”ç”¨åˆ°å…¨é‡æ•°æ®ä½†ä¸é‡æ–°è®­ç»ƒï¼‰
                        result_columns = ['close'] + top_features
                        if datetime_col:
                            result_columns = [datetime_col] + result_columns
                        current_df = current_df[result_columns].copy()
                        
                        print(f"   ğŸ“Š è¾“å…¥ç‰¹å¾æ•°: {remaining_features}")
                        print(f"   âœ… é€‰æ‹©ç‰¹å¾æ•°: {len(top_features)}")
                        print(f"   ğŸ¯ æœ‰æ•ˆç›®æ ‡æ•°: {valid_targets}/3")
                        print(f"   ğŸ† Top-5ç‰¹å¾: {top_features[:5]}")
                        
                        # ä¿å­˜ç‰¹å¾é‡è¦æ€§ç”¨äºè¿”å›
                        feature_importance_dict = dict(combined_importance.nlargest(final_k))
                        
                        results['pipeline_steps'].append({
                            'step': 'importance_selection',
                            'method': importance_method,
                            'train_ratio': train_ratio,
                            'train_samples': split_idx,
                            'valid_targets': valid_targets,
                            'selected_features': top_features,
                            'feature_importance': feature_importance_dict
                        })
                    else:
                        print("   âš ï¸ æ— æœ‰æ•ˆç›®æ ‡æˆ–é‡è¦æ€§è®¡ç®—å¤±è´¥ï¼Œä¿æŒå½“å‰ç‰¹å¾")
                        feature_importance_dict = {}
            else:
                print("   âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„æ•°å€¼ç‰¹å¾")
        else:
            print(f"\nâœ… å½“å‰ç‰¹å¾æ•°({remaining_features})å·²æ»¡è¶³ç›®æ ‡ï¼Œè·³è¿‡é‡è¦æ€§é€‰æ‹©")
            feature_importance_dict = {}
            results['pipeline_steps'].append({
                'step': 'importance_selection',
                'skipped': True,
                'reason': f'features_count({remaining_features}) <= target({final_k})',
                'remaining_features': remaining_features
            })
        
        # æœ€ç»ˆç»“æœ
        datetime_col = 'datetime' if 'datetime' in current_df.columns else None
        exclude_cols = ['close']
        if datetime_col:
            exclude_cols.append(datetime_col)
        final_features = [col for col in current_df.columns if col not in exclude_cols]
        results.update({
            'final_features_df': current_df,
            'final_features': final_features,
            'final_features_count': len(final_features),
            'feature_importance': feature_importance_dict,
            'reduction_ratio': (results['original_features'] - len(final_features)) / results['original_features']
        })
        
        print("\n" + "=" * 60)
        print("ğŸ‰ ç»¼åˆç‰¹å¾é€‰æ‹©ç®¡é“å®Œæˆ!")
        print(f"   ğŸ“Š åŸå§‹ç‰¹å¾æ•°: {results['original_features']}")
        print(f"   âœ… æœ€ç»ˆç‰¹å¾æ•°: {len(final_features)}")
        print(f"   ğŸ“‰ ç‰¹å¾å‰Šå‡ç‡: {results['reduction_ratio']:.1%}")
        
        # æ˜¾ç¤ºç®¡é“æ­¥éª¤ç»Ÿè®¡
        if results['pipeline_steps']:
            auto_clean_step = next((s for s in results['pipeline_steps'] if s['step'] == 'auto_feature_cleaning'), None)
            if auto_clean_step and auto_clean_step['removed_features']:
                print(f"   ğŸ§½ è‡ªåŠ¨ç‰¹å¾æ¸…æ´—: ç§»é™¤ {len(auto_clean_step['removed_features'])} ä¸ª")
        
        if final_features:
            print(f"   ğŸ† æœ€ç»ˆTop-10ç‰¹å¾: {final_features[:10]}")
        
        return results

    def scale_features(self, features_df: pd.DataFrame, 
                       scaler_type: str = 'robust',
                       train_ratio: float = 0.8,
                       save_path: str = 'scaler.pkl',
                       exclude_cols: Optional[List[str]] = None) -> Dict:
        """
        å¯¹ç‰¹å¾åšå°ºåº¦æ ‡å‡†åŒ–ï¼ˆæ—¶é—´åºåˆ—é˜²æ³„æ¼ï¼šä»…ç”¨è®­ç»ƒæ®µ fitï¼Œå…¶ä½™æ®µ transformï¼‰
        
        Parameters
        ----------
        features_df : pd.DataFrame
            å·²å®Œæˆç‰¹å¾é€‰æ‹©çš„ç‰¹å¾æ•°æ®ï¼ŒåŒ…å« 'close'
        scaler_type : str, default 'robust'
            ç¼©æ”¾æ–¹å¼: 'robust' | 'standard' | 'minmax'
        train_ratio : float, default 0.8
            è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆæ—¶é—´åˆ‡åˆ†ï¼‰
        save_path : str
            æŒä¹…åŒ–ç¼©æ”¾å™¨è·¯å¾„ï¼ˆpickleï¼‰
        exclude_cols : list
            ä¸å‚ä¸ç¼©æ”¾çš„åˆ—ï¼ˆé»˜è®¤: ['close'] + datetimeï¼‰
        
        Returns
        -------
        dict:
            {
              'scaled_df': ç¼©æ”¾åçš„æ•°æ®ï¼ˆä¿æŒåŸç´¢å¼•ä¸åˆ—é¡ºåºï¼‰
              'scaler': å·²æ‹Ÿåˆç¼©æ”¾å™¨å¯¹è±¡
              'train_index': è®­ç»ƒåŒºé—´ç´¢å¼•
              'valid_index': éªŒè¯/æœªæ¥åŒºé—´ç´¢å¼•
              'feature_cols': å®é™…ç¼©æ”¾çš„ç‰¹å¾åˆ—
              'scaler_path': ä¿å­˜è·¯å¾„
            }
        """
        print("ğŸ“ å¼€å§‹ç‰¹å¾æ ‡å‡†åŒ–...")
        
        df = features_df.copy()
        if df.empty:
            raise ValueError("scale_features: è¾“å…¥çš„ç‰¹å¾æ•°æ®ä¸ºç©º")
        
        # è¯†åˆ«æ’é™¤åˆ—
        datetime_col = 'datetime' if 'datetime' in df.columns else None
        if exclude_cols is None:
            exclude_cols = ['close']
            if datetime_col:
                exclude_cols.append(datetime_col)
        
        feature_cols = [c for c in df.columns if c not in exclude_cols]
        if not feature_cols:
            raise ValueError("scale_features: æ²¡æœ‰å¯ç¼©æ”¾çš„ç‰¹å¾åˆ—")
        
        # æ—¶é—´åˆ‡åˆ†ï¼ˆä¿æŒä¸ç‰¹å¾é€‰æ‹©ä¸€è‡´çš„é€»è¾‘ï¼‰
        n_samples = len(df)
        split_idx = int(n_samples * train_ratio)
        if split_idx < 30:
            raise ValueError(f"scale_features: è®­ç»ƒé›†æ ·æœ¬è¿‡å°‘({split_idx})ï¼Œæ— æ³•æ‹Ÿåˆç¼©æ”¾å™¨")
        
        train_index = df.index[:split_idx]
        valid_index = df.index[split_idx:]
        
        print(f"   ğŸ“Š æ—¶é—´åˆ‡åˆ†: è®­ç»ƒé›† {split_idx}/{n_samples} ({train_ratio:.1%})")
        print(f"   ğŸ“… è®­ç»ƒæ®µ: {train_index.min().date()} ~ {train_index.max().date()}")
        if len(valid_index) > 0:
            print(f"   ğŸ“… éªŒè¯æ®µ: {valid_index.min().date()} ~ {valid_index.max().date()}")
        
        train_X = df.loc[train_index, feature_cols]
        valid_X = df.loc[valid_index, feature_cols] if len(valid_index) > 0 else None
        
        # é€‰æ‹©ç¼©æ”¾å™¨
        if scaler_type == 'robust':
            scaler = RobustScaler()
            print(f"   ğŸ”§ ä½¿ç”¨ RobustScaler (ä¸­ä½æ•°-IQRæ ‡å‡†åŒ–ï¼Œé€‚åˆé‡‘èæ•°æ®)")
        elif scaler_type == 'standard':
            scaler = StandardScaler()
            print(f"   ğŸ”§ ä½¿ç”¨ StandardScaler (å‡å€¼-æ ‡å‡†å·®æ ‡å‡†åŒ–)")
        elif scaler_type == 'minmax':
            scaler = MinMaxScaler()
            print(f"   ğŸ”§ ä½¿ç”¨ MinMaxScaler (æœ€å°-æœ€å¤§å€¼æ ‡å‡†åŒ–)")
        else:
            raise ValueError("scaler_type å¿…é¡»æ˜¯ 'robust' | 'standard' | 'minmax'")
        
        # æ‹Ÿåˆ + å˜æ¢ï¼ˆåªåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆï¼‰
        print(f"   ğŸ¯ åœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆç¼©æ”¾å™¨...")
        scaler.fit(train_X.fillna(0))  # å¤„ç†å¯èƒ½çš„ç¼ºå¤±å€¼
        scaled_train = scaler.transform(train_X.fillna(0))
        
        if valid_X is not None:
            print(f"   ğŸ”„ å¯¹éªŒè¯é›†è¿›è¡Œå˜æ¢...")
            scaled_valid = scaler.transform(valid_X.fillna(0))
        
        # å›å¡«ç»“æœï¼ˆä¿æŒåŸæœ‰çš„åˆ—ç»“æ„å’Œç´¢å¼•ï¼‰
        scaled_df = df.copy()
        scaled_df.loc[train_index, feature_cols] = scaled_train
        if valid_X is not None:
            scaled_df.loc[valid_index, feature_cols] = scaled_valid
        
        # æŒä¹…åŒ–ç¼©æ”¾å™¨å’Œå…ƒæ•°æ®
        try:
            # ä¿å­˜ç¼©æ”¾å™¨å’Œå…ƒæ•°æ®
            scaler_data = {
                'scaler': scaler,
                'feature_cols': feature_cols,
                'scaler_type': scaler_type,
                'train_ratio': train_ratio,
                'train_samples': split_idx,
                'total_samples': n_samples,
                'train_range': (str(train_index.min().date()), str(train_index.max().date())),
                'valid_range': (str(valid_index.min().date()), str(valid_index.max().date())) if len(valid_index) > 0 else None,
                'fit_timestamp': datetime.now().isoformat(),
                'feature_count': len(feature_cols)
            }
            
            with open(save_path, 'wb') as f:
                pickle.dump(scaler_data, f)
            
            # å¦å¤–ä¿å­˜ä¸€ä¸ªå¯è¯»çš„å…ƒæ•°æ®æ–‡ä»¶
            meta_path = save_path.replace('.pkl', '_meta.json')
            readable_meta = {k: v for k, v in scaler_data.items() if k != 'scaler'}  # æ’é™¤ä¸èƒ½JSONåºåˆ—åŒ–çš„scalerå¯¹è±¡
            with open(meta_path, 'w', encoding='utf-8') as f:
                json.dump(readable_meta, f, indent=2, ensure_ascii=False)
                
            print(f"   âœ… ç¼©æ”¾å™¨å·²ä¿å­˜: {save_path}")
            print(f"   ğŸ“‹ å…ƒæ•°æ®å·²ä¿å­˜: {meta_path}")
            
        except Exception as e:
            print(f"   âš ï¸ ç¼©æ”¾å™¨ä¿å­˜å¤±è´¥: {e}")
        
        # è®¡ç®—ç¼©æ”¾å‰åçš„ç»Ÿè®¡ä¿¡æ¯
        original_stats = train_X.describe()
        scaled_stats = pd.DataFrame(scaled_train, columns=feature_cols).describe()
        
        print(f"\nğŸ“Š ç¼©æ”¾æ•ˆæœç»Ÿè®¡:")
        print(f"   ğŸ”¢ ç¼©æ”¾ç‰¹å¾æ•°: {len(feature_cols)}")
        print(f"   ğŸ“ˆ åŸå§‹æ•°æ®èŒƒå›´: å‡å€¼ [{original_stats.loc['mean'].min():.4f}, {original_stats.loc['mean'].max():.4f}]")
        print(f"   ğŸ“‰ ç¼©æ”¾åèŒƒå›´: å‡å€¼ [{scaled_stats.loc['mean'].min():.4f}, {scaled_stats.loc['mean'].max():.4f}]")
        print(f"   ğŸ“Š åŸå§‹æ ‡å‡†å·®: [{original_stats.loc['std'].min():.4f}, {original_stats.loc['std'].max():.4f}]")
        print(f"   ğŸ“Š ç¼©æ”¾åæ ‡å‡†å·®: [{scaled_stats.loc['std'].min():.4f}, {scaled_stats.loc['std'].max():.4f}]")
        
        # æ˜¾ç¤ºç¼©æ”¾æœ€å‰§çƒˆçš„ç‰¹å¾
        original_ranges = original_stats.loc['max'] - original_stats.loc['min']
        scaled_ranges = scaled_stats.loc['max'] - scaled_stats.loc['min']
        scale_ratios = original_ranges / (scaled_ranges + 1e-8)
        top_scaled_features = scale_ratios.nlargest(5)
        
        print(f"\nğŸ¯ ç¼©æ”¾æ•ˆæœæœ€æ˜æ˜¾çš„ç‰¹å¾:")
        for i, (feature, ratio) in enumerate(top_scaled_features.items(), 1):
            orig_range = original_ranges[feature]
            scaled_range = scaled_ranges[feature]
            print(f"   {i}. {feature}: {orig_range:.4f} â†’ {scaled_range:.4f} (å‹ç¼© {ratio:.1f}x)")
        
        return {
            'scaled_df': scaled_df,
            'scaler': scaler,
            'train_index': train_index,
            'valid_index': valid_index,
            'feature_cols': feature_cols,
            'scaler_path': save_path,
            'meta_path': meta_path,
            'scaler_type': scaler_type,
            'train_samples': split_idx,
            'feature_count': len(feature_cols)
        }

    def analyze_features(self, features_df: pd.DataFrame) -> Dict:
        """
        åˆ†æç‰¹å¾åˆ†å¸ƒå’Œè´¨é‡ï¼ˆåº”åœ¨ç‰¹å¾é€‰æ‹©ä¹‹åä½¿ç”¨ï¼‰
        
        Parameters:
        -----------
        features_df : pd.DataFrame
            ç‰¹å¾æ•°æ®ï¼ˆå·²ç»è¿‡ç‰¹å¾é€‰æ‹©çš„æ•°æ®ï¼‰
            
        Returns:
        --------
        dict
            ç‰¹å¾åˆ†æç»“æœ
        """
        print("ğŸ“Š å¼€å§‹ç‰¹å¾åˆ†æ...")
        
        # é¦–å…ˆå±•ç¤ºç‰¹å¾æ•°æ®é¢„è§ˆ
        self._display_feature_preview(features_df, "åˆ†æç‰¹å¾")
        
        # åŠ¨æ€æ£€æµ‹datetimeåˆ—
        datetime_col = 'datetime' if 'datetime' in features_df.columns else None
        exclude_cols = ['close']
        if datetime_col:
            exclude_cols.append(datetime_col)
        feature_cols = [col for col in features_df.columns if col not in exclude_cols]
        
        analysis = {
            'total_features': len(feature_cols),
            'missing_values': {},
            'extreme_values': {},
            'distributions': {}
        }
        
        # ç¼ºå¤±å€¼åˆ†æ
        for col in feature_cols:
            missing_count = features_df[col].isnull().sum()
            missing_pct = missing_count / len(features_df) * 100
            if missing_count > 0:
                analysis['missing_values'][col] = {
                    'count': missing_count,
                    'percentage': missing_pct
                }
        
        # æå€¼åˆ†æ
        for col in feature_cols:
            if features_df[col].dtype in ['float64', 'int64']:
                q1 = features_df[col].quantile(0.25)
                q3 = features_df[col].quantile(0.75)
                iqr = q3 - q1
                lower_bound = q1 - 1.5 * iqr
                upper_bound = q3 + 1.5 * iqr
                
                outliers = ((features_df[col] < lower_bound) | (features_df[col] > upper_bound)).sum()
                if outliers > 0:
                    analysis['extreme_values'][col] = {
                        'count': outliers,
                        'percentage': outliers / len(features_df) * 100,
                        'bounds': (lower_bound, upper_bound)
                    }
        
        # åˆ†å¸ƒç»Ÿè®¡
        numeric_features = features_df[feature_cols].select_dtypes(include=[np.number])
        analysis['distributions'] = {
            'mean': numeric_features.mean().to_dict(),
            'std': numeric_features.std().to_dict(),
            'skewness': numeric_features.skew().to_dict(),
            'kurtosis': numeric_features.kurtosis().to_dict()
        }
        
        # è¾“å‡ºåˆ†æç»“æœ
        print("\nğŸ“ˆ ç‰¹å¾åˆ†æç»“æœ:")
        print(f"   ğŸ”¢ æ€»ç‰¹å¾æ•°: {analysis['total_features']}")
        print(f"   âŒ ç¼ºå¤±å€¼ç‰¹å¾: {len(analysis['missing_values'])}")
        print(f"   âš ï¸ å¼‚å¸¸å€¼ç‰¹å¾: {len(analysis['extreme_values'])}")
        
        if analysis['missing_values']:
            print("   ğŸ“‹ ç¼ºå¤±å€¼è¯¦æƒ…:")
            for col, info in analysis['missing_values'].items():
                print(f"      {col}: {info['count']} ({info['percentage']:.1f}%)")
        
        if analysis['extreme_values']:
            print("   ğŸ“‹ å¼‚å¸¸å€¼è¯¦æƒ…:")
            for col, info in list(analysis['extreme_values'].items())[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                print(f"      {col}: {info['count']} ({info['percentage']:.1f}%)")
        

        
        # æ·»åŠ æ›´å¤šç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š æ•´ä½“æ•°æ®è´¨é‡è¯„ä¼°:")
        total_cells = len(features_df) * len(feature_cols)
        missing_cells = sum(analysis['missing_values'][col]['count'] for col in analysis['missing_values'])
        data_completeness = (total_cells - missing_cells) / total_cells * 100 if total_cells > 0 else 100
        print(f"   ğŸ“Š æ•°æ®å®Œæ•´æ€§: {data_completeness:.1f}%")
        print(f"   ğŸ“ˆ æ•°æ®èŒƒå›´: {features_df.index.min().date()} ~ {features_df.index.max().date()}")
        print(f"   ğŸ“… æ•°æ®ç‚¹æ•°: {len(features_df)} ä¸ªæ—¶é—´ç‚¹")
        
        return analysis
    
    def _display_feature_preview(self, features_df: pd.DataFrame, data_type: str = "ç‰¹å¾"):
        """
        æ™ºèƒ½å±•ç¤ºç‰¹å¾æ•°æ®é¢„è§ˆ
        
        Parameters:
        -----------
        features_df : pd.DataFrame
            ç‰¹å¾æ•°æ®æ¡†
        data_type : str
            æ•°æ®ç±»å‹æè¿°
        """
        print(f"\nğŸ“Š {data_type}æ•°æ®é¢„è§ˆ:")
        print("=" * 50)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"ğŸ“ˆ æ•°æ®å½¢çŠ¶: {features_df.shape[0]} è¡Œ Ã— {features_df.shape[1]} åˆ—")
        print(f"ğŸ“… æ—¶é—´èŒƒå›´: {features_df.index.min().date()} åˆ° {features_df.index.max().date()}")
        
        # ç‰¹å¾åˆ—ï¼ˆæ’é™¤closeåˆ—ï¼‰
        feature_cols = [col for col in features_df.columns if col != 'close']
        print(f"ğŸ”¢ ç‰¹å¾æ•°é‡: {len(feature_cols)}")
        
        # æ•°æ®è´¨é‡æ¦‚è§ˆ
        missing_info = features_df.isnull().sum()
        missing_features = missing_info[missing_info > 0]
        print(f"âŒ ç¼ºå¤±å€¼ç‰¹å¾: {len(missing_features)} ä¸ª")
        
        # å±•ç¤ºç­–ç•¥ï¼šæ ¹æ®ç‰¹å¾æ•°é‡å†³å®šå±•ç¤ºè¯¦ç»†ç¨‹åº¦
        if len(feature_cols) <= 10:
            # ç‰¹å¾è¾ƒå°‘ï¼šå±•ç¤ºæ‰€æœ‰ç‰¹å¾çš„è¯¦ç»†ä¿¡æ¯
            print("\nğŸ“‹ æ‰€æœ‰ç‰¹å¾è¯¦æƒ…:")
            for i, col in enumerate(feature_cols, 1):
                stats = features_df[col].describe()
                missing_pct = (features_df[col].isnull().sum() / len(features_df)) * 100
                print(f"  {i:2d}. {col:<25} | å‡å€¼:{stats['mean']:8.4f} | æ ‡å‡†å·®:{stats['std']:8.4f} | ç¼ºå¤±:{missing_pct:5.1f}%")
                
        elif len(feature_cols) <= 30:
            # ç‰¹å¾é€‚ä¸­ï¼šå±•ç¤ºå‰10ä¸ªç‰¹å¾çš„è¯¦ç»†ä¿¡æ¯ + ç»Ÿè®¡æ¦‚è§ˆ
            print("\nğŸ“‹ å‰10ä¸ªç‰¹å¾è¯¦æƒ…:")
            for i, col in enumerate(feature_cols[:10], 1):
                stats = features_df[col].describe()
                missing_pct = (features_df[col].isnull().sum() / len(features_df)) * 100
                print(f"  {i:2d}. {col:<25} | å‡å€¼:{stats['mean']:8.4f} | æ ‡å‡†å·®:{stats['std']:8.4f} | ç¼ºå¤±:{missing_pct:5.1f}%")
            
            if len(feature_cols) > 10:
                print(f"  ... è¿˜æœ‰ {len(feature_cols) - 10} ä¸ªç‰¹å¾")
                
        else:
            # ç‰¹å¾å¾ˆå¤šï¼šåªå±•ç¤ºç»Ÿè®¡æ¦‚è§ˆå’Œç‰¹å¾åç§°åˆ†ç±»
            print("\nğŸ“‹ ç‰¹å¾ç»Ÿè®¡æ¦‚è§ˆ:")
            numeric_features = features_df[feature_cols].select_dtypes(include=[np.number])
            overall_stats = numeric_features.describe().T
            
            print(f"  å¹³å‡å€¼èŒƒå›´: {overall_stats['mean'].min():.4f} ~ {overall_stats['mean'].max():.4f}")
            print(f"  æ ‡å‡†å·®èŒƒå›´: {overall_stats['std'].min():.4f} ~ {overall_stats['std'].max():.4f}")
            print(f"  æœ€å°å€¼èŒƒå›´: {overall_stats['min'].min():.4f} ~ {overall_stats['min'].max():.4f}")
            print(f"  æœ€å¤§å€¼èŒƒå›´: {overall_stats['max'].min():.4f} ~ {overall_stats['max'].max():.4f}")
            
            # ç‰¹å¾åˆ†ç±»å±•ç¤º
            print("\nğŸ·ï¸  ç‰¹å¾åˆ†ç±»:")
            
            categories = {
                'æ”¶ç›Šç‡ç‰¹å¾': [col for col in feature_cols if 'return' in col],
                'åŠ¨é‡ç‰¹å¾': [col for col in feature_cols if 'momentum' in col],
                'æ»šåŠ¨ç»Ÿè®¡': [col for col in feature_cols if 'rolling' in col],
                'æ³¢åŠ¨ç‡ç‰¹å¾': [col for col in feature_cols if any(x in col for x in ['volatility', 'atr', 'skewness', 'kurtosis'])],
                'æˆäº¤é‡ç‰¹å¾': [col for col in feature_cols if 'volume' in col],
                'ä»·æ ¼ç‰¹å¾': [col for col in feature_cols if any(x in col for x in ['price', 'high', 'low', 'open', 'ratio'])],
                'æŠ€æœ¯æŒ‡æ ‡': [col for col in feature_cols if any(x in col for x in ['rsi', 'bb', 'macd'])],
                'è‡ªåŠ¨ç‰¹å¾': [col for col in feature_cols if col.startswith('auto_')],
                'å…¶ä»–ç‰¹å¾': [col for col in feature_cols if not any([
                    'return' in col, 'momentum' in col, 'rolling' in col,
                    any(x in col for x in ['volatility', 'atr', 'skewness', 'kurtosis']),
                    'volume' in col, any(x in col for x in ['price', 'high', 'low', 'open', 'ratio']),
                    any(x in col for x in ['rsi', 'bb', 'macd']), col.startswith('auto_')
                ])]
            }
            
            for category, features in categories.items():
                if features:
                    print(f"  ğŸ“Œ {category} ({len(features)}ä¸ª): {', '.join(features[:5])}{'...' if len(features) > 5 else ''}")
        
        # æ•°æ®æ ·æœ¬é¢„è§ˆï¼ˆå‰5è¡Œï¼Œé‡è¦åˆ—ï¼‰
        print("\nğŸ“„ æ•°æ®æ ·æœ¬é¢„è§ˆï¼ˆå‰5è¡Œï¼‰:")
        preview_cols = ['close'] + feature_cols[:min(6, len(feature_cols))]  # close + æœ€å¤š6ä¸ªç‰¹å¾
        preview_data = features_df[preview_cols].head()
        
        # æ ¼å¼åŒ–æ˜¾ç¤º
        pd.set_option('display.max_columns', 10)
        pd.set_option('display.width', 120)
        pd.set_option('display.float_format', '{:.4f}'.format)
        print(preview_data)
        pd.reset_option('display.max_columns')
        pd.reset_option('display.width')
        pd.reset_option('display.float_format')
        
        # ä¿å­˜åˆ°æ–‡ä»¶æç¤º
        if len(features_df) > 100 or len(feature_cols) > 20:
            print("\nğŸ’¾ æç¤º: æ•°æ®è¾ƒå¤§ï¼Œå¯ä½¿ç”¨ features_df.to_csv('features.csv') ä¿å­˜å®Œæ•´æ•°æ®")
            
        print("=" * 50)
    



if __name__ == "__main__":
    """
    ä½¿ç”¨ç¤ºä¾‹ - ä»…ç”¨äºæ¼”ç¤ºçœŸå®æ•°æ®åŠ è½½å’Œç‰¹å¾å·¥ç¨‹
    """
    print("ğŸ¯ è‚¡ç¥¨ç‰¹å¾å·¥ç¨‹ - çœŸå®æ•°æ®ç‰ˆæœ¬")
    print("=" * 50)
    
    try:
        # åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹å™¨
        engineer = FeatureEngineer(use_talib=True, use_tsfresh=True)
        
        # ç¤ºä¾‹ï¼šåŠ è½½å¹³å®‰é“¶è¡Œ(000001)æœ€è¿‘1å¹´çš„æ•°æ®
        symbol = "000001"
        start_date = "2023-01-01" 
        end_date = "2024-12-31"
        
        # åŠ è½½çœŸå®æ•°æ®
        data = engineer.load_stock_data(symbol, start_date, end_date)
        
        if len(data) < 100:
            print("âš ï¸ æ•°æ®é‡ä¸è¶³100å¤©ï¼Œå»ºè®®æ‰©å¤§æ—¶é—´èŒƒå›´")
        
        # ç”Ÿæˆç‰¹å¾
        print("\nğŸ­ ç”ŸæˆæŠ€æœ¯ç‰¹å¾...")
        features_df = engineer.prepare_features(
            data, 
            use_auto_features=True,  # å¯ç”¨è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆ
            window_size=20,
            max_auto_features=30  # é™åˆ¶è‡ªåŠ¨ç‰¹å¾æ•°é‡
        )
        total_features = features_df.shape[1] - 1
        manual_count = len([col for col in features_df.columns if not col.startswith('auto_') and col != 'close'])
        auto_count = len([col for col in features_df.columns if col.startswith('auto_')])
        print(f"âœ… æˆåŠŸç”Ÿæˆ {total_features} ä¸ªç‰¹å¾ (æ‰‹å·¥:{manual_count} + è‡ªåŠ¨:{auto_count})")
        
        # ç‰¹å¾é€‰æ‹©
        print("\nğŸ¯ æ‰§è¡Œç‰¹å¾é€‰æ‹©...")
        selection_results = engineer.select_features(
            features_df,
            final_k=20,
            variance_threshold=0.01,
            correlation_threshold=0.9,
            train_ratio=0.8  # åªç”¨80%çš„å†å²æ•°æ®è®¡ç®—ç‰¹å¾é‡è¦æ€§ï¼Œé˜²æ­¢æ•°æ®æ³„æ¼
        )
        
        final_features = selection_results['final_features']
        print(f"âœ… æœ€ç»ˆé€‰æ‹© {len(final_features)} ä¸ªé‡è¦ç‰¹å¾")
        
        # ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆæ–°å¢æ­¥éª¤ï¼‰
        print("\nï¿½ æ‰§è¡Œç‰¹å¾æ ‡å‡†åŒ–...")
        scale_results = engineer.scale_features(
            selection_results['final_features_df'],
            scaler_type='robust',  # é‡‘èæ•°æ®æ¨èä½¿ç”¨RobustScaler
            train_ratio=0.8,       # ä¸ç‰¹å¾é€‰æ‹©ä¿æŒä¸€è‡´çš„æ—¶é—´åˆ‡åˆ†
            save_path='feature_scaler.pkl'
        )
        scaled_df = scale_results['scaled_df']
        print(f"âœ… ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆï¼Œç¼©æ”¾å™¨å·²ä¿å­˜åˆ° {scale_results['scaler_path']}")
        
        # ç‰¹å¾åˆ†æï¼ˆä½¿ç”¨æ ‡å‡†åŒ–åçš„æ•°æ®ï¼‰
        print("\nğŸ“Š åˆ†ææ ‡å‡†åŒ–åçš„ç‰¹å¾è´¨é‡...")
        analysis = engineer.analyze_features(scaled_df)
        
        print(f"\nğŸ“‹ å¤„ç†å®Œæˆï¼")
        print(f"   ğŸ”¢ åŸå§‹æ•°æ®: {len(data)} å¤©")
        print(f"   ğŸ­ ç”Ÿæˆç‰¹å¾: {features_df.shape[1]-1} ä¸ª")
        print(f"   ğŸ¯ æœ€ç»ˆç‰¹å¾: {len(final_features)} ä¸ª")
        print(f"   ï¿½ æ ‡å‡†åŒ–ç‰¹å¾: {scale_results['feature_count']} ä¸ª")
        print(f"   ï¿½ğŸ“Š ç‰¹å¾è´¨é‡: {analysis['total_features'] - len(analysis['missing_values'])} ä¸ªæ— ç¼ºå¤±å€¼")
        
        print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
        print("   1. engineer.load_stock_data() - åŠ è½½çœŸå®è‚¡ç¥¨æ•°æ®")
        print("   2. engineer.prepare_features() - ç”ŸæˆæŠ€æœ¯ç‰¹å¾")
        print("   3. engineer.select_features() - æ‰§è¡Œç‰¹å¾é€‰æ‹©")
        print("   4. engineer.scale_features() - ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆé˜²æ³„æ¼ï¼‰")
        print("   5. engineer.analyze_features() - åˆ†æç‰¹å¾è´¨é‡")
        
        print(f"\nğŸ’¾ è¾“å‡ºæ–‡ä»¶:")
        print(f"   ğŸ“¦ ç‰¹å¾ç¼©æ”¾å™¨: {scale_results['scaler_path']}")
        print(f"   ğŸ“‹ ç¼©æ”¾å…ƒæ•°æ®: {scale_results['meta_path']}")
        print("   ğŸ“Š å¯ç”¨ scaled_df.to_csv('scaled_features.csv') ä¿å­˜æ ‡å‡†åŒ–ç‰¹å¾")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {e}")
        print("è¯·æ£€æŸ¥:")
        print("  1. InfluxDBæœåŠ¡æ˜¯å¦è¿è¡Œ")
        print("  2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        print("  3. è‚¡ç¥¨ä»£ç å’Œæ—¥æœŸèŒƒå›´æ˜¯å¦æ­£ç¡®")