"""
ç‰¹å¾å·¥ç¨‹æ¨¡å— - è‚¡ç¥¨æ•°æ®ç‰¹å¾ç”Ÿæˆ
åŒ…å«æ‰‹å·¥ç‰¹å¾å’Œè‡ªåŠ¨ç‰¹å¾ç”ŸæˆåŠŸèƒ½
"""
import pandas as pd
import numpy as np
import warnings
from typing import Dict, List, Optional, Tuple
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import matplotlib.pyplot as plt
import seaborn as sns

# å¯é€‰ä¾èµ–å¯¼å…¥
try:
    import talib
    TALIB_AVAILABLE = True
except ImportError:
    TALIB_AVAILABLE = False
    print("âš ï¸ talib æœªå®‰è£…ï¼Œéƒ¨åˆ†æŠ€æœ¯æŒ‡æ ‡å°†ä½¿ç”¨pandaså®ç°")

try:
    import tsfresh
    from tsfresh import extract_features
    from tsfresh.feature_extraction import ComprehensiveFCParameters, MinimalFCParameters
    from tsfresh.utilities.dataframe_functions import impute
    TSFRESH_AVAILABLE = True
except ImportError:
    TSFRESH_AVAILABLE = False
    print("âš ï¸ tsfresh æœªå®‰è£…ï¼Œè‡ªåŠ¨ç‰¹å¾ç”Ÿæˆä¸å¯ç”¨")

warnings.filterwarnings('ignore')


class FeatureEngineer:
    """
    ç‰¹å¾å·¥ç¨‹ç±» - è‚¡ç¥¨æ•°æ®ç‰¹å¾ç”Ÿæˆ
    æ”¯æŒæ‰‹å·¥ç‰¹å¾å’Œè‡ªåŠ¨ç‰¹å¾ç”Ÿæˆ
    """
    
    def __init__(self, use_talib: bool = True, use_tsfresh: bool = True):
        """
        åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹å™¨
        
        Parameters:
        -----------
        use_talib : bool, default=True
            æ˜¯å¦ä½¿ç”¨talibåº“è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
        use_tsfresh : bool, default=True
            æ˜¯å¦å¯ç”¨tsfreshè‡ªåŠ¨ç‰¹å¾ç”Ÿæˆ
        """
        self.use_talib = use_talib and TALIB_AVAILABLE
        self.use_tsfresh = use_tsfresh and TSFRESH_AVAILABLE
        self.scaler = None
        
        print(f"ğŸ”§ ç‰¹å¾å·¥ç¨‹å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ“Š TA-Lib: {'âœ…' if self.use_talib else 'âŒ'}")
        print(f"   ğŸ¤– TSFresh: {'âœ…' if self.use_tsfresh else 'âŒ'}")
    
    def prepare_manual_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ç”Ÿæˆæ‰‹å·¥åŸºç¡€ç‰¹å¾ï¼ˆå¯è§£é‡Šç‰¹å¾ï¼‰
        
        Parameters:
        -----------
        df : pd.DataFrame
            åŒ…å«OHLCVæ•°æ®çš„DataFrame
            
        Returns:
        --------
        pd.DataFrame
            åŒ…å«æ‰‹å·¥ç‰¹å¾çš„DataFrame
        """
        print("ğŸ”¨ å¼€å§‹ç”Ÿæˆæ‰‹å·¥åŸºç¡€ç‰¹å¾...")
        data = df.copy()
        
        # 1. æ”¶ç›Šç‡ç‰¹å¾ (Returns)
        print("   ğŸ“ˆ è®¡ç®—æ”¶ç›Šç‡ç‰¹å¾...")
        data['return_1d'] = data['close'].pct_change()
        data['return_5d'] = data['close'].pct_change(5)
        data['return_10d'] = data['close'].pct_change(10)
        data['return_20d'] = data['close'].pct_change(20)
        
        # 2. æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾ (Rolling Statistics)
        print("   ğŸ“Š è®¡ç®—æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾...")
        windows = [5, 10, 20, 30]
        for window in windows:
            # æ»šåŠ¨å‡å€¼
            data[f'rolling_mean_{window}d'] = data['close'].rolling(window).mean()
            # æ»šåŠ¨æ ‡å‡†å·®
            data[f'rolling_std_{window}d'] = data['close'].rolling(window).std()
            # æ»šåŠ¨ä¸­ä½æ•°
            data[f'rolling_median_{window}d'] = data['close'].rolling(window).median()
            # ä»·æ ¼ç›¸å¯¹ä½ç½®
            data[f'price_position_{window}d'] = (data['close'] - data[f'rolling_mean_{window}d']) / data[f'rolling_std_{window}d']
        
        # 3. åŠ¨é‡ç‰¹å¾ (Momentum)
        print("   ğŸš€ è®¡ç®—åŠ¨é‡ç‰¹å¾...")
        momentum_periods = [3, 5, 10, 20]
        for period in momentum_periods:
            data[f'momentum_{period}d'] = (data['close'] / data['close'].shift(period)) - 1
        
        # 4. ATRå’Œæ³¢åŠ¨ç‡ç‰¹å¾ (Volatility)
        print("   ğŸ“Š è®¡ç®—æ³¢åŠ¨ç‡ç‰¹å¾...")
        if self.use_talib:
            data['atr_14'] = talib.ATR(data['high'].values, data['low'].values, data['close'].values, timeperiod=14)
        else:
            # æ‰‹å·¥è®¡ç®—ATR
            data['tr1'] = data['high'] - data['low']
            data['tr2'] = abs(data['high'] - data['close'].shift(1))
            data['tr3'] = abs(data['low'] - data['close'].shift(1))
            data['true_range'] = data[['tr1', 'tr2', 'tr3']].max(axis=1)
            data['atr_14'] = data['true_range'].rolling(14).mean()
            data.drop(['tr1', 'tr2', 'tr3', 'true_range'], axis=1, inplace=True)
        
        # ä»·æ ¼æ³¢åŠ¨ç‡
        data['volatility_5d'] = data['return_1d'].rolling(5).std()
        data['volatility_20d'] = data['return_1d'].rolling(20).std()
        
        # ååº¦å’Œå³°åº¦
        data['skewness_20d'] = data['return_1d'].rolling(20).skew()
        data['kurtosis_20d'] = data['return_1d'].rolling(20).kurt()
        
        # 5. æˆäº¤é‡ç‰¹å¾ (Volume Features)
        print("   ğŸ’° è®¡ç®—æˆäº¤é‡ç‰¹å¾...")
        data['volume_change'] = data['volume'].pct_change()
        data['volume_ma_5'] = data['volume'].rolling(5).mean()
        data['volume_ma_20'] = data['volume'].rolling(20).mean()
        data['volume_ratio_5d'] = data['volume'] / data['volume_ma_5']
        data['volume_ratio_20d'] = data['volume'] / data['volume_ma_20']
        
        # æˆäº¤é‡å˜åŒ–ç‡
        volume_periods = [3, 5, 10]
        for period in volume_periods:
            data[f'volume_roc_{period}d'] = (data['volume'] / data['volume'].shift(period)) - 1
        
        # 6. ä»·æ ¼èŒƒå›´ç‰¹å¾ (Price Range)
        print("   ğŸ“ è®¡ç®—ä»·æ ¼èŒƒå›´ç‰¹å¾...")
        data['high_low_ratio'] = data['high'] / data['low']
        data['high_close_ratio'] = data['high'] / data['close']
        data['low_close_ratio'] = data['low'] / data['close']
        data['open_close_ratio'] = data['close'] / data['open']
        
        # ä»·æ ¼èŒƒå›´ç›¸å¯¹åŒ–
        data['price_range'] = data['high'] - data['low']
        data['price_range_pct'] = data['price_range'] / data['close']
        data['open_close_range'] = abs(data['close'] - data['open'])
        data['open_close_range_pct'] = data['open_close_range'] / data['close']
        
        # 7. æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾ (Technical Indicators)
        print("   ğŸ” è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾...")
        
        # RSI
        if self.use_talib:
            data['rsi_14'] = talib.RSI(data['close'].values, timeperiod=14)
        else:
            data['rsi_14'] = self._calculate_rsi(data['close'], window=14)
        
        # å¸ƒæ—å¸¦
        if self.use_talib:
            bb_upper, bb_middle, bb_lower = talib.BBANDS(data['close'].values, timeperiod=20, nbdevup=2, nbdevdn=2)
            data['bb_upper'] = bb_upper
            data['bb_middle'] = bb_middle
            data['bb_lower'] = bb_lower
        else:
            bb_window = 20
            bb_std = 2
            bb_ma = data['close'].rolling(bb_window).mean()
            bb_std_val = data['close'].rolling(bb_window).std()
            data['bb_upper'] = bb_ma + (bb_std_val * bb_std)
            data['bb_middle'] = bb_ma
            data['bb_lower'] = bb_ma - (bb_std_val * bb_std)
        
        data['bb_position'] = (data['close'] - data['bb_lower']) / (data['bb_upper'] - data['bb_lower'])
        data['bb_width'] = (data['bb_upper'] - data['bb_lower']) / data['bb_middle']
        
        # MACD
        if self.use_talib:
            macd, macd_signal, macd_hist = talib.MACD(data['close'].values, fastperiod=12, slowperiod=26, signalperiod=9)
            data['macd'] = macd
            data['macd_signal'] = macd_signal
            data['macd_hist'] = macd_hist
        else:
            ema_12 = data['close'].ewm(span=12).mean()
            ema_26 = data['close'].ewm(span=26).mean()
            data['macd'] = ema_12 - ema_26
            data['macd_signal'] = data['macd'].ewm(span=9).mean()
            data['macd_hist'] = data['macd'] - data['macd_signal']
        
        # æ¸…ç†æ— ç”¨åˆ—å¹¶å»é™¤ç¼ºå¤±å€¼
        feature_columns = [col for col in data.columns 
                          if col not in ['datetime', 'open', 'high', 'low', 'close', 'volume', 'turnover', 'open_interest']]
        
        result = data[['datetime', 'close'] + feature_columns].dropna()
        
        print(f"âœ… æ‰‹å·¥ç‰¹å¾ç”Ÿæˆå®Œæˆï¼Œç‰¹å¾æ•°é‡: {len(feature_columns)}")
        return result
    
    def prepare_auto_features(self, df: pd.DataFrame, window_size: int = 30, 
                            max_features: int = 100, n_jobs: int = 1) -> pd.DataFrame:
        """
        ä½¿ç”¨tsfreshè‡ªåŠ¨ç”Ÿæˆç‰¹å¾
        
        Parameters:
        -----------
        df : pd.DataFrame
            åŒ…å«OHLCVæ•°æ®çš„DataFrame
        window_size : int, default=30
            ç‰¹å¾æå–çš„çª—å£å¤§å°
        max_features : int, default=100
            æœ€å¤§ç‰¹å¾æ•°é‡ï¼ˆç”¨äºæ§åˆ¶ç»´åº¦çˆ†ç‚¸ï¼‰
        n_jobs : int, default=1
            å¹¶è¡Œå¤„ç†çš„ä½œä¸šæ•°
            
        Returns:
        --------
        pd.DataFrame
            åŒ…å«è‡ªåŠ¨ç”Ÿæˆç‰¹å¾çš„DataFrame
        """
        if not self.use_tsfresh:
            print("âŒ tsfreshä¸å¯ç”¨ï¼Œè·³è¿‡è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆ")
            return df[['datetime', 'close']].copy()
        
        print("ğŸ¤– å¼€å§‹è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆ...")
        print(f"   ğŸ“Š çª—å£å¤§å°: {window_size}")
        print(f"   ğŸ”¢ æœ€å¤§ç‰¹å¾æ•°: {max_features}")
        
        data = df.copy()
        
        # å‡†å¤‡tsfreshæ ¼å¼çš„æ•°æ®
        tsfresh_data = []
        
        # ä¸ºæ¯ä¸ªçª—å£åˆ›å»ºæ—¶åºæ•°æ®
        for i in range(window_size, len(data)):
            window_data = data.iloc[i-window_size:i].copy()
            window_id = i - window_size
            
            # ä¸ºtsfreshæ ¼å¼æ·»åŠ idå’Œsortåˆ—
            window_data['id'] = window_id
            window_data['time'] = range(len(window_data))
            
            tsfresh_data.append(window_data)
        
        if not tsfresh_data:
            print("âŒ æ•°æ®é‡ä¸è¶³ä»¥ç”Ÿæˆè‡ªåŠ¨ç‰¹å¾")
            return df[['datetime', 'close']].copy()
        
        # åˆå¹¶æ‰€æœ‰çª—å£æ•°æ®
        combined_data = pd.concat(tsfresh_data, ignore_index=True)
        
        # é€‰æ‹©è¦æå–ç‰¹å¾çš„åˆ—
        value_columns = ['close', 'volume', 'high', 'low', 'open']
        value_columns = [col for col in value_columns if col in combined_data.columns]
        
        try:
            print("   ğŸ”„ æå–ç‰¹å¾ä¸­...")
            
            # ä½¿ç”¨æœ€å°ç‰¹å¾é›†ä»¥æ§åˆ¶ç»´åº¦
            if max_features <= 50:
                fc_parameters = MinimalFCParameters()
            else:
                fc_parameters = ComprehensiveFCParameters()
            
            # æå–ç‰¹å¾
            extracted_features = extract_features(
                combined_data[['id', 'time'] + value_columns],
                column_id='id',
                column_sort='time',
                default_fc_parameters=fc_parameters,
                n_jobs=n_jobs
            )
            
            # å¤„ç†ç¼ºå¤±å€¼
            impute(extracted_features)
            
            # ç‰¹å¾é€‰æ‹©ï¼ˆæ§åˆ¶ç»´åº¦ï¼‰
            if len(extracted_features.columns) > max_features:
                print(f"   âœ‚ï¸ ç‰¹å¾é™ç»´: {len(extracted_features.columns)} -> {max_features}")
                
                # ç®€å•çš„æ–¹å·®ç­›é€‰
                feature_vars = extracted_features.var()
                selected_features = feature_vars.nlargest(max_features).index
                extracted_features = extracted_features[selected_features]
            
            # æ·»åŠ æ—¶é—´æˆ³å’Œä»·æ ¼ä¿¡æ¯
            result_indices = list(range(window_size, len(data)))
            result_data = data.iloc[result_indices][['datetime', 'close']].reset_index(drop=True)
            
            # åˆå¹¶ç‰¹å¾
            extracted_features.reset_index(drop=True, inplace=True)
            result = pd.concat([result_data, extracted_features], axis=1)
            
            print(f"âœ… è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆå®Œæˆï¼Œç‰¹å¾æ•°é‡: {len(extracted_features.columns)}")
            return result
            
        except Exception as e:
            print(f"âŒ è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆå¤±è´¥: {str(e)}")
            return df[['datetime', 'close']].copy()
    
    def prepare_combined_features(self, df: pd.DataFrame, window_size: int = 30,
                                auto_features: bool = True, max_auto_features: int = 50) -> pd.DataFrame:
        """
        ç”Ÿæˆç»„åˆç‰¹å¾ï¼ˆæ‰‹å·¥ + è‡ªåŠ¨ï¼‰
        
        Parameters:
        -----------
        df : pd.DataFrame
            åŸå§‹OHLCVæ•°æ®
        window_size : int, default=30
            è‡ªåŠ¨ç‰¹å¾æå–çš„çª—å£å¤§å°
        auto_features : bool, default=True
            æ˜¯å¦åŒ…å«è‡ªåŠ¨ç‰¹å¾
        max_auto_features : int, default=50
            æœ€å¤§è‡ªåŠ¨ç‰¹å¾æ•°é‡
            
        Returns:
        --------
        pd.DataFrame
            åŒ…å«æ‰€æœ‰ç‰¹å¾çš„DataFrame
        """
        print("ğŸ”§ å¼€å§‹ç”Ÿæˆç»„åˆç‰¹å¾...")
        
        # ç”Ÿæˆæ‰‹å·¥ç‰¹å¾
        manual_features = self.prepare_manual_features(df)
        
        if not auto_features or not self.use_tsfresh:
            print("âœ… ä»…ä½¿ç”¨æ‰‹å·¥ç‰¹å¾")
            return manual_features
        
        # ç”Ÿæˆè‡ªåŠ¨ç‰¹å¾
        auto_features_df = self.prepare_auto_features(df, window_size, max_auto_features)
        
        # åˆå¹¶ç‰¹å¾ï¼ˆä»¥æ‰‹å·¥ç‰¹å¾ä¸ºä¸»ï¼‰
        if len(auto_features_df) > 0 and len(auto_features_df.columns) > 2:
            # æ‰¾åˆ°é‡å çš„æ—¶é—´èŒƒå›´
            manual_times = set(manual_features['datetime'])
            auto_times = set(auto_features_df['datetime'])
            common_times = manual_times.intersection(auto_times)
            
            if common_times:
                # ç­›é€‰å…±åŒæ—¶é—´æ®µçš„æ•°æ®
                manual_filtered = manual_features[manual_features['datetime'].isin(common_times)].copy()
                auto_filtered = auto_features_df[auto_features_df['datetime'].isin(common_times)].copy()
                
                # æŒ‰æ—¶é—´æ’åº
                manual_filtered = manual_filtered.sort_values('datetime').reset_index(drop=True)
                auto_filtered = auto_filtered.sort_values('datetime').reset_index(drop=True)
                
                # åˆå¹¶ç‰¹å¾ï¼ˆå»é™¤é‡å¤çš„datetimeå’Œcloseåˆ—ï¼‰
                auto_features_only = auto_filtered.drop(['datetime', 'close'], axis=1, errors='ignore')
                combined = pd.concat([manual_filtered, auto_features_only], axis=1)
                
                print(f"âœ… ç»„åˆç‰¹å¾ç”Ÿæˆå®Œæˆ")
                print(f"   ğŸ“Š æ‰‹å·¥ç‰¹å¾: {len(manual_filtered.columns) - 2}")
                print(f"   ğŸ¤– è‡ªåŠ¨ç‰¹å¾: {len(auto_features_only.columns)}")
                print(f"   ğŸ¯ æ€»ç‰¹å¾æ•°: {len(combined.columns) - 2}")
                
                return combined
        
        print("âš ï¸ è‡ªåŠ¨ç‰¹å¾åˆå¹¶å¤±è´¥ï¼Œä»…è¿”å›æ‰‹å·¥ç‰¹å¾")
        return manual_features
    
    def analyze_features(self, features_df: pd.DataFrame, plot: bool = True) -> Dict:
        """
        åˆ†æç‰¹å¾åˆ†å¸ƒå’Œè´¨é‡
        
        Parameters:
        -----------
        features_df : pd.DataFrame
            ç‰¹å¾æ•°æ®
        plot : bool, default=True
            æ˜¯å¦ç»˜åˆ¶åˆ†æå›¾è¡¨
            
        Returns:
        --------
        Dict
            ç‰¹å¾åˆ†æç»“æœ
        """
        print("ğŸ“Š å¼€å§‹ç‰¹å¾åˆ†æ...")
        
        feature_cols = [col for col in features_df.columns if col not in ['datetime', 'close']]
        
        analysis = {
            'total_features': len(feature_cols),
            'missing_values': {},
            'extreme_values': {},
            'distributions': {}
        }
        
        # ç¼ºå¤±å€¼åˆ†æ
        for col in feature_cols:
            missing_count = features_df[col].isnull().sum()
            missing_pct = missing_count / len(features_df) * 100
            if missing_count > 0:
                analysis['missing_values'][col] = {
                    'count': missing_count,
                    'percentage': missing_pct
                }
        
        # æå€¼åˆ†æ
        for col in feature_cols:
            if features_df[col].dtype in ['float64', 'int64']:
                q1 = features_df[col].quantile(0.25)
                q3 = features_df[col].quantile(0.75)
                iqr = q3 - q1
                lower_bound = q1 - 1.5 * iqr
                upper_bound = q3 + 1.5 * iqr
                
                outliers = ((features_df[col] < lower_bound) | (features_df[col] > upper_bound)).sum()
                if outliers > 0:
                    analysis['extreme_values'][col] = {
                        'count': outliers,
                        'percentage': outliers / len(features_df) * 100,
                        'bounds': (lower_bound, upper_bound)
                    }
        
        # åˆ†å¸ƒç»Ÿè®¡
        numeric_features = features_df[feature_cols].select_dtypes(include=[np.number])
        analysis['distributions'] = {
            'mean': numeric_features.mean().to_dict(),
            'std': numeric_features.std().to_dict(),
            'min': numeric_features.min().to_dict(),
            'max': numeric_features.max().to_dict()
        }
        
        # æ‰“å°åˆ†æç»“æœ
        print(f"ğŸ“ˆ ç‰¹å¾åˆ†æç»“æœ:")
        print(f"   ğŸ”¢ æ€»ç‰¹å¾æ•°: {analysis['total_features']}")
        print(f"   âŒ ç¼ºå¤±å€¼ç‰¹å¾: {len(analysis['missing_values'])}")
        print(f"   âš ï¸ å¼‚å¸¸å€¼ç‰¹å¾: {len(analysis['extreme_values'])}")
        
        if analysis['missing_values']:
            print("   ğŸ“‹ ç¼ºå¤±å€¼è¯¦æƒ…:")
            for col, info in list(analysis['missing_values'].items())[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"      {col}: {info['count']} ({info['percentage']:.1f}%)")
        
        if analysis['extreme_values']:
            print("   ğŸ“‹ å¼‚å¸¸å€¼è¯¦æƒ…:")
            for col, info in list(analysis['extreme_values'].items())[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"      {col}: {info['count']} ({info['percentage']:.1f}%)")
        
        # ç»˜å›¾åˆ†æ
        if plot and len(numeric_features.columns) > 0:
            self._plot_feature_analysis(numeric_features)
        
        return analysis
    
    def _calculate_rsi(self, prices: pd.Series, window: int = 14) -> pd.Series:
        """æ‰‹å·¥è®¡ç®—RSI"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def _plot_feature_analysis(self, features_df: pd.DataFrame, max_plots: int = 12):
        """ç»˜åˆ¶ç‰¹å¾åˆ†æå›¾è¡¨"""
        try:
            plt.rcParams['font.sans-serif'] = ['SimHei']
            plt.rcParams['axes.unicode_minus'] = False
            
            n_features = min(len(features_df.columns), max_plots)
            fig, axes = plt.subplots(3, 4, figsize=(16, 12))
            axes = axes.ravel()
            
            for i, col in enumerate(features_df.columns[:n_features]):
                # åˆ†å¸ƒç›´æ–¹å›¾
                axes[i].hist(features_df[col].dropna(), bins=50, alpha=0.7, edgecolor='black')
                axes[i].set_title(f'{col}', fontsize=10)
                axes[i].tick_params(labelsize=8)
            
            # éšè—å¤šä½™çš„å­å›¾
            for i in range(n_features, len(axes)):
                axes[i].set_visible(False)
            
            plt.tight_layout()
            plt.suptitle('ç‰¹å¾åˆ†å¸ƒåˆ†æ', fontsize=14, y=0.98)
            plt.show()
            
        except Exception as e:
            print(f"âš ï¸ ç»˜å›¾å¤±è´¥: {str(e)}")


# æµ‹è¯•å‡½æ•°
def test_feature_engineering():
    """æµ‹è¯•ç‰¹å¾å·¥ç¨‹åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•ç‰¹å¾å·¥ç¨‹åŠŸèƒ½")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    dates = pd.date_range('2022-01-01', periods=200, freq='D')
    np.random.seed(42)
    
    test_data = pd.DataFrame({
        'datetime': dates,
        'open': 100 + np.cumsum(np.random.randn(200) * 0.5),
        'high': 0,
        'low': 0,
        'close': 0,
        'volume': np.random.randint(1000, 10000, 200)
    })
    
    # ç”ŸæˆOHLCæ•°æ®
    test_data['close'] = test_data['open'] + np.random.randn(200) * 0.3
    test_data['high'] = np.maximum(test_data['open'], test_data['close']) + np.random.rand(200) * 0.5
    test_data['low'] = np.minimum(test_data['open'], test_data['close']) - np.random.rand(200) * 0.5
    
    # æµ‹è¯•ç‰¹å¾å·¥ç¨‹
    engineer = FeatureEngineer()
    
    # æµ‹è¯•æ‰‹å·¥ç‰¹å¾
    manual_features = engineer.prepare_manual_features(test_data)
    print(f"ğŸ“Š æ‰‹å·¥ç‰¹å¾æµ‹è¯•å®Œæˆï¼Œç‰¹å¾æ•°é‡: {len(manual_features.columns) - 2}")
    
    # åˆ†æç‰¹å¾
    analysis = engineer.analyze_features(manual_features, plot=False)
    
    return manual_features, analysis


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    features, analysis = test_feature_engineering()
    print("âœ… ç‰¹å¾å·¥ç¨‹æµ‹è¯•å®Œæˆ")