#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨ç‰¹å¾å·¥ç¨‹æ¨¡å— - ä½¿ç”¨çœŸå®InfluxDBå†å²æ•°æ®

åŠŸèƒ½:
1. ä»InfluxDBåŠ è½½çœŸå®è‚¡ç¥¨å†å²æ•°æ®
2. ç”ŸæˆæŠ€æœ¯åˆ†æç‰¹å¾ 
3. ç‰¹å¾é€‰æ‹©å’Œä¼˜åŒ–
4. ç‰¹å¾è´¨é‡åˆ†æ

ä½œè€…: Your Name
æ—¥æœŸ: 2024
"""

import pandas as pd
import numpy as np
import sys
import os
from typing import Dict, List, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')

# InfluxDBç›¸å…³
sys.path.append(r'd:\vscode projects\stock\stock_info')
from utils import get_influxdb_client
from stock_market_data_akshare import get_history_data

# æœºå™¨å­¦ä¹ ç›¸å…³ 
from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_regression
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# å°è¯•å¯¼å…¥å¯é€‰åº“
try:
    import talib
    HAS_TALIB = True
except ImportError:
    HAS_TALIB = False
    print("âš ï¸ talib æœªå®‰è£…ï¼Œéƒ¨åˆ†æŠ€æœ¯æŒ‡æ ‡å°†ä½¿ç”¨pandaså®ç°")

try:
    from tsfresh import extract_features
    HAS_TSFRESH = True
except ImportError:
    HAS_TSFRESH = False
    print("âš ï¸ tsfresh æœªå®‰è£…ï¼Œè‡ªåŠ¨ç‰¹å¾ç”Ÿæˆä¸å¯ç”¨")

try:
    from xgboost import XGBRegressor
    HAS_XGBOOST = True
except ImportError:
    HAS_XGBOOST = False
    print("âš ï¸ xgboost æœªå®‰è£…ï¼Œå°†ä½¿ç”¨RandomForestè¿›è¡Œç‰¹å¾é‡è¦æ€§è¯„ä¼°")

try:
    import matplotlib.pyplot as plt
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False


class FeatureEngineer:
    """
    è‚¡ç¥¨ç‰¹å¾å·¥ç¨‹ç±» - ä½¿ç”¨çœŸå®InfluxDBæ•°æ®
    
    åŠŸèƒ½:
    1. ä»InfluxDBåŠ è½½çœŸå®å†å²è‚¡ç¥¨æ•°æ®
    2. ç”ŸæˆæŠ€æœ¯åˆ†æç‰¹å¾
    3. ç‰¹å¾é€‰æ‹©å’Œä¼˜åŒ– 
    4. ç‰¹å¾è´¨é‡åˆ†æ
    """
    
    def __init__(self, use_talib: bool = True, use_tsfresh: bool = True):
        """
        åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹å™¨
        
        Parameters:
        -----------
        use_talib : bool, default=True
            æ˜¯å¦ä½¿ç”¨TA-Libè¿›è¡ŒæŠ€æœ¯æŒ‡æ ‡è®¡ç®—
        use_tsfresh : bool, default=False  
            æ˜¯å¦ä½¿ç”¨TSFreshè¿›è¡Œè‡ªåŠ¨ç‰¹å¾ç”Ÿæˆ
        """
        self.use_talib = use_talib and HAS_TALIB
        self.use_tsfresh = use_tsfresh and HAS_TSFRESH
        self.use_xgboost = HAS_XGBOOST
        
        # åˆå§‹åŒ–InfluxDBå®¢æˆ·ç«¯
        self.influx_client = get_influxdb_client()
        if not self.influx_client:
            raise ConnectionError("æ— æ³•è¿æ¥åˆ°InfluxDBï¼Œè¯·æ£€æŸ¥æœåŠ¡çŠ¶æ€å’Œé…ç½®")
        self.query_api = self.influx_client.query_api()
        
        print("ğŸ”§ ç‰¹å¾å·¥ç¨‹å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ“Š TA-Lib: {'âœ…' if self.use_talib else 'âŒ'}")
        print(f"   ğŸ¤– TSFresh: {'âœ…' if self.use_tsfresh else 'âŒ'}")
        print(f"   ğŸ’¾ InfluxDB: âœ…")
    
    def load_stock_data(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """
        ä»InfluxDBåŠ è½½çœŸå®è‚¡ç¥¨å†å²æ•°æ®
        
        Parameters:
        -----------
        symbol : str
            è‚¡ç¥¨ä»£ç ï¼Œå¦‚ '000001' 
        start_date : str
            å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ 'YYYY-MM-DD'
        end_date : str  
            ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ 'YYYY-MM-DD'
        
        Returns:
        --------
        pd.DataFrame
            åŒ…å«OHLCVæ•°æ®çš„DataFrame
        """
        print(f"ğŸ“Š ä»InfluxDBåŠ è½½è‚¡ç¥¨æ•°æ®: {symbol} ({start_date} åˆ° {end_date})")
        
        try:
            # è½¬æ¢æ—¥æœŸæ ¼å¼ä¸ºInfluxDB FluxæŸ¥è¯¢æ ¼å¼
            start_flux = pd.to_datetime(start_date).strftime('%Y-%m-%dT00:00:00Z')
            end_flux = pd.to_datetime(end_date).strftime('%Y-%m-%dT23:59:59Z')
            
            # ä»InfluxDBè·å–æ•°æ®
            df = get_history_data(self.query_api, symbol, start_flux, end_flux)
            
            if df.empty:
                raise ValueError(f"æœªæ‰¾åˆ°è‚¡ç¥¨ {symbol} åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ•°æ®")
            
            # æ ‡å‡†åŒ–åˆ—å 
            column_mapping = {
                'æ—¥æœŸ': 'timestamp',
                'å¼€ç›˜': 'open', 
                'æ”¶ç›˜': 'close',
                'æœ€é«˜': 'high',
                'æœ€ä½': 'low', 
                'æˆäº¤é‡': 'volume',
                'æˆäº¤é¢': 'turnover'
            }
            
            # é‡å‘½åå­˜åœ¨çš„åˆ—
            existing_cols = {k: v for k, v in column_mapping.items() if k in df.columns}
            df = df.rename(columns=existing_cols)
            
            # ç¡®ä¿æ—¶é—´åˆ—æ­£ç¡®å¤„ç†
            if 'timestamp' in df.columns:
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                df = df.set_index('timestamp')
            elif 'æ—¥æœŸ' in df.columns:
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
                df = df.set_index('æ—¥æœŸ')
            
            # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            for col in ['open', 'high', 'low', 'close', 'volume']:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # åˆ é™¤ç¼ºå¤±å€¼
            df = df.dropna()
            
            # æŒ‰æ—¶é—´æ’åº
            df = df.sort_index()
            
            print(f"âœ… æˆåŠŸåŠ è½½ {len(df)} æ¡æ•°æ®è®°å½•")
            print(f"ğŸ“… æ•°æ®æ—¶é—´èŒƒå›´: {df.index.min().date()} åˆ° {df.index.max().date()}")
            
            return df
            
        except Exception as e:
            print(f"âŒ åŠ è½½è‚¡ç¥¨æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            raise

    def prepare_features(self, data: pd.DataFrame, use_auto_features: bool = True, 
                        window_size: int = 20, max_auto_features: int = 50) -> pd.DataFrame:
        """
        ç»Ÿä¸€ç‰¹å¾ç”Ÿæˆæ–¹æ³• - æ”¯æŒæ‰‹å·¥ç‰¹å¾å’Œå¯é€‰çš„è‡ªåŠ¨ç‰¹å¾
        
        Parameters:
        -----------
        data : pd.DataFrame
            åŸå§‹è‚¡ç¥¨æ•°æ®ï¼ŒåŒ…å«OHLCVåˆ—
        use_auto_features : bool, default=False
            æ˜¯å¦ä½¿ç”¨TSFreshè‡ªåŠ¨ç”Ÿæˆç‰¹å¾
        window_size : int, default=20
            è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆçš„æ»‘åŠ¨çª—å£å¤§å°
        max_auto_features : int, default=50
            è‡ªåŠ¨ç‰¹å¾çš„æœ€å¤§æ•°é‡
        
        Returns:
        --------
        pd.DataFrame
            åŒ…å«æ‰€æœ‰ç‰¹å¾çš„æ•°æ®æ¡†
        """
        print("ğŸ”¨ å¼€å§‹ç‰¹å¾ç”Ÿæˆ...")
        
        # === 1. æ‰‹å·¥ç‰¹å¾ç”Ÿæˆ ===
        print("ğŸ“Š ç”Ÿæˆæ‰‹å·¥ç‰¹å¾...")
        data = data.copy()
        
        # æ”¶ç›Šç‡ç‰¹å¾
        print("   ğŸ“ˆ è®¡ç®—æ”¶ç›Šç‡ç‰¹å¾...")
        data['return_1d'] = data['close'].pct_change()
        data['return_5d'] = data['close'].pct_change(5)
        data['return_10d'] = data['close'].pct_change(10)
        data['return_20d'] = data['close'].pct_change(20)
        
        # æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾
        print("   ğŸ“Š è®¡ç®—æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾...")
        for window in [5, 10, 20, 30]:
            data[f'rolling_mean_{window}d'] = data['close'].rolling(window).mean()
            data[f'rolling_std_{window}d'] = data['close'].rolling(window).std()
            data[f'rolling_median_{window}d'] = data['close'].rolling(window).median()
            data[f'price_position_{window}d'] = (data['close'] - data['close'].rolling(window).min()) / (
                data['close'].rolling(window).max() - data['close'].rolling(window).min() + 1e-8) * 2 - 1
        
        # åŠ¨é‡ç‰¹å¾
        print("   ğŸš€ è®¡ç®—åŠ¨é‡ç‰¹å¾...")
        data['momentum_3d'] = data['close'] / data['close'].shift(3) - 1
        data['momentum_5d'] = data['close'] / data['close'].shift(5) - 1
        data['momentum_10d'] = data['close'] / data['close'].shift(10) - 1
        data['momentum_20d'] = data['close'] / data['close'].shift(20) - 1
        
        # æ³¢åŠ¨ç‡ç‰¹å¾  
        print("   ğŸ“Š è®¡ç®—æ³¢åŠ¨ç‡ç‰¹å¾...")
        if self.use_talib:
            data['atr_14'] = talib.ATR(data['high'], data['low'], data['close'], timeperiod=14)
        else:
            data['high_low'] = data['high'] - data['low']
            data['high_close'] = np.abs(data['high'] - data['close'].shift())
            data['low_close'] = np.abs(data['low'] - data['close'].shift())
            data['atr_14'] = pd.concat([data['high_low'], data['high_close'], data['low_close']], axis=1).max(axis=1).rolling(14).mean()
            data.drop(['high_low', 'high_close', 'low_close'], axis=1, inplace=True)
        
        data['volatility_5d'] = data['return_1d'].rolling(5).std()
        data['volatility_20d'] = data['return_1d'].rolling(20).std()
        data['skewness_20d'] = data['return_1d'].rolling(20).skew()
        data['kurtosis_20d'] = data['return_1d'].rolling(20).kurt()
        
        # æˆäº¤é‡ç‰¹å¾
        print("   ğŸ’° è®¡ç®—æˆäº¤é‡ç‰¹å¾...")
        data['volume_change'] = data['volume'].pct_change()
        data['volume_ma_5'] = data['volume'].rolling(5).mean()
        data['volume_ma_20'] = data['volume'].rolling(20).mean()
        data['volume_ratio_5d'] = data['volume'] / data['volume_ma_5']
        data['volume_ratio_20d'] = data['volume'] / data['volume_ma_20']
        data['volume_roc_3d'] = data['volume'].pct_change(3)
        data['volume_roc_5d'] = data['volume'].pct_change(5)
        data['volume_roc_10d'] = data['volume'].pct_change(10)
        
        # ä»·æ ¼èŒƒå›´ç‰¹å¾
        print("   ğŸ“ è®¡ç®—ä»·æ ¼èŒƒå›´ç‰¹å¾...")
        data['high_low_ratio'] = data['high'] / data['low']
        data['high_close_ratio'] = data['high'] / data['close']
        data['low_close_ratio'] = data['low'] / data['close']
        data['open_close_ratio'] = data['open'] / data['close']
        data['price_range'] = data['high'] - data['low']
        data['price_range_pct'] = (data['high'] - data['low']) / data['close']
        data['open_close_range'] = np.abs(data['open'] - data['close'])
        data['open_close_range_pct'] = np.abs(data['open'] - data['close']) / data['close']
        
        # æŠ€æœ¯æŒ‡æ ‡
        print("   ğŸ” è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾...")
        if self.use_talib:
            data['rsi_14'] = talib.RSI(data['close'], timeperiod=14)
            data['bb_upper'], data['bb_middle'], data['bb_lower'] = talib.BBANDS(data['close'])
            data['macd'], data['macd_signal'], data['macd_hist'] = talib.MACD(data['close'])
        else:
            # RSIçš„pandaså®ç°
            delta = data['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
            rs = gain / loss
            data['rsi_14'] = 100 - (100 / (1 + rs))
            
            # å¸ƒæ—å¸¦çš„pandaså®ç°
            data['bb_middle'] = data['close'].rolling(20).mean()
            bb_std = data['close'].rolling(20).std()
            data['bb_upper'] = data['bb_middle'] + (bb_std * 2)
            data['bb_lower'] = data['bb_middle'] - (bb_std * 2)
            
            # MACDçš„pandaså®ç°
            ema_12 = data['close'].ewm(span=12).mean()
            ema_26 = data['close'].ewm(span=26).mean()
            data['macd'] = ema_12 - ema_26
            data['macd_signal'] = data['macd'].ewm(span=9).mean()
            data['macd_hist'] = data['macd'] - data['macd_signal']
        
        # å¸ƒæ—å¸¦ä½ç½®å’Œå®½åº¦
        data['bb_position'] = (data['close'] - data['bb_lower']) / (data['bb_upper'] - data['bb_lower'] + 1e-8)
        data['bb_width'] = (data['bb_upper'] - data['bb_lower']) / data['bb_middle']
        
        # æ¸…ç†æ— ç”¨åˆ—
        feature_columns = [col for col in data.columns 
                          if col not in ['datetime', 'open', 'high', 'low', 'close', 'volume', 'turnover', 'open_interest']]
        
        # åˆ›å»ºæ‰‹å·¥ç‰¹å¾ç»“æœï¼Œä¿æŒæ—¶é—´ç´¢å¼•ä¸€è‡´æ€§
        manual_result = data[['close'] + feature_columns].dropna()
        # ä¸ºäº†åç»­åŒ¹é…ï¼Œæ·»åŠ ä¸€ä¸ªæ•°å€¼ç´¢å¼•åˆ—ä½œä¸ºæ—¶é—´æˆ³
        manual_result['time_idx'] = range(len(manual_result))
        print(f"   âœ… æ‰‹å·¥ç‰¹å¾ç”Ÿæˆå®Œæˆï¼Œç‰¹å¾æ•°é‡: {len(feature_columns)}")
        
        # === 2. è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆï¼ˆå¯é€‰ï¼‰===
        if use_auto_features and self.use_tsfresh:
            print("\nğŸ¤– ç”Ÿæˆè‡ªåŠ¨ç‰¹å¾...")
            print(f"   ğŸ“Š çª—å£å¤§å°: {window_size}")
            print(f"   ğŸ”¢ æœ€å¤§ç‰¹å¾æ•°: {max_auto_features}")
            
            try:
                # å‡†å¤‡tsfreshæ ¼å¼çš„æ•°æ® - ä½¿ç”¨æ¸…ç†åçš„æ•°æ®
                clean_data = data.dropna()  # ä½¿ç”¨ä¸manual_resultç›¸åŒçš„æ¸…ç†é€»è¾‘
                tsfresh_data = []
                
                for i in range(window_size, len(clean_data)):
                    window_data = clean_data.iloc[i-window_size:i]
                    for col in ['close', 'volume']:
                        if col in window_data.columns:
                            for j, value in enumerate(window_data[col]):
                                tsfresh_data.append({
                                    'id': i,
                                    'time': j,
                                    'value': value,
                                    'variable': col
                                })
                
                if tsfresh_data:
                    tsfresh_df = pd.DataFrame(tsfresh_data)
                    
                    # æå–ç‰¹å¾
                    from tsfresh.feature_extraction import MinimalFCParameters
                    extracted_features = extract_features(
                        tsfresh_df,
                        column_id='id',
                        column_sort='time',
                        column_value='value',
                        default_fc_parameters=MinimalFCParameters()
                    )
                    
                    # é€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾
                    if len(extracted_features.columns) > max_auto_features:
                        feature_vars = extracted_features.var()
                        selected_features = feature_vars.nlargest(max_auto_features).index
                        extracted_features = extracted_features[selected_features]
                    
                    # åˆ›å»ºè‡ªåŠ¨ç‰¹å¾ç»“æœæ•°æ®æ¡†
                    result_indices = range(window_size, len(clean_data))
                    auto_result = pd.DataFrame({
                        'close': clean_data.iloc[result_indices]['close'].values,
                        'time_idx': result_indices  # ä½¿ç”¨ç›¸åŒçš„æ—¶é—´ç´¢å¼•
                    })
                    
                    # æ·»åŠ è‡ªåŠ¨ç‰¹å¾
                    for col in extracted_features.columns:
                        auto_result[f'auto_{col}'] = extracted_features[col].values
                    
                    auto_result = auto_result.dropna()
                    print(f"   âœ… è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆå®Œæˆï¼Œç‰¹å¾æ•°é‡: {len(extracted_features.columns)}")
                    
                    # åˆå¹¶æ‰‹å·¥ç‰¹å¾å’Œè‡ªåŠ¨ç‰¹å¾ - ä½¿ç”¨time_idxè¿›è¡ŒåŒ¹é…
                    manual_times = set(manual_result['time_idx'])
                    auto_times = set(auto_result['time_idx'])
                    common_times = manual_times.intersection(auto_times)
                    
                    if common_times and len(common_times) > 0:
                        # æŒ‰time_idxåŒ¹é…æ•°æ®
                        manual_filtered = manual_result[manual_result['time_idx'].isin(common_times)].copy()
                        auto_filtered = auto_result[auto_result['time_idx'].isin(common_times)].copy()
                        
                        # æŒ‰time_idxæ’åºç¡®ä¿å¯¹åº”å…³ç³»æ­£ç¡®
                        manual_filtered = manual_filtered.sort_values('time_idx').reset_index(drop=True)
                        auto_filtered = auto_filtered.sort_values('time_idx').reset_index(drop=True)
                        
                        # åˆå¹¶ç‰¹å¾ï¼ˆä¿ç•™æ‰‹å·¥ç‰¹å¾çš„æ‰€æœ‰åˆ—ï¼Œæ·»åŠ è‡ªåŠ¨ç‰¹å¾ï¼‰
                        auto_features_only = auto_filtered.drop(['close', 'time_idx'], axis=1, errors='ignore')
                        combined_result = pd.concat([manual_filtered, auto_features_only], axis=1)
                        
                        # æ·»åŠ çœŸå®çš„æ—¶é—´ç´¢å¼•
                        if not combined_result.empty:
                            time_indices = combined_result['time_idx'].values
                            combined_result.index = clean_data.index[time_indices]
                            combined_result = combined_result.drop('time_idx', axis=1)  # åˆ é™¤ä¸´æ—¶ç´¢å¼•åˆ—
                        
                        feature_count = len(combined_result.columns) - 1  # æ’é™¤closeåˆ—
                        print(f"âœ… ç‰¹å¾åˆå¹¶å®Œæˆï¼Œå…±äº«æ ·æœ¬: {len(common_times)}, æ€»ç‰¹å¾æ•°é‡: {feature_count}")
                        
                        return combined_result
                    else:
                        print(f"âš ï¸ æ‰‹å·¥ç‰¹å¾å’Œè‡ªåŠ¨ç‰¹å¾æ—¶é—´ä¸åŒ¹é…")
                        print(f"   æ‰‹å·¥ç‰¹å¾æ—¶é—´èŒƒå›´: {len(manual_times)} ä¸ªæ ·æœ¬")
                        print(f"   è‡ªåŠ¨ç‰¹å¾æ—¶é—´èŒƒå›´: {len(auto_times)} ä¸ªæ ·æœ¬")
                        print(f"   é‡å æ ·æœ¬: {len(common_times)} ä¸ª")
                        
            except Exception as e:
                print(f"âš ï¸ è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆå¤±è´¥: {e}")
                print("   ç»§ç»­ä½¿ç”¨æ‰‹å·¥ç‰¹å¾...")
        
        # æ¸…ç†ä¸´æ—¶ç´¢å¼•åˆ—å¹¶è¿”å›æ‰‹å·¥ç‰¹å¾
        if 'time_idx' in manual_result.columns:
            manual_result = manual_result.drop('time_idx', axis=1)
        
        feature_count = len(manual_result.columns) - 1  # æ’é™¤closeåˆ—
        print(f"âœ… æ‰‹å·¥ç‰¹å¾ç”Ÿæˆå®Œæˆï¼Œç‰¹å¾æ•°é‡: {feature_count}")
        
        return manual_result

    def select_features(self, features_df: pd.DataFrame, final_k: int = 20,
                       variance_threshold: float = 0.01, correlation_threshold: float = 0.95,
                       importance_method: str = 'random_forest') -> Dict:
        """
        ç»Ÿä¸€çš„ç‰¹å¾é€‰æ‹©ç®¡é“ - æ•´åˆäº†æ–¹å·®è¿‡æ»¤ã€ç›¸å…³æ€§å»é™¤å’Œé‡è¦æ€§é€‰æ‹©
        
        Parameters:
        -----------
        features_df : pd.DataFrame
            ç‰¹å¾æ•°æ®æ¡†ï¼Œåº”åŒ…å«ç›®æ ‡åˆ—'close'
        final_k : int, default=20
            æœ€ç»ˆä¿ç•™çš„ç‰¹å¾æ•°é‡
        variance_threshold : float, default=0.01
            æ–¹å·®é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„ç‰¹å¾å°†è¢«åˆ é™¤
        correlation_threshold : float, default=0.95
            ç›¸å…³æ€§é˜ˆå€¼ï¼Œé«˜äºæ­¤å€¼çš„ç‰¹å¾å¯¹ä¸­ä¿ç•™ä¸€ä¸ª
        importance_method : str, default='random_forest'
            é‡è¦æ€§è¯„ä¼°æ–¹æ³• ('random_forest' æˆ– 'xgboost')
            
        Returns:
        --------
        dict
            åŒ…å«å„æ­¥éª¤ç»“æœçš„ç»¼åˆä¿¡æ¯
        """
        print("ğŸš€ å¼€å§‹ç»¼åˆç‰¹å¾é€‰æ‹©ç®¡é“...")
        # åŠ¨æ€è®¡ç®—åŸå§‹ç‰¹å¾æ•°
        datetime_col = 'datetime' if 'datetime' in features_df.columns else None
        exclude_cols = ['close']
        if datetime_col:
            exclude_cols.append(datetime_col)
        original_feature_count = len(features_df.columns) - len(exclude_cols)
        
        print(f"   ğŸ¯ ç›®æ ‡: ä» {original_feature_count} ä¸ªç‰¹å¾ä¸­é€‰æ‹© {final_k} ä¸ª")
        print("=" * 60)
        
        results = {
            'original_features': original_feature_count,
            'final_k': final_k,
            'pipeline_steps': []
        }
        
        current_df = features_df.copy()
        
        # æ­¥éª¤1: æ–¹å·®é˜ˆå€¼è¿‡æ»¤
        print("ğŸ”¸ æ­¥éª¤1: æ–¹å·®é˜ˆå€¼è¿‡æ»¤")
        # åŠ¨æ€æ£€æŸ¥datetimeåˆ—
        datetime_col = 'datetime' if 'datetime' in current_df.columns else None
        exclude_cols = ['close']
        if datetime_col:
            exclude_cols.append(datetime_col)
        feature_cols = [col for col in current_df.columns if col not in exclude_cols]
        
        if feature_cols:
            # æå–æ•°å€¼ç‰¹å¾
            features_only = current_df[feature_cols].select_dtypes(include=[np.number])
            
            if not features_only.empty:
                # åº”ç”¨æ–¹å·®é˜ˆå€¼è¿‡æ»¤
                selector = VarianceThreshold(threshold=variance_threshold)
                selector.fit(features_only.fillna(0))
                
                # è·å–ä¿ç•™çš„ç‰¹å¾
                selected_mask = selector.get_support()
                removed_features = [col for col, keep in zip(features_only.columns, selected_mask) if not keep]
                kept_features = [col for col, keep in zip(features_only.columns, selected_mask) if keep]
                
                # æ„å»ºç»“æœDataFrame
                result_columns = ['close'] + kept_features
                if datetime_col:
                    result_columns = [datetime_col] + result_columns
                current_df = current_df[result_columns].copy()
                
                print(f"   ğŸ“Š åŸå§‹ç‰¹å¾æ•°: {len(feature_cols)}")
                print(f"   âŒ åˆ é™¤ä½æ–¹å·®ç‰¹å¾: {len(removed_features)}")
                print(f"   âœ… ä¿ç•™ç‰¹å¾æ•°: {len(kept_features)}")
                
                results['pipeline_steps'].append({
                    'step': 'variance_filter',
                    'removed_features': removed_features,
                    'remaining_features': len(kept_features)
                })
            else:
                print("   âš ï¸ æ²¡æœ‰æ•°å€¼å‹ç‰¹å¾ï¼Œè·³è¿‡æ–¹å·®è¿‡æ»¤")
        else:
            print("   âš ï¸ æ²¡æœ‰ç‰¹å¾åˆ—ï¼Œè·³è¿‡æ–¹å·®è¿‡æ»¤")
        
        # æ­¥éª¤2: é«˜å…±çº¿æ€§å»é™¤
        print("\nğŸ”¸ æ­¥éª¤2: é«˜å…±çº¿æ€§ç‰¹å¾å»é™¤")
        # åŠ¨æ€æ£€æŸ¥datetimeåˆ—
        datetime_col = 'datetime' if 'datetime' in current_df.columns else None
        exclude_cols = ['close']
        if datetime_col:
            exclude_cols.append(datetime_col)
        feature_cols = [col for col in current_df.columns if col not in exclude_cols]
        
        if len(feature_cols) >= 2:
            # æå–æ•°å€¼ç‰¹å¾å¹¶è®¡ç®—ç›¸å…³çŸ©é˜µ
            features_only = current_df[feature_cols].select_dtypes(include=[np.number])
            
            if not features_only.empty and len(features_only.columns) >= 2:
                corr_matrix = features_only.corr().fillna(0)
                
                # æ‰¾å‡ºéœ€è¦åˆ é™¤çš„é«˜ç›¸å…³ç‰¹å¾
                removed_features = []
                remaining_features = list(features_only.columns)
                
                for i in range(len(corr_matrix.columns)):
                    for j in range(i+1, len(corr_matrix.columns)):
                        if (corr_matrix.columns[i] in remaining_features and 
                            corr_matrix.columns[j] in remaining_features):
                            if abs(corr_matrix.iloc[i, j]) > correlation_threshold:
                                # åˆ é™¤æ–¹å·®è¾ƒå°çš„ç‰¹å¾
                                var_i = features_only[corr_matrix.columns[i]].var()
                                var_j = features_only[corr_matrix.columns[j]].var()
                                
                                if var_i < var_j:
                                    removed_features.append(corr_matrix.columns[i])
                                    remaining_features.remove(corr_matrix.columns[i])
                                else:
                                    removed_features.append(corr_matrix.columns[j])
                                    remaining_features.remove(corr_matrix.columns[j])
                
                result_columns = ['close'] + remaining_features
                if datetime_col:
                    result_columns = [datetime_col] + result_columns
                current_df = current_df[result_columns].copy()
                
                print(f"   ğŸ“Š è¾“å…¥ç‰¹å¾æ•°: {len(feature_cols)}")
                print(f"   âŒ åˆ é™¤é«˜ç›¸å…³ç‰¹å¾: {len(removed_features)}")
                print(f"   âœ… ä¿ç•™ç‰¹å¾æ•°: {len(remaining_features)}")
                
                results['pipeline_steps'].append({
                    'step': 'correlation_filter',
                    'removed_features': removed_features,
                    'remaining_features': len(remaining_features)
                })
            else:
                print("   âš ï¸ æ•°å€¼ç‰¹å¾ä¸è¶³ï¼Œè·³è¿‡å…±çº¿æ€§æ£€æŸ¥")
        else:
            print("   âš ï¸ ç‰¹å¾æ•°ä¸è¶³2ä¸ªï¼Œè·³è¿‡å…±çº¿æ€§æ£€æŸ¥")
        
        # æ­¥éª¤3: åŸºäºé‡è¦æ€§çš„ç‰¹å¾é€‰æ‹©
        # è®¡ç®—å‰©ä½™ç‰¹å¾æ•°ï¼ˆæ’é™¤closeå’Œå¯èƒ½çš„datetimeåˆ—ï¼‰
        datetime_col = 'datetime' if 'datetime' in current_df.columns else None
        exclude_cols = ['close']
        if datetime_col:
            exclude_cols.append(datetime_col)
        remaining_features = len(current_df.columns) - len(exclude_cols)
        
        if remaining_features > final_k:
            print(f"\nğŸ”¸ æ­¥éª¤3: åŸºäºé‡è¦æ€§é€‰æ‹©Top-{final_k}ç‰¹å¾")
            
            feature_cols = [col for col in current_df.columns if col not in exclude_cols]
            features_data = current_df[feature_cols].select_dtypes(include=[np.number]).fillna(0)
            
            if not features_data.empty:
                # ç”Ÿæˆå¤šä¸ªé¢„æµ‹ç›®æ ‡ï¼ˆä¸åŒæ—¶é—´è·¨åº¦çš„æ”¶ç›Šç‡ï¼‰
                importance_results = {}
                combined_importance = pd.Series(0.0, index=features_data.columns)
                
                # ä¸ºä¸åŒçš„é¢„æµ‹ç›®æ ‡è®¡ç®—ç‰¹å¾é‡è¦æ€§
                targets = {
                    'return_1d': current_df['close'].pct_change().shift(-1),
                    'return_5d': current_df['close'].pct_change(5).shift(-5),
                    'return_10d': current_df['close'].pct_change(10).shift(-10)
                }
                
                for target_name, target_values in targets.items():
                    try:
                        # å‡†å¤‡è®­ç»ƒæ•°æ®
                        valid_indices = ~(target_values.isna() | features_data.isna().any(axis=1))
                        if valid_indices.sum() < 50:  # è‡³å°‘éœ€è¦50ä¸ªæ ·æœ¬
                            continue
                            
                        X = features_data[valid_indices]
                        y = target_values[valid_indices]
                        
                        # é€‰æ‹©æ¨¡å‹
                        if importance_method == 'xgboost' and self.use_xgboost:
                            model = XGBRegressor(n_estimators=100, random_state=42, verbosity=0)
                        else:
                            model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
                        
                        # è®­ç»ƒæ¨¡å‹å¹¶è·å–ç‰¹å¾é‡è¦æ€§
                        model.fit(X, y)
                        feature_importance = pd.Series(model.feature_importances_, index=X.columns)
                        importance_results[target_name] = feature_importance
                        
                        # ç´¯åŠ é‡è¦æ€§ï¼ˆç”¨äºç»¼åˆæ’åï¼‰
                        combined_importance += feature_importance
                        
                    except Exception as e:
                        continue
                
                if importance_results:
                    # é€‰æ‹©top-kç‰¹å¾
                    top_features = combined_importance.nlargest(final_k).index.tolist()
                    
                    # æ„å»ºç»“æœDataFrame
                    result_columns = ['close'] + top_features
                    if datetime_col:
                        result_columns = [datetime_col] + result_columns
                    current_df = current_df[result_columns].copy()
                    
                    print(f"   ğŸ“Š è¾“å…¥ç‰¹å¾æ•°: {remaining_features}")
                    print(f"   âœ… é€‰æ‹©ç‰¹å¾æ•°: {final_k}")
                    print(f"   ğŸ† Top-5ç‰¹å¾: {top_features[:5]}")
                    
                    # ä¿å­˜ç‰¹å¾é‡è¦æ€§ç”¨äºè¿”å›
                    feature_importance_dict = dict(combined_importance.nlargest(final_k))
                    
                    results['pipeline_steps'].append({
                        'step': 'importance_selection',
                        'method': importance_method,
                        'selected_features': top_features,
                        'feature_importance': feature_importance_dict
                    })
                else:
                    print("   âš ï¸ é‡è¦æ€§è®¡ç®—å¤±è´¥ï¼Œä¿æŒå½“å‰ç‰¹å¾")
                    feature_importance_dict = {}
            else:
                print("   âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„æ•°å€¼ç‰¹å¾")
        else:
            print(f"\nâœ… å½“å‰ç‰¹å¾æ•°({remaining_features})å·²æ»¡è¶³ç›®æ ‡ï¼Œè·³è¿‡é‡è¦æ€§é€‰æ‹©")
            feature_importance_dict = {}
            results['pipeline_steps'].append({
                'step': 'importance_selection',
                'skipped': True,
                'reason': f'features_count({remaining_features}) <= target({final_k})',
                'remaining_features': remaining_features
            })
        
        # æœ€ç»ˆç»“æœ
        datetime_col = 'datetime' if 'datetime' in current_df.columns else None
        exclude_cols = ['close']
        if datetime_col:
            exclude_cols.append(datetime_col)
        final_features = [col for col in current_df.columns if col not in exclude_cols]
        results.update({
            'final_features_df': current_df,
            'final_features': final_features,
            'final_features_count': len(final_features),
            'feature_importance': feature_importance_dict,
            'reduction_ratio': (results['original_features'] - len(final_features)) / results['original_features']
        })
        
        print("\n" + "=" * 60)
        print("ğŸ‰ ç»¼åˆç‰¹å¾é€‰æ‹©ç®¡é“å®Œæˆ!")
        print(f"   ğŸ“Š åŸå§‹ç‰¹å¾æ•°: {results['original_features']}")
        print(f"   âœ… æœ€ç»ˆç‰¹å¾æ•°: {len(final_features)}")
        print(f"   ğŸ“‰ ç‰¹å¾å‰Šå‡ç‡: {results['reduction_ratio']:.1%}")
        if final_features:
            print(f"   ğŸ† æœ€ç»ˆTop-10ç‰¹å¾: {final_features[:10]}")
        
        return results

    def analyze_features(self, features_df: pd.DataFrame, plot: bool = True) -> Dict:
        """
        åˆ†æç‰¹å¾åˆ†å¸ƒå’Œè´¨é‡ï¼ˆåº”åœ¨ç‰¹å¾é€‰æ‹©ä¹‹åä½¿ç”¨ï¼‰
        
        Parameters:
        -----------
        features_df : pd.DataFrame
            ç‰¹å¾æ•°æ®ï¼ˆå·²ç»è¿‡ç‰¹å¾é€‰æ‹©çš„æ•°æ®ï¼‰
        plot : bool, default=True
            æ˜¯å¦ç»˜åˆ¶åˆ†æå›¾è¡¨
            
        Returns:
        --------
        dict
            ç‰¹å¾åˆ†æç»“æœ
        """
        print("ğŸ“Š å¼€å§‹ç‰¹å¾åˆ†æ...")
        
        # é¦–å…ˆå±•ç¤ºç‰¹å¾æ•°æ®é¢„è§ˆ
        self._display_feature_preview(features_df, "åˆ†æç‰¹å¾")
        
        # åŠ¨æ€æ£€æµ‹datetimeåˆ—
        datetime_col = 'datetime' if 'datetime' in features_df.columns else None
        exclude_cols = ['close']
        if datetime_col:
            exclude_cols.append(datetime_col)
        feature_cols = [col for col in features_df.columns if col not in exclude_cols]
        
        analysis = {
            'total_features': len(feature_cols),
            'missing_values': {},
            'extreme_values': {},
            'distributions': {}
        }
        
        # ç¼ºå¤±å€¼åˆ†æ
        for col in feature_cols:
            missing_count = features_df[col].isnull().sum()
            missing_pct = missing_count / len(features_df) * 100
            if missing_count > 0:
                analysis['missing_values'][col] = {
                    'count': missing_count,
                    'percentage': missing_pct
                }
        
        # æå€¼åˆ†æ
        for col in feature_cols:
            if features_df[col].dtype in ['float64', 'int64']:
                q1 = features_df[col].quantile(0.25)
                q3 = features_df[col].quantile(0.75)
                iqr = q3 - q1
                lower_bound = q1 - 1.5 * iqr
                upper_bound = q3 + 1.5 * iqr
                
                outliers = ((features_df[col] < lower_bound) | (features_df[col] > upper_bound)).sum()
                if outliers > 0:
                    analysis['extreme_values'][col] = {
                        'count': outliers,
                        'percentage': outliers / len(features_df) * 100,
                        'bounds': (lower_bound, upper_bound)
                    }
        
        # åˆ†å¸ƒç»Ÿè®¡
        numeric_features = features_df[feature_cols].select_dtypes(include=[np.number])
        analysis['distributions'] = {
            'mean': numeric_features.mean().to_dict(),
            'std': numeric_features.std().to_dict(),
            'skewness': numeric_features.skew().to_dict(),
            'kurtosis': numeric_features.kurtosis().to_dict()
        }
        
        # è¾“å‡ºåˆ†æç»“æœ
        print("\nğŸ“ˆ ç‰¹å¾åˆ†æç»“æœ:")
        print(f"   ğŸ”¢ æ€»ç‰¹å¾æ•°: {analysis['total_features']}")
        print(f"   âŒ ç¼ºå¤±å€¼ç‰¹å¾: {len(analysis['missing_values'])}")
        print(f"   âš ï¸ å¼‚å¸¸å€¼ç‰¹å¾: {len(analysis['extreme_values'])}")
        
        if analysis['missing_values']:
            print("   ğŸ“‹ ç¼ºå¤±å€¼è¯¦æƒ…:")
            for col, info in analysis['missing_values'].items():
                print(f"      {col}: {info['count']} ({info['percentage']:.1f}%)")
        
        if analysis['extreme_values']:
            print("   ğŸ“‹ å¼‚å¸¸å€¼è¯¦æƒ…:")
            for col, info in list(analysis['extreme_values'].items())[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                print(f"      {col}: {info['count']} ({info['percentage']:.1f}%)")
        
        # ç»˜å›¾åˆ†æ
        if plot and len(numeric_features.columns) > 0:
            try:
                self._plot_feature_analysis(numeric_features)
            except Exception as e:
                print(f"âš ï¸ ç»˜å›¾åŠŸèƒ½ä¸å¯ç”¨: {str(e)}")
                print("æ¨èä½¿ç”¨ features_df.to_csv('ç‰¹å¾æ•°æ®.csv') ä¿å­˜æ•°æ®ååœ¨Excelä¸­æŸ¥çœ‹")
        
        # æ·»åŠ æ›´å¤šç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š æ•´ä½“æ•°æ®è´¨é‡è¯„ä¼°:")
        total_cells = len(features_df) * len(feature_cols)
        missing_cells = sum(analysis['missing_values'][col]['count'] for col in analysis['missing_values'])
        data_completeness = (total_cells - missing_cells) / total_cells * 100 if total_cells > 0 else 100
        print(f"   ğŸ“Š æ•°æ®å®Œæ•´æ€§: {data_completeness:.1f}%")
        print(f"   ğŸ“ˆ æ•°æ®èŒƒå›´: {features_df.index.min().date()} ~ {features_df.index.max().date()}")
        print(f"   ğŸ“… æ•°æ®ç‚¹æ•°: {len(features_df)} ä¸ªæ—¶é—´ç‚¹")
        
        return analysis
    
    def _display_feature_preview(self, features_df: pd.DataFrame, data_type: str = "ç‰¹å¾"):
        """
        æ™ºèƒ½å±•ç¤ºç‰¹å¾æ•°æ®é¢„è§ˆ
        
        Parameters:
        -----------
        features_df : pd.DataFrame
            ç‰¹å¾æ•°æ®æ¡†
        data_type : str
            æ•°æ®ç±»å‹æè¿°
        """
        print(f"\nğŸ“Š {data_type}æ•°æ®é¢„è§ˆ:")
        print("=" * 50)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"ğŸ“ˆ æ•°æ®å½¢çŠ¶: {features_df.shape[0]} è¡Œ Ã— {features_df.shape[1]} åˆ—")
        print(f"ğŸ“… æ—¶é—´èŒƒå›´: {features_df.index.min().date()} åˆ° {features_df.index.max().date()}")
        
        # ç‰¹å¾åˆ—ï¼ˆæ’é™¤closeåˆ—ï¼‰
        feature_cols = [col for col in features_df.columns if col != 'close']
        print(f"ğŸ”¢ ç‰¹å¾æ•°é‡: {len(feature_cols)}")
        
        # æ•°æ®è´¨é‡æ¦‚è§ˆ
        missing_info = features_df.isnull().sum()
        missing_features = missing_info[missing_info > 0]
        print(f"âŒ ç¼ºå¤±å€¼ç‰¹å¾: {len(missing_features)} ä¸ª")
        
        # å±•ç¤ºç­–ç•¥ï¼šæ ¹æ®ç‰¹å¾æ•°é‡å†³å®šå±•ç¤ºè¯¦ç»†ç¨‹åº¦
        if len(feature_cols) <= 10:
            # ç‰¹å¾è¾ƒå°‘ï¼šå±•ç¤ºæ‰€æœ‰ç‰¹å¾çš„è¯¦ç»†ä¿¡æ¯
            print("\nğŸ“‹ æ‰€æœ‰ç‰¹å¾è¯¦æƒ…:")
            for i, col in enumerate(feature_cols, 1):
                stats = features_df[col].describe()
                missing_pct = (features_df[col].isnull().sum() / len(features_df)) * 100
                print(f"  {i:2d}. {col:<25} | å‡å€¼:{stats['mean']:8.4f} | æ ‡å‡†å·®:{stats['std']:8.4f} | ç¼ºå¤±:{missing_pct:5.1f}%")
                
        elif len(feature_cols) <= 30:
            # ç‰¹å¾é€‚ä¸­ï¼šå±•ç¤ºå‰10ä¸ªç‰¹å¾çš„è¯¦ç»†ä¿¡æ¯ + ç»Ÿè®¡æ¦‚è§ˆ
            print("\nğŸ“‹ å‰10ä¸ªç‰¹å¾è¯¦æƒ…:")
            for i, col in enumerate(feature_cols[:10], 1):
                stats = features_df[col].describe()
                missing_pct = (features_df[col].isnull().sum() / len(features_df)) * 100
                print(f"  {i:2d}. {col:<25} | å‡å€¼:{stats['mean']:8.4f} | æ ‡å‡†å·®:{stats['std']:8.4f} | ç¼ºå¤±:{missing_pct:5.1f}%")
            
            if len(feature_cols) > 10:
                print(f"  ... è¿˜æœ‰ {len(feature_cols) - 10} ä¸ªç‰¹å¾")
                
        else:
            # ç‰¹å¾å¾ˆå¤šï¼šåªå±•ç¤ºç»Ÿè®¡æ¦‚è§ˆå’Œç‰¹å¾åç§°åˆ†ç±»
            print("\nğŸ“‹ ç‰¹å¾ç»Ÿè®¡æ¦‚è§ˆ:")
            numeric_features = features_df[feature_cols].select_dtypes(include=[np.number])
            overall_stats = numeric_features.describe().T
            
            print(f"  å¹³å‡å€¼èŒƒå›´: {overall_stats['mean'].min():.4f} ~ {overall_stats['mean'].max():.4f}")
            print(f"  æ ‡å‡†å·®èŒƒå›´: {overall_stats['std'].min():.4f} ~ {overall_stats['std'].max():.4f}")
            print(f"  æœ€å°å€¼èŒƒå›´: {overall_stats['min'].min():.4f} ~ {overall_stats['min'].max():.4f}")
            print(f"  æœ€å¤§å€¼èŒƒå›´: {overall_stats['max'].min():.4f} ~ {overall_stats['max'].max():.4f}")
            
            # ç‰¹å¾åˆ†ç±»å±•ç¤º
            self._categorize_features(feature_cols)
        
        # æ•°æ®æ ·æœ¬é¢„è§ˆï¼ˆå‰5è¡Œï¼Œé‡è¦åˆ—ï¼‰
        print("\nğŸ“„ æ•°æ®æ ·æœ¬é¢„è§ˆï¼ˆå‰5è¡Œï¼‰:")
        preview_cols = ['close'] + feature_cols[:min(6, len(feature_cols))]  # close + æœ€å¤š6ä¸ªç‰¹å¾
        preview_data = features_df[preview_cols].head()
        
        # æ ¼å¼åŒ–æ˜¾ç¤º
        pd.set_option('display.max_columns', 10)
        pd.set_option('display.width', 120)
        pd.set_option('display.float_format', '{:.4f}'.format)
        print(preview_data)
        pd.reset_option('display.max_columns')
        pd.reset_option('display.width')
        pd.reset_option('display.float_format')
        
        # ä¿å­˜åˆ°æ–‡ä»¶æç¤º
        if len(features_df) > 100 or len(feature_cols) > 20:
            print("\nğŸ’¾ æç¤º: æ•°æ®è¾ƒå¤§ï¼Œå¯ä½¿ç”¨ features_df.to_csv('features.csv') ä¿å­˜å®Œæ•´æ•°æ®")
            
        print("=" * 50)
    
    def _categorize_features(self, feature_cols: list):
        """
        å°†ç‰¹å¾æŒ‰ç±»å‹åˆ†ç±»å±•ç¤º
        """
        print("\nğŸ·ï¸  ç‰¹å¾åˆ†ç±»:")
        
        categories = {
            'æ”¶ç›Šç‡ç‰¹å¾': [col for col in feature_cols if 'return' in col],
            'åŠ¨é‡ç‰¹å¾': [col for col in feature_cols if 'momentum' in col],
            'æ»šåŠ¨ç»Ÿè®¡': [col for col in feature_cols if 'rolling' in col],
            'æ³¢åŠ¨ç‡ç‰¹å¾': [col for col in feature_cols if any(x in col for x in ['volatility', 'atr', 'skewness', 'kurtosis'])],
            'æˆäº¤é‡ç‰¹å¾': [col for col in feature_cols if 'volume' in col],
            'ä»·æ ¼ç‰¹å¾': [col for col in feature_cols if any(x in col for x in ['price', 'high', 'low', 'open', 'ratio'])],
            'æŠ€æœ¯æŒ‡æ ‡': [col for col in feature_cols if any(x in col for x in ['rsi', 'bb', 'macd'])],
            'è‡ªåŠ¨ç‰¹å¾': [col for col in feature_cols if col.startswith('auto_')],
            'å…¶ä»–ç‰¹å¾': [col for col in feature_cols if not any([
                'return' in col, 'momentum' in col, 'rolling' in col,
                any(x in col for x in ['volatility', 'atr', 'skewness', 'kurtosis']),
                'volume' in col, any(x in col for x in ['price', 'high', 'low', 'open', 'ratio']),
                any(x in col for x in ['rsi', 'bb', 'macd']), col.startswith('auto_')
            ])]
        }
        
        for category, features in categories.items():
            if features:
                print(f"  ğŸ“Œ {category} ({len(features)}ä¸ª): {', '.join(features[:5])}{'...' if len(features) > 5 else ''}")
    
    def _plot_feature_analysis(self, features_df: pd.DataFrame, max_plots: int = 12):
        """ç»˜åˆ¶ç‰¹å¾åˆ†æå›¾è¡¨"""
        if not HAS_MATPLOTLIB:
            print("âš ï¸ matplotlib æœªå®‰è£…ï¼Œæ— æ³•æ˜¾ç¤ºå›¾è¡¨")
            print("ğŸ’¾ å»ºè®®ä½¿ç”¨: pip install matplotlib")
            return
            
        try:
            import matplotlib.pyplot as plt
            
            # è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
            
            n_features = min(len(features_df.columns), max_plots)
            if n_features == 0:
                print("âš ï¸ æ²¡æœ‰æ•°å€¼ç‰¹å¾å¯ä»¥ç»˜åˆ¶")
                return
                
            # è®¡ç®—å­å›¾å¸ƒå±€
            rows = (n_features + 3) // 4  # æ¯è¡Œ4ä¸ªå­å›¾
            cols = min(4, n_features)
            
            fig, axes = plt.subplots(rows, cols, figsize=(16, 4 * rows))
            if rows == 1 and cols == 1:
                axes = [axes]
            elif rows == 1:
                axes = axes
            else:
                axes = axes.ravel()
            
            print(f"ğŸ“ˆ æ­£åœ¨ç”Ÿæˆ {n_features} ä¸ªç‰¹å¾çš„åˆ†å¸ƒå›¾...")
            
            for i, col in enumerate(features_df.columns[:n_features]):
                try:
                    # è®¡ç®—æœ‰æ•ˆæ•°æ®
                    valid_data = features_df[col].dropna()
                    if len(valid_data) == 0:
                        axes[i].text(0.5, 0.5, f'{col}\næ— æœ‰æ•ˆæ•°æ®', 
                                   ha='center', va='center', transform=axes[i].transAxes)
                        continue
                    
                    # ç»˜åˆ¶ç›´æ–¹å›¾
                    n_bins = min(30, max(10, len(valid_data) // 10))
                    axes[i].hist(valid_data, bins=n_bins, alpha=0.7, edgecolor='black', color='skyblue')
                    axes[i].set_title(f'{col}\nå‡å€¼:{valid_data.mean():.3f}, æ ‡å‡†å·®:{valid_data.std():.3f}', fontsize=10)
                    axes[i].tick_params(labelsize=8)
                    axes[i].grid(True, alpha=0.3)
                    
                except Exception as e:
                    axes[i].text(0.5, 0.5, f'{col}\nç»˜å›¾å¤±è´¥: {str(e)[:20]}', 
                               ha='center', va='center', transform=axes[i].transAxes)
            
            # éšè—å¤šä½™çš„å­å›¾
            for i in range(n_features, len(axes)):
                axes[i].set_visible(False)
            
            plt.tight_layout()
            plt.suptitle(f'ç‰¹å¾åˆ†å¸ƒåˆ†æ (Top-{n_features})', fontsize=14, y=0.98)
            
            # æ˜¾ç¤ºå›¾è¡¨
            print("ğŸ“ˆ æ­£åœ¨æ˜¾ç¤ºç‰¹å¾åˆ†å¸ƒå›¾...")
            plt.show()
            
        except ImportError:
            print("âš ï¸ matplotlib å¯¼å…¥å¤±è´¥")
        except Exception as e:
            print(f"âš ï¸ ç»˜å›¾è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            print("ğŸ’¾ å»ºè®®ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶ååœ¨å…¶ä»–å·¥å…·ä¸­æŸ¥çœ‹")


if __name__ == "__main__":
    """
    ä½¿ç”¨ç¤ºä¾‹ - ä»…ç”¨äºæ¼”ç¤ºçœŸå®æ•°æ®åŠ è½½å’Œç‰¹å¾å·¥ç¨‹
    """
    print("ğŸ¯ è‚¡ç¥¨ç‰¹å¾å·¥ç¨‹ - çœŸå®æ•°æ®ç‰ˆæœ¬")
    print("=" * 50)
    
    try:
        # åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹å™¨
        engineer = FeatureEngineer(use_talib=True, use_tsfresh=True)
        
        # ç¤ºä¾‹ï¼šåŠ è½½å¹³å®‰é“¶è¡Œ(000001)æœ€è¿‘1å¹´çš„æ•°æ®
        symbol = "000001"
        start_date = "2023-01-01" 
        end_date = "2024-12-31"
        
        # åŠ è½½çœŸå®æ•°æ®
        data = engineer.load_stock_data(symbol, start_date, end_date)
        
        if len(data) < 100:
            print("âš ï¸ æ•°æ®é‡ä¸è¶³100å¤©ï¼Œå»ºè®®æ‰©å¤§æ—¶é—´èŒƒå›´")
        
        # ç”Ÿæˆç‰¹å¾
        print("\nğŸ­ ç”ŸæˆæŠ€æœ¯ç‰¹å¾...")
        features_df = engineer.prepare_features(data, use_auto_features=True)
        print(f"âœ… æˆåŠŸç”Ÿæˆ {features_df.shape[1]-1} ä¸ªç‰¹å¾")
        
        # ç‰¹å¾é€‰æ‹©
        print("\nğŸ¯ æ‰§è¡Œç‰¹å¾é€‰æ‹©...")
        selection_results = engineer.select_features(
            features_df,
            final_k=20,
            variance_threshold=0.01,
            correlation_threshold=0.9
        )
        
        final_features = selection_results['final_features']
        print(f"âœ… æœ€ç»ˆé€‰æ‹© {len(final_features)} ä¸ªé‡è¦ç‰¹å¾")
        
        # ç‰¹å¾åˆ†æ
        print("\nğŸ“Š åˆ†æç‰¹å¾è´¨é‡...")
        analysis = engineer.analyze_features(selection_results['final_features_df'], plot=True)
        
        print(f"\nğŸ“‹ å¤„ç†å®Œæˆï¼")
        print(f"   ğŸ”¢ åŸå§‹æ•°æ®: {len(data)} å¤©")
        print(f"   ğŸ­ ç”Ÿæˆç‰¹å¾: {features_df.shape[1]-1} ä¸ª")
        print(f"   ğŸ¯ æœ€ç»ˆç‰¹å¾: {len(final_features)} ä¸ª")
        print(f"   ğŸ“Š ç‰¹å¾è´¨é‡: {analysis['total_features'] - len(analysis['missing_values'])} ä¸ªæ— ç¼ºå¤±å€¼")
        
        print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
        print("   1. engineer.load_stock_data() - åŠ è½½çœŸå®è‚¡ç¥¨æ•°æ®")
        print("   2. engineer.prepare_features() - ç”ŸæˆæŠ€æœ¯ç‰¹å¾")
        print("   3. engineer.select_features() - æ‰§è¡Œç‰¹å¾é€‰æ‹©")
        print("   4. engineer.analyze_features() - åˆ†æç‰¹å¾è´¨é‡")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {e}")
        print("è¯·æ£€æŸ¥:")
        print("  1. InfluxDBæœåŠ¡æ˜¯å¦è¿è¡Œ")
        print("  2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        print("  3. è‚¡ç¥¨ä»£ç å’Œæ—¥æœŸèŒƒå›´æ˜¯å¦æ­£ç¡®")