"""
ç‰¹å¾å·¥ç¨‹æ¨¡å— - è‚¡ç¥¨æ•°æ®ç‰¹å¾ç”Ÿæˆ
åŒ…å«æ‰‹å·¥ç‰¹å¾å’Œè‡ªåŠ¨ç‰¹å¾ç”ŸæˆåŠŸèƒ½
"""
import pandas as pd
import numpy as np
import warnings
import sys
import os
from typing import Dict, List, Optional, Tuple
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_regression
from sklearn.ensemble import RandomForestRegressor
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# å¯é€‰ä¾èµ–å¯¼å…¥
try:
    import talib
    TALIB_AVAILABLE = True
except ImportError:
    TALIB_AVAILABLE = False
    print("âš ï¸ talib æœªå®‰è£…ï¼Œéƒ¨åˆ†æŠ€æœ¯æŒ‡æ ‡å°†ä½¿ç”¨pandaså®ç°")

try:
    import tsfresh
    from tsfresh import extract_features
    from tsfresh.feature_extraction import ComprehensiveFCParameters, MinimalFCParameters
    from tsfresh.utilities.dataframe_functions import impute
    TSFRESH_AVAILABLE = True
except ImportError:
    TSFRESH_AVAILABLE = False
    print("âš ï¸ tsfresh æœªå®‰è£…ï¼Œè‡ªåŠ¨ç‰¹å¾ç”Ÿæˆä¸å¯ç”¨")

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("âš ï¸ xgboost æœªå®‰è£…ï¼Œå°†ä½¿ç”¨RandomForestè¿›è¡Œç‰¹å¾é‡è¦æ€§è¯„ä¼°")

# æ·»åŠ stock_infoè·¯å¾„ä»¥å¯¼å…¥ç›¸å…³æ¨¡å—
stock_info_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "stock_info")
if stock_info_path not in sys.path:
    sys.path.insert(0, stock_info_path)

try:
    import utils
    from stock_market_data_akshare import get_history_data
    INFLUXDB_AVAILABLE = True
except ImportError:
    INFLUXDB_AVAILABLE = False
    print("âš ï¸ InfluxDBç›¸å…³æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œæ— æ³•åŠ è½½çœŸå®è‚¡ç¥¨æ•°æ®")

warnings.filterwarnings('ignore')


class FeatureEngineer:
    """
    ç‰¹å¾å·¥ç¨‹ç±» - è‚¡ç¥¨æ•°æ®ç‰¹å¾ç”Ÿæˆ
    æ”¯æŒæ‰‹å·¥ç‰¹å¾å’Œè‡ªåŠ¨ç‰¹å¾ç”Ÿæˆ
    """
    
    def __init__(self, use_talib: bool = True, use_tsfresh: bool = True):
        """
        åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹å™¨
        
        Parameters:
        -----------
        use_talib : bool, default=True
            æ˜¯å¦ä½¿ç”¨talibåº“è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
        use_tsfresh : bool, default=True
            æ˜¯å¦å¯ç”¨tsfreshè‡ªåŠ¨ç‰¹å¾ç”Ÿæˆ
        """
        self.use_talib = use_talib and TALIB_AVAILABLE
        self.use_tsfresh = use_tsfresh and TSFRESH_AVAILABLE
        self.scaler = None
        
        print(f"ğŸ”§ ç‰¹å¾å·¥ç¨‹å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ“Š TA-Lib: {'âœ…' if self.use_talib else 'âŒ'}")
        print(f"   ğŸ¤– TSFresh: {'âœ…' if self.use_tsfresh else 'âŒ'}")
    
    def prepare_manual_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ç”Ÿæˆæ‰‹å·¥åŸºç¡€ç‰¹å¾ï¼ˆå¯è§£é‡Šç‰¹å¾ï¼‰
        
        Parameters:
        -----------
        df : pd.DataFrame
            åŒ…å«OHLCVæ•°æ®çš„DataFrame
            
        Returns:
        --------
        pd.DataFrame
            åŒ…å«æ‰‹å·¥ç‰¹å¾çš„DataFrame
        """
        print("ğŸ”¨ å¼€å§‹ç”Ÿæˆæ‰‹å·¥åŸºç¡€ç‰¹å¾...")
        data = df.copy()
        
        # 1. æ”¶ç›Šç‡ç‰¹å¾ (Returns)
        print("   ğŸ“ˆ è®¡ç®—æ”¶ç›Šç‡ç‰¹å¾...")
        data['return_1d'] = data['close'].pct_change()
        data['return_5d'] = data['close'].pct_change(5)
        data['return_10d'] = data['close'].pct_change(10)
        data['return_20d'] = data['close'].pct_change(20)
        
        # 2. æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾ (Rolling Statistics)
        print("   ğŸ“Š è®¡ç®—æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾...")
        windows = [5, 10, 20, 30]
        for window in windows:
            # æ»šåŠ¨å‡å€¼
            data[f'rolling_mean_{window}d'] = data['close'].rolling(window).mean()
            # æ»šåŠ¨æ ‡å‡†å·®
            data[f'rolling_std_{window}d'] = data['close'].rolling(window).std()
            # æ»šåŠ¨ä¸­ä½æ•°
            data[f'rolling_median_{window}d'] = data['close'].rolling(window).median()
            # ä»·æ ¼ç›¸å¯¹ä½ç½®
            data[f'price_position_{window}d'] = (data['close'] - data[f'rolling_mean_{window}d']) / data[f'rolling_std_{window}d']
        
        # 3. åŠ¨é‡ç‰¹å¾ (Momentum)
        print("   ğŸš€ è®¡ç®—åŠ¨é‡ç‰¹å¾...")
        momentum_periods = [3, 5, 10, 20]
        for period in momentum_periods:
            data[f'momentum_{period}d'] = (data['close'] / data['close'].shift(period)) - 1
        
        # 4. ATRå’Œæ³¢åŠ¨ç‡ç‰¹å¾ (Volatility)
        print("   ğŸ“Š è®¡ç®—æ³¢åŠ¨ç‡ç‰¹å¾...")
        if self.use_talib:
            data['atr_14'] = talib.ATR(data['high'].values, data['low'].values, data['close'].values, timeperiod=14)
        else:
            # æ‰‹å·¥è®¡ç®—ATR
            data['tr1'] = data['high'] - data['low']
            data['tr2'] = abs(data['high'] - data['close'].shift(1))
            data['tr3'] = abs(data['low'] - data['close'].shift(1))
            data['true_range'] = data[['tr1', 'tr2', 'tr3']].max(axis=1)
            data['atr_14'] = data['true_range'].rolling(14).mean()
            data.drop(['tr1', 'tr2', 'tr3', 'true_range'], axis=1, inplace=True)
        
        # ä»·æ ¼æ³¢åŠ¨ç‡
        data['volatility_5d'] = data['return_1d'].rolling(5).std()
        data['volatility_20d'] = data['return_1d'].rolling(20).std()
        
        # ååº¦å’Œå³°åº¦
        data['skewness_20d'] = data['return_1d'].rolling(20).skew()
        data['kurtosis_20d'] = data['return_1d'].rolling(20).kurt()
        
        # 5. æˆäº¤é‡ç‰¹å¾ (Volume Features)
        print("   ğŸ’° è®¡ç®—æˆäº¤é‡ç‰¹å¾...")
        data['volume_change'] = data['volume'].pct_change()
        data['volume_ma_5'] = data['volume'].rolling(5).mean()
        data['volume_ma_20'] = data['volume'].rolling(20).mean()
        data['volume_ratio_5d'] = data['volume'] / data['volume_ma_5']
        data['volume_ratio_20d'] = data['volume'] / data['volume_ma_20']
        
        # æˆäº¤é‡å˜åŒ–ç‡
        volume_periods = [3, 5, 10]
        for period in volume_periods:
            data[f'volume_roc_{period}d'] = (data['volume'] / data['volume'].shift(period)) - 1
        
        # 6. ä»·æ ¼èŒƒå›´ç‰¹å¾ (Price Range)
        print("   ğŸ“ è®¡ç®—ä»·æ ¼èŒƒå›´ç‰¹å¾...")
        data['high_low_ratio'] = data['high'] / data['low']
        data['high_close_ratio'] = data['high'] / data['close']
        data['low_close_ratio'] = data['low'] / data['close']
        data['open_close_ratio'] = data['close'] / data['open']
        
        # ä»·æ ¼èŒƒå›´ç›¸å¯¹åŒ–
        data['price_range'] = data['high'] - data['low']
        data['price_range_pct'] = data['price_range'] / data['close']
        data['open_close_range'] = abs(data['close'] - data['open'])
        data['open_close_range_pct'] = data['open_close_range'] / data['close']
        
        # 7. æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾ (Technical Indicators)
        print("   ğŸ” è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾...")
        
        # RSI
        if self.use_talib:
            data['rsi_14'] = talib.RSI(data['close'].values, timeperiod=14)
        else:
            # æ‰‹å·¥è®¡ç®—RSIï¼ˆç›¸å¯¹å¼ºå¼±æŒ‡æ•°ï¼‰
            delta = data['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss
            data['rsi_14'] = 100 - (100 / (1 + rs))
        
        # å¸ƒæ—å¸¦
        if self.use_talib:
            bb_upper, bb_middle, bb_lower = talib.BBANDS(data['close'].values, timeperiod=20, nbdevup=2, nbdevdn=2)
            data['bb_upper'] = bb_upper
            data['bb_middle'] = bb_middle
            data['bb_lower'] = bb_lower
        else:
            bb_window = 20
            bb_std = 2
            bb_ma = data['close'].rolling(bb_window).mean()
            bb_std_val = data['close'].rolling(bb_window).std()
            data['bb_upper'] = bb_ma + (bb_std_val * bb_std)
            data['bb_middle'] = bb_ma
            data['bb_lower'] = bb_ma - (bb_std_val * bb_std)
        
        data['bb_position'] = (data['close'] - data['bb_lower']) / (data['bb_upper'] - data['bb_lower'])
        data['bb_width'] = (data['bb_upper'] - data['bb_lower']) / data['bb_middle']
        
        # MACD
        if self.use_talib:
            macd, macd_signal, macd_hist = talib.MACD(data['close'].values, fastperiod=12, slowperiod=26, signalperiod=9)
            data['macd'] = macd
            data['macd_signal'] = macd_signal
            data['macd_hist'] = macd_hist
        else:
            ema_12 = data['close'].ewm(span=12).mean()
            ema_26 = data['close'].ewm(span=26).mean()
            data['macd'] = ema_12 - ema_26
            data['macd_signal'] = data['macd'].ewm(span=9).mean()
            data['macd_hist'] = data['macd'] - data['macd_signal']
        
        # æ¸…ç†æ— ç”¨åˆ—å¹¶å»é™¤ç¼ºå¤±å€¼
        feature_columns = [col for col in data.columns 
                          if col not in ['datetime', 'open', 'high', 'low', 'close', 'volume', 'turnover', 'open_interest']]
        
        result = data[['datetime', 'close'] + feature_columns].dropna()
        
        print(f"âœ… æ‰‹å·¥ç‰¹å¾ç”Ÿæˆå®Œæˆï¼Œç‰¹å¾æ•°é‡: {len(feature_columns)}")
        return result
    
    def prepare_auto_features(self, df: pd.DataFrame, window_size: int = 30, 
                            max_features: int = 100, n_jobs: int = 1) -> pd.DataFrame:
        """
        ä½¿ç”¨tsfreshè‡ªåŠ¨ç”Ÿæˆç‰¹å¾
        
        Parameters:
        -----------
        df : pd.DataFrame
            åŒ…å«OHLCVæ•°æ®çš„DataFrame
        window_size : int, default=30
            ç‰¹å¾æå–çš„çª—å£å¤§å°
        max_features : int, default=100
            æœ€å¤§ç‰¹å¾æ•°é‡ï¼ˆç”¨äºæ§åˆ¶ç»´åº¦çˆ†ç‚¸ï¼‰
        n_jobs : int, default=1
            å¹¶è¡Œå¤„ç†çš„ä½œä¸šæ•°
            
        Returns:
        --------
        pd.DataFrame
            åŒ…å«è‡ªåŠ¨ç”Ÿæˆç‰¹å¾çš„DataFrame
        """
        if not self.use_tsfresh:
            print("âŒ tsfreshä¸å¯ç”¨ï¼Œè·³è¿‡è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆ")
            return df[['datetime', 'close']].copy()
        
        print("ğŸ¤– å¼€å§‹è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆ...")
        print(f"   ğŸ“Š çª—å£å¤§å°: {window_size}")
        print(f"   ğŸ”¢ æœ€å¤§ç‰¹å¾æ•°: {max_features}")
        
        data = df.copy()
        
        # å‡†å¤‡tsfreshæ ¼å¼çš„æ•°æ®
        tsfresh_data = []
        
        # ä¸ºæ¯ä¸ªçª—å£åˆ›å»ºæ—¶åºæ•°æ®
        for i in range(window_size, len(data)):
            window_data = data.iloc[i-window_size:i].copy()
            window_id = i - window_size
            
            # ä¸ºtsfreshæ ¼å¼æ·»åŠ idå’Œsortåˆ—
            window_data['id'] = window_id
            window_data['time'] = range(len(window_data))
            
            tsfresh_data.append(window_data)
        
        if not tsfresh_data:
            print("âŒ æ•°æ®é‡ä¸è¶³ä»¥ç”Ÿæˆè‡ªåŠ¨ç‰¹å¾")
            return df[['datetime', 'close']].copy()
        
        # åˆå¹¶æ‰€æœ‰çª—å£æ•°æ®
        combined_data = pd.concat(tsfresh_data, ignore_index=True)
        
        # é€‰æ‹©è¦æå–ç‰¹å¾çš„åˆ—
        value_columns = ['close', 'volume', 'high', 'low', 'open']
        value_columns = [col for col in value_columns if col in combined_data.columns]
        
        try:
            print("   ğŸ”„ æå–ç‰¹å¾ä¸­...")
            
            # ä½¿ç”¨æœ€å°ç‰¹å¾é›†ä»¥æ§åˆ¶ç»´åº¦
            if max_features <= 50:
                fc_parameters = MinimalFCParameters()
            else:
                fc_parameters = ComprehensiveFCParameters()
            
            # æå–ç‰¹å¾
            extracted_features = extract_features(
                combined_data[['id', 'time'] + value_columns],
                column_id='id',
                column_sort='time',
                default_fc_parameters=fc_parameters,
                n_jobs=n_jobs
            )
            
            # å¤„ç†ç¼ºå¤±å€¼
            impute(extracted_features)
            
            # ç‰¹å¾é€‰æ‹©ï¼ˆæ§åˆ¶ç»´åº¦ï¼‰
            if len(extracted_features.columns) > max_features:
                print(f"   âœ‚ï¸ ç‰¹å¾é™ç»´: {len(extracted_features.columns)} -> {max_features}")
                
                # ç®€å•çš„æ–¹å·®ç­›é€‰
                feature_vars = extracted_features.var()
                selected_features = feature_vars.nlargest(max_features).index
                extracted_features = extracted_features[selected_features]
            
            # æ·»åŠ æ—¶é—´æˆ³å’Œä»·æ ¼ä¿¡æ¯
            result_indices = list(range(window_size, len(data)))
            result_data = data.iloc[result_indices][['datetime', 'close']].reset_index(drop=True)
            
            # åˆå¹¶ç‰¹å¾
            extracted_features.reset_index(drop=True, inplace=True)
            result = pd.concat([result_data, extracted_features], axis=1)
            
            print(f"âœ… è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆå®Œæˆï¼Œç‰¹å¾æ•°é‡: {len(extracted_features.columns)}")
            return result
            
        except Exception as e:
            print(f"âŒ è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆå¤±è´¥: {str(e)}")
            return df[['datetime', 'close']].copy()
    
    def prepare_combined_features(self, df: pd.DataFrame, window_size: int = 30,
                                auto_features: bool = True, max_auto_features: int = 50) -> pd.DataFrame:
        """
        ç”Ÿæˆç»„åˆç‰¹å¾ï¼ˆæ‰‹å·¥ + è‡ªåŠ¨ï¼‰
        
        Parameters:
        -----------
        df : pd.DataFrame
            åŸå§‹OHLCVæ•°æ®
        window_size : int, default=30
            è‡ªåŠ¨ç‰¹å¾æå–çš„çª—å£å¤§å°
        auto_features : bool, default=True
            æ˜¯å¦åŒ…å«è‡ªåŠ¨ç‰¹å¾
        max_auto_features : int, default=50
            æœ€å¤§è‡ªåŠ¨ç‰¹å¾æ•°é‡
            
        Returns:
        --------
        pd.DataFrame
            åŒ…å«æ‰€æœ‰ç‰¹å¾çš„DataFrame
        """
        print("ğŸ”§ å¼€å§‹ç”Ÿæˆç»„åˆç‰¹å¾...")
        
        # ç”Ÿæˆæ‰‹å·¥ç‰¹å¾
        manual_features = self.prepare_manual_features(df)
        
        if not auto_features or not self.use_tsfresh:
            print("âœ… ä»…ä½¿ç”¨æ‰‹å·¥ç‰¹å¾")
            return manual_features
        
        # ç”Ÿæˆè‡ªåŠ¨ç‰¹å¾
        auto_features_df = self.prepare_auto_features(df, window_size, max_auto_features)
        
        # åˆå¹¶ç‰¹å¾ï¼ˆä»¥æ‰‹å·¥ç‰¹å¾ä¸ºä¸»ï¼‰
        if len(auto_features_df) > 0 and len(auto_features_df.columns) > 2:
            # æ‰¾åˆ°é‡å çš„æ—¶é—´èŒƒå›´
            manual_times = set(manual_features['datetime'])
            auto_times = set(auto_features_df['datetime'])
            common_times = manual_times.intersection(auto_times)
            
            if common_times:
                # ç­›é€‰å…±åŒæ—¶é—´æ®µçš„æ•°æ®
                manual_filtered = manual_features[manual_features['datetime'].isin(common_times)].copy()
                auto_filtered = auto_features_df[auto_features_df['datetime'].isin(common_times)].copy()
                
                # æŒ‰æ—¶é—´æ’åº
                manual_filtered = manual_filtered.sort_values('datetime').reset_index(drop=True)
                auto_filtered = auto_filtered.sort_values('datetime').reset_index(drop=True)
                
                # åˆå¹¶ç‰¹å¾ï¼ˆå»é™¤é‡å¤çš„datetimeå’Œcloseåˆ—ï¼‰
                auto_features_only = auto_filtered.drop(['datetime', 'close'], axis=1, errors='ignore')
                combined = pd.concat([manual_filtered, auto_features_only], axis=1)
                
                print(f"âœ… ç»„åˆç‰¹å¾ç”Ÿæˆå®Œæˆ")
                print(f"   ğŸ“Š æ‰‹å·¥ç‰¹å¾: {len(manual_filtered.columns) - 2}")
                print(f"   ğŸ¤– è‡ªåŠ¨ç‰¹å¾: {len(auto_features_only.columns)}")
                print(f"   ğŸ¯ æ€»ç‰¹å¾æ•°: {len(combined.columns) - 2}")
                
                return combined
        
        print("âš ï¸ è‡ªåŠ¨ç‰¹å¾åˆå¹¶å¤±è´¥ï¼Œä»…è¿”å›æ‰‹å·¥ç‰¹å¾")
        return manual_features
    
    def analyze_features(self, features_df: pd.DataFrame, plot: bool = True) -> Dict:
        """
        åˆ†æç‰¹å¾åˆ†å¸ƒå’Œè´¨é‡
        
        Parameters:
        -----------
        features_df : pd.DataFrame
            ç‰¹å¾æ•°æ®
        plot : bool, default=True
            æ˜¯å¦ç»˜åˆ¶åˆ†æå›¾è¡¨
            
        Returns:
        --------
        Dict
            ç‰¹å¾åˆ†æç»“æœ
        """
        print("ğŸ“Š å¼€å§‹ç‰¹å¾åˆ†æ...")
        
        feature_cols = [col for col in features_df.columns if col not in ['datetime', 'close']]
        
        analysis = {
            'total_features': len(feature_cols),
            'missing_values': {},
            'extreme_values': {},
            'distributions': {}
        }
        
        # ç¼ºå¤±å€¼åˆ†æ
        for col in feature_cols:
            missing_count = features_df[col].isnull().sum()
            missing_pct = missing_count / len(features_df) * 100
            if missing_count > 0:
                analysis['missing_values'][col] = {
                    'count': missing_count,
                    'percentage': missing_pct
                }
        
        # æå€¼åˆ†æ
        for col in feature_cols:
            if features_df[col].dtype in ['float64', 'int64']:
                q1 = features_df[col].quantile(0.25)
                q3 = features_df[col].quantile(0.75)
                iqr = q3 - q1
                lower_bound = q1 - 1.5 * iqr
                upper_bound = q3 + 1.5 * iqr
                
                outliers = ((features_df[col] < lower_bound) | (features_df[col] > upper_bound)).sum()
                if outliers > 0:
                    analysis['extreme_values'][col] = {
                        'count': outliers,
                        'percentage': outliers / len(features_df) * 100,
                        'bounds': (lower_bound, upper_bound)
                    }
        
        # åˆ†å¸ƒç»Ÿè®¡
        numeric_features = features_df[feature_cols].select_dtypes(include=[np.number])
        analysis['distributions'] = {
            'mean': numeric_features.mean().to_dict(),
            'std': numeric_features.std().to_dict(),
            'min': numeric_features.min().to_dict(),
            'max': numeric_features.max().to_dict()
        }
        
        # æ‰“å°åˆ†æç»“æœ
        print(f"ğŸ“ˆ ç‰¹å¾åˆ†æç»“æœ:")
        print(f"   ğŸ”¢ æ€»ç‰¹å¾æ•°: {analysis['total_features']}")
        print(f"   âŒ ç¼ºå¤±å€¼ç‰¹å¾: {len(analysis['missing_values'])}")
        print(f"   âš ï¸ å¼‚å¸¸å€¼ç‰¹å¾: {len(analysis['extreme_values'])}")
        
        if analysis['missing_values']:
            print("   ğŸ“‹ ç¼ºå¤±å€¼è¯¦æƒ…:")
            for col, info in list(analysis['missing_values'].items())[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"      {col}: {info['count']} ({info['percentage']:.1f}%)")
        
        if analysis['extreme_values']:
            print("   ğŸ“‹ å¼‚å¸¸å€¼è¯¦æƒ…:")
            for col, info in list(analysis['extreme_values'].items())[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"      {col}: {info['count']} ({info['percentage']:.1f}%)")
        
        # ç»˜å›¾åˆ†æ
        if plot and len(numeric_features.columns) > 0:
            self._plot_feature_analysis(numeric_features)
        
        return analysis
    
    def _plot_feature_analysis(self, features_df: pd.DataFrame, max_plots: int = 12):
        """ç»˜åˆ¶ç‰¹å¾åˆ†æå›¾è¡¨"""
        try:
            plt.rcParams['font.sans-serif'] = ['SimHei']
            plt.rcParams['axes.unicode_minus'] = False
            
            n_features = min(len(features_df.columns), max_plots)
            fig, axes = plt.subplots(3, 4, figsize=(16, 12))
            axes = axes.ravel()
            
            for i, col in enumerate(features_df.columns[:n_features]):
                # åˆ†å¸ƒç›´æ–¹å›¾
                axes[i].hist(features_df[col].dropna(), bins=50, alpha=0.7, edgecolor='black')
                axes[i].set_title(f'{col}', fontsize=10)
                axes[i].tick_params(labelsize=8)
            
            # éšè—å¤šä½™çš„å­å›¾
            for i in range(n_features, len(axes)):
                axes[i].set_visible(False)
            
            plt.tight_layout()
            plt.suptitle('ç‰¹å¾åˆ†å¸ƒåˆ†æ', fontsize=14, y=0.98)
            plt.show()
            
        except Exception as e:
            print(f"âš ï¸ ç»˜å›¾å¤±è´¥: {str(e)}")

    def select_features(self, features_df: pd.DataFrame, 
                       final_k: int = 50,
                       variance_threshold: float = 0.01,
                       correlation_threshold: float = 0.95,
                       importance_method: str = 'random_forest',
                       target_col: str = 'close',
                       prediction_horizons: List[int] = [1, 5, 10]) -> Dict:
        """
        ç»¼åˆç‰¹å¾é€‰æ‹©ï¼ˆé›†æˆæ‰€æœ‰æ–¹æ³•ï¼‰
        
        Parameters:
        -----------
        features_df : pd.DataFrame
            åŸå§‹ç‰¹å¾æ•°æ®
        final_k : int, default=50
            æœ€ç»ˆä¿ç•™çš„ç‰¹å¾æ•°é‡
        variance_threshold : float, default=0.01
            æ–¹å·®é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„ç‰¹å¾å°†è¢«åˆ é™¤
        correlation_threshold : float, default=0.95
            ç›¸å…³ç³»æ•°é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼çš„ç‰¹å¾å¯¹å°†åˆ é™¤å…¶ä¸­ä¸€ä¸ª
        importance_method : str, default='random_forest'
            é‡è¦æ€§è¯„ä¼°æ–¹æ³• ('random_forest', 'xgboost')
        target_col : str, default='close'
            ç›®æ ‡åˆ—å
        prediction_horizons : List[int], default=[1, 5, 10]
            é¢„æµ‹æ—¶é—´è·¨åº¦åˆ—è¡¨
            
        Returns:
        --------
        Dict
            åŒ…å«å„æ­¥éª¤ç»“æœçš„ç»¼åˆä¿¡æ¯
        """
        print("ğŸš€ å¼€å§‹ç»¼åˆç‰¹å¾é€‰æ‹©ç®¡é“...")
        print(f"   ğŸ¯ ç›®æ ‡: ä» {len(features_df.columns) - 2} ä¸ªç‰¹å¾ä¸­é€‰æ‹© {final_k} ä¸ª")
        print("=" * 60)
        
        results = {
            'original_features': len(features_df.columns) - 2,
            'final_k': final_k,
            'pipeline_steps': []
        }
        
        current_df = features_df.copy()
        
        # æ­¥éª¤1: æ–¹å·®é˜ˆå€¼è¿‡æ»¤
        print("ğŸ”¸ æ­¥éª¤1: æ–¹å·®é˜ˆå€¼è¿‡æ»¤")
        feature_cols = [col for col in current_df.columns if col not in ['datetime', 'close']]
        
        if feature_cols:
            # æå–æ•°å€¼ç‰¹å¾
            features_only = current_df[feature_cols].select_dtypes(include=[np.number])
            
            if not features_only.empty:
                # åº”ç”¨æ–¹å·®é˜ˆå€¼è¿‡æ»¤
                selector = VarianceThreshold(threshold=variance_threshold)
                selector.fit(features_only.fillna(0))
                
                # è·å–ä¿ç•™çš„ç‰¹å¾
                selected_mask = selector.get_support()
                removed_features = [col for col, keep in zip(features_only.columns, selected_mask) if not keep]
                kept_features = [col for col, keep in zip(features_only.columns, selected_mask) if keep]
                
                # æ„å»ºç»“æœDataFrame
                result_columns = ['datetime', 'close'] + kept_features
                current_df = current_df[result_columns].copy()
                
                print(f"   ğŸ“Š åŸå§‹ç‰¹å¾æ•°: {len(feature_cols)}")
                print(f"   âŒ åˆ é™¤ä½æ–¹å·®ç‰¹å¾: {len(removed_features)}")
                print(f"   âœ… ä¿ç•™ç‰¹å¾æ•°: {len(kept_features)}")
                
                results['pipeline_steps'].append({
                    'step': 'variance_filter',
                    'removed_features': removed_features,
                    'remaining_features': len(kept_features)
                })
            else:
                print("   âš ï¸ æ²¡æœ‰æ•°å€¼å‹ç‰¹å¾ï¼Œè·³è¿‡æ–¹å·®è¿‡æ»¤")
        else:
            print("   âš ï¸ æ²¡æœ‰ç‰¹å¾åˆ—ï¼Œè·³è¿‡æ–¹å·®è¿‡æ»¤")
        
        # æ­¥éª¤2: é«˜å…±çº¿æ€§å»é™¤
        print("\nğŸ”¸ æ­¥éª¤2: é«˜å…±çº¿æ€§ç‰¹å¾å»é™¤")
        feature_cols = [col for col in current_df.columns if col not in ['datetime', 'close']]
        
        if len(feature_cols) >= 2:
            # æå–æ•°å€¼ç‰¹å¾å¹¶è®¡ç®—ç›¸å…³çŸ©é˜µ
            features_only = current_df[feature_cols].select_dtypes(include=[np.number])
            
            if not features_only.empty and len(features_only.columns) >= 2:
                # è®¡ç®—ç›¸å…³çŸ©é˜µ
                correlation_matrix = features_only.corr().abs()
                
                # æ‰¾åˆ°é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹
                removed_features = []
                remaining_features = list(features_only.columns)
                
                for i in range(len(correlation_matrix.columns)):
                    for j in range(i + 1, len(correlation_matrix.columns)):
                        col1 = correlation_matrix.columns[i]
                        col2 = correlation_matrix.columns[j]
                        
                        if col1 in remaining_features and col2 in remaining_features:
                            corr_value = correlation_matrix.iloc[i, j]
                            
                            if not pd.isna(corr_value) and corr_value > correlation_threshold:
                                # åˆ é™¤æ–¹å·®è¾ƒå°çš„ç‰¹å¾
                                var1 = features_only[col1].var()
                                var2 = features_only[col2].var()
                                
                                feature_to_remove = col1 if var1 < var2 else col2
                                if feature_to_remove in remaining_features:
                                    remaining_features.remove(feature_to_remove)
                                    removed_features.append(feature_to_remove)
                
                # æ„å»ºç»“æœDataFrame
                result_columns = ['datetime', 'close'] + remaining_features
                current_df = current_df[result_columns].copy()
                
                print(f"   ğŸ“Š è¾“å…¥ç‰¹å¾æ•°: {len(feature_cols)}")
                print(f"   âŒ åˆ é™¤é«˜ç›¸å…³ç‰¹å¾: {len(removed_features)}")
                print(f"   âœ… ä¿ç•™ç‰¹å¾æ•°: {len(remaining_features)}")
                
                results['pipeline_steps'].append({
                    'step': 'correlation_filter',
                    'removed_features': removed_features,
                    'remaining_features': len(remaining_features)
                })
            else:
                print("   âš ï¸ æ•°å€¼ç‰¹å¾ä¸è¶³ï¼Œè·³è¿‡å…±çº¿æ€§æ£€æŸ¥")
        else:
            print("   âš ï¸ ç‰¹å¾æ•°ä¸è¶³2ä¸ªï¼Œè·³è¿‡å…±çº¿æ€§æ£€æŸ¥")
        
        # æ­¥éª¤3: åŸºäºé‡è¦æ€§çš„æœ€ç»ˆé€‰æ‹©
        remaining_features = len(current_df.columns) - 2
        if remaining_features > final_k:
            print(f"\nğŸ”¸ æ­¥éª¤3: åŸºäºé‡è¦æ€§é€‰æ‹©Top-{final_k}ç‰¹å¾")
            
            feature_cols = [col for col in current_df.columns if col not in ['datetime', 'close']]
            features_data = current_df[feature_cols].select_dtypes(include=[np.number]).fillna(0)
            
            if not features_data.empty:
                # ç”Ÿæˆå¤šä¸ªé¢„æµ‹ç›®æ ‡ï¼ˆä¸åŒæ—¶é—´è·¨åº¦çš„æ”¶ç›Šç‡ï¼‰
                importance_results = {}
                combined_importance = pd.Series(0.0, index=features_data.columns)
                
                for horizon in prediction_horizons:
                    # ç”Ÿæˆç›®æ ‡å˜é‡ï¼ˆæœªæ¥æ”¶ç›Šç‡ï¼‰
                    target = current_df[target_col].pct_change(horizon).shift(-horizon)
                    
                    # å»é™¤NaNå€¼
                    valid_mask = ~(target.isna() | features_data.isnull().any(axis=1))
                    if valid_mask.sum() < 50:
                        continue
                    
                    X_valid = features_data[valid_mask]
                    y_valid = target[valid_mask]
                    
                    try:
                        # é€‰æ‹©æ¨¡å‹
                        if importance_method == 'random_forest':
                            model = RandomForestRegressor(
                                n_estimators=100, 
                                random_state=42, 
                                n_jobs=-1,
                                max_depth=10
                            )
                        elif importance_method == 'xgboost' and XGBOOST_AVAILABLE:
                            model = xgb.XGBRegressor(
                                n_estimators=100,
                                random_state=42,
                                n_jobs=-1,
                                max_depth=6
                            )
                        else:
                            model = RandomForestRegressor(
                                n_estimators=100, 
                                random_state=42, 
                                n_jobs=-1,
                                max_depth=10
                            )
                        
                        # è®­ç»ƒæ¨¡å‹
                        model.fit(X_valid, y_valid)
                        
                        # è·å–ç‰¹å¾é‡è¦æ€§
                        feature_importance = pd.Series(model.feature_importances_, index=X_valid.columns)
                        importance_results[f'{horizon}d'] = feature_importance
                        
                        # ç´¯åŠ é‡è¦æ€§ï¼ˆç”¨äºç»¼åˆæ’åï¼‰
                        combined_importance += feature_importance
                        
                    except Exception as e:
                        continue
                
                if importance_results:
                    # é€‰æ‹©top-kç‰¹å¾
                    top_features = combined_importance.nlargest(final_k).index.tolist()
                    
                    # æ„å»ºç»“æœDataFrame
                    result_columns = ['datetime', 'close'] + top_features
                    current_df = current_df[result_columns].copy()
                    
                    print(f"   ğŸ“Š è¾“å…¥ç‰¹å¾æ•°: {remaining_features}")
                    print(f"   âœ… é€‰æ‹©ç‰¹å¾æ•°: {len(top_features)}")
                    print(f"   ğŸ† Top-5ç‰¹å¾: {top_features[:5]}")
                    
                    results['pipeline_steps'].append({
                        'step': 'importance_selection',
                        'method': importance_method,
                        'selected_features': top_features,
                        'remaining_features': len(top_features)
                    })
                else:
                    print("   âŒ é‡è¦æ€§è®¡ç®—å¤±è´¥ï¼Œä¿æŒå½“å‰ç‰¹å¾")
            else:
                print("   âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„æ•°å€¼ç‰¹å¾")
        else:
            print(f"\nâœ… å½“å‰ç‰¹å¾æ•°({remaining_features})å·²æ»¡è¶³ç›®æ ‡ï¼Œè·³è¿‡é‡è¦æ€§é€‰æ‹©")
            results['pipeline_steps'].append({
                'step': 'importance_selection',
                'skipped': True,
                'reason': f'features_count({remaining_features}) <= target({final_k})',
                'remaining_features': remaining_features
            })
        
        # æœ€ç»ˆç»“æœ
        final_features = [col for col in current_df.columns if col not in ['datetime', 'close']]
        results.update({
            'final_features_df': current_df,
            'final_features': final_features,
            'final_features_count': len(final_features),
            'reduction_ratio': (results['original_features'] - len(final_features)) / results['original_features']
        })
        
        print("\n" + "=" * 60)
        print("ğŸ‰ ç»¼åˆç‰¹å¾é€‰æ‹©ç®¡é“å®Œæˆ!")
        print(f"   ğŸ“Š åŸå§‹ç‰¹å¾æ•°: {results['original_features']}")
        print(f"   âœ… æœ€ç»ˆç‰¹å¾æ•°: {len(final_features)}")
        print(f"   ğŸ“‰ ç‰¹å¾å‰Šå‡ç‡: {results['reduction_ratio']:.1%}")
        if final_features:
            print(f"   ğŸ† æœ€ç»ˆTop-10ç‰¹å¾: {final_features[:10]}")
        
        return results


def load_real_stock_data(symbol: str = "000001", start_date: str = "2022-01-01", end_date: str = "2024-12-31") -> pd.DataFrame:
    """
    ä»InfluxDBåŠ è½½çœŸå®è‚¡ç¥¨æ•°æ®
    
    Parameters:
    -----------
    symbol : str, default="000001"
        è‚¡ç¥¨ä»£ç 
    start_date : str, default="2022-01-01"
        å¼€å§‹æ—¥æœŸ
    end_date : str, default="2024-12-31"
        ç»“æŸæ—¥æœŸ
        
    Returns:
    --------
    pd.DataFrame
        åŒ…å«OHLCVæ•°æ®çš„DataFrameï¼Œå¦‚æœåŠ è½½å¤±è´¥åˆ™è¿”å›None
    """
    if not INFLUXDB_AVAILABLE:
        print("âŒ InfluxDBæ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•åŠ è½½çœŸå®æ•°æ®")
        return None
    
    try:
        print(f"ğŸ”— ä»InfluxDBåŠ è½½ {symbol} æ•°æ®...")
        
        # è·å–InfluxDBå®¢æˆ·ç«¯
        client = utils.get_influxdb_client()
        if client is None:
            print("âŒ æ— æ³•è¿æ¥åˆ°InfluxDB")
            return None
        
        query_api = client.query_api()
        
        # è½¬æ¢æ—¥æœŸæ ¼å¼
        start_str_rfc = f"{start_date}T00:00:00Z"
        end_str_rfc = f"{end_date}T23:59:59Z"
        
        # è·å–å†å²æ•°æ®
        df = get_history_data(query_api, symbol, start_str_rfc, end_str_rfc)
        
        if df.empty:
            print(f"âŒ InfluxDBä¸­æœªæ‰¾åˆ° {symbol} çš„æ•°æ®")
            client.close()
            return None
        
        # æ ‡å‡†åŒ–åˆ—å
        column_mapping = {
            'æ—¥æœŸ': 'datetime',
            'å¼€ç›˜': 'open',
            'æœ€é«˜': 'high', 
            'æœ€ä½': 'low',
            'æ”¶ç›˜': 'close',
            'æˆäº¤é‡': 'volume',
            'æˆäº¤é¢': 'turnover'
        }
        
        df = df.rename(columns=column_mapping)
        
        # ç¡®ä¿datetimeåˆ—æ˜¯æ­£ç¡®çš„æ—¶é—´æ ¼å¼
        df['datetime'] = pd.to_datetime(df['datetime'])
        df = df.sort_values('datetime').reset_index(drop=True)
        
        # æ·»åŠ ç¼ºå¤±çš„åˆ—
        if 'turnover' not in df.columns:
            df['turnover'] = 0.0
        
        print(f"âœ… ä»InfluxDBæˆåŠŸåŠ è½½ {len(df)} æ¡ {symbol} æ•°æ®")
        print(f"ğŸ“… æ•°æ®æ—¶é—´èŒƒå›´: {df['datetime'].min().date()} åˆ° {df['datetime'].max().date()}")
        
        client.close()
        return df[['datetime', 'open', 'high', 'low', 'close', 'volume', 'turnover']]
        
    except Exception as e:
        print(f"âŒ ä»InfluxDBåŠ è½½æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return None


# æµ‹è¯•å‡½æ•°
def test_feature_selection():
    """æµ‹è¯•åˆå¹¶åçš„ç‰¹å¾é€‰æ‹©åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•ç‰¹å¾é€‰æ‹©åŠŸèƒ½")
    print("=" * 50)
    
    # åŠ è½½çœŸå®è‚¡ç¥¨æ•°æ®
    print("ğŸ“Š æ•°æ®åŠ è½½é˜¶æ®µ...")
    test_data = load_real_stock_data("000001", "2023-01-01", "2024-12-31")
    
    if test_data is None or len(test_data) < 100:
        print("âŒ æ— æ³•è·å–çœŸå®è‚¡ç¥¨æ•°æ®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        import numpy as np
        dates = pd.date_range('2023-01-01', periods=300, freq='D')
        np.random.seed(42)
        test_data = pd.DataFrame({
            'datetime': dates,
            'open': np.random.rand(300) * 100 + 50,
            'high': np.random.rand(300) * 100 + 60,
            'low': np.random.rand(300) * 100 + 40,
            'close': np.random.rand(300) * 100 + 55,
            'volume': np.random.rand(300) * 1000000,
            'turnover': np.random.rand(300) * 10000000
        })
        data_source = "æ¨¡æ‹Ÿæ•°æ®"
    else:
        # é™åˆ¶æ•°æ®é‡ä»¥åŠ å¿«æµ‹è¯•
        if len(test_data) > 400:
            test_data = test_data.tail(400).reset_index(drop=True)
        data_source = "çœŸå®æ•°æ®"
    
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ ({data_source})ï¼Œæ•°æ®ç‚¹æ•°: {len(test_data)}")
    
    # ç”Ÿæˆç‰¹å¾
    print("\nğŸ“Š ç”Ÿæˆç‰¹å¾...")
    engineer = FeatureEngineer(use_tsfresh=False)  # å…³é—­tsfreshä»¥åŠ å¿«æµ‹è¯•
    features_df = engineer.prepare_manual_features(test_data)
    
    if features_df is None or len(features_df.columns) <= 2:
        print("âŒ ç‰¹å¾ç”Ÿæˆå¤±è´¥")
        return None
    
    original_feature_count = len(features_df.columns) - 2
    print(f"âœ… ç‰¹å¾ç”Ÿæˆå®Œæˆï¼Œç‰¹å¾æ•°é‡: {original_feature_count}")
    
    # æµ‹è¯•åˆå¹¶çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•
    print(f"\nï¿½ æµ‹è¯•ç»Ÿä¸€ç‰¹å¾é€‰æ‹©æ–¹æ³•...")
    selection_results = engineer.select_features(
        features_df, 
        final_k=25,
        variance_threshold=0.001,
        correlation_threshold=0.95,
        importance_method='random_forest'
    )
    
    # ç»“æœæ±‡æ€»
    print(f"\nğŸ“‹ æµ‹è¯•ç»“æœæ±‡æ€»:")
    print(f"   ğŸ“Š åŸå§‹ç‰¹å¾æ•°é‡: {original_feature_count}")
    print(f"   ğŸ† æœ€ç»ˆç‰¹å¾æ•°é‡: {selection_results['final_features_count']}")
    print(f"   ğŸ“‰ ç‰¹å¾å‰Šå‡ç‡: {selection_results['reduction_ratio']:.1%}")
    
    # ç‰¹å¾è´¨é‡æ£€æŸ¥
    final_features_df = selection_results['final_features_df']
    final_analysis = engineer.analyze_features(final_features_df, plot=False)
    
    print(f"   ğŸ” ç‰¹å¾è´¨é‡: ç¼ºå¤±å€¼ç‰¹å¾ {len(final_analysis['missing_values'])} ä¸ª")
    print(f"   ğŸ“ˆ å¼‚å¸¸å€¼ç‰¹å¾: {len(final_analysis['extreme_values'])} ä¸ª")
    
    print(f"\nâœ… ç»Ÿä¸€ç‰¹å¾é€‰æ‹©æµ‹è¯•æˆåŠŸå®Œæˆ!")
    print(f"ï¿½ ç°åœ¨åªéœ€è¦è°ƒç”¨ä¸€ä¸ª select_features() æ–¹æ³•å³å¯å®Œæˆæ‰€æœ‰ç‰¹å¾é€‰æ‹©")
    
    return selection_results


if __name__ == "__main__":
    print("ğŸ® ç‰¹å¾å·¥ç¨‹ä¸é€‰æ‹©æµ‹è¯•")
    print("=" * 50)
    
    try:
        # è¿è¡Œæµ‹è¯•
        results = test_feature_selection()
        
        if results is not None:
            print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
            print("   ğŸ“Š ç‰¹å¾ç”Ÿæˆ: engineer.prepare_manual_features()")
            print("   ğŸ¯ ç‰¹å¾é€‰æ‹©: engineer.select_features()  # ä¸€ä¸ªæ–¹æ³•å®Œæˆæ‰€æœ‰æ­¥éª¤")
            print("   ğŸ” ç‰¹å¾åˆ†æ: engineer.analyze_features()")
            print("\nğŸ‰ æ‰€æœ‰åŠŸèƒ½æµ‹è¯•å®Œæˆ!")
        else:
            print("\nâŒ æµ‹è¯•å¤±è´¥")
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()