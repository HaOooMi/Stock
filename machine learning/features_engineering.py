"""
ç‰¹å¾å·¥ç¨‹æ¨¡å— - è‚¡ç¥¨æ•°æ®ç‰¹å¾ç”Ÿæˆ
åŒ…å«æ‰‹å·¥ç‰¹å¾å’Œè‡ªåŠ¨ç‰¹å¾ç”ŸæˆåŠŸèƒ½
"""
import pandas as pd
import numpy as np
import warnings
import sys
import os
from typing import Dict, List, Optional, Tuple
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import matplotlib.pyplot as plt
import seaborn as sns

# å¯é€‰ä¾èµ–å¯¼å…¥
try:
    import talib
    TALIB_AVAILABLE = True
except ImportError:
    TALIB_AVAILABLE = False
    print("âš ï¸ talib æœªå®‰è£…ï¼Œéƒ¨åˆ†æŠ€æœ¯æŒ‡æ ‡å°†ä½¿ç”¨pandaså®ç°")

try:
    import tsfresh
    from tsfresh import extract_features
    from tsfresh.feature_extraction import ComprehensiveFCParameters, MinimalFCParameters
    from tsfresh.utilities.dataframe_functions import impute
    TSFRESH_AVAILABLE = True
except ImportError:
    TSFRESH_AVAILABLE = False
    print("âš ï¸ tsfresh æœªå®‰è£…ï¼Œè‡ªåŠ¨ç‰¹å¾ç”Ÿæˆä¸å¯ç”¨")

# æ·»åŠ stock_infoè·¯å¾„ä»¥å¯¼å…¥ç›¸å…³æ¨¡å—
stock_info_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "stock_info")
if stock_info_path not in sys.path:
    sys.path.insert(0, stock_info_path)

try:
    import utils
    from stock_market_data_akshare import get_history_data
    INFLUXDB_AVAILABLE = True
except ImportError:
    INFLUXDB_AVAILABLE = False
    print("âš ï¸ InfluxDBç›¸å…³æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œæ— æ³•åŠ è½½çœŸå®è‚¡ç¥¨æ•°æ®")

warnings.filterwarnings('ignore')


class FeatureEngineer:
    """
    ç‰¹å¾å·¥ç¨‹ç±» - è‚¡ç¥¨æ•°æ®ç‰¹å¾ç”Ÿæˆ
    æ”¯æŒæ‰‹å·¥ç‰¹å¾å’Œè‡ªåŠ¨ç‰¹å¾ç”Ÿæˆ
    """
    
    def __init__(self, use_talib: bool = True, use_tsfresh: bool = True):
        """
        åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹å™¨
        
        Parameters:
        -----------
        use_talib : bool, default=True
            æ˜¯å¦ä½¿ç”¨talibåº“è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
        use_tsfresh : bool, default=True
            æ˜¯å¦å¯ç”¨tsfreshè‡ªåŠ¨ç‰¹å¾ç”Ÿæˆ
        """
        self.use_talib = use_talib and TALIB_AVAILABLE
        self.use_tsfresh = use_tsfresh and TSFRESH_AVAILABLE
        self.scaler = None
        
        print(f"ğŸ”§ ç‰¹å¾å·¥ç¨‹å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ“Š TA-Lib: {'âœ…' if self.use_talib else 'âŒ'}")
        print(f"   ğŸ¤– TSFresh: {'âœ…' if self.use_tsfresh else 'âŒ'}")
    
    def prepare_manual_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ç”Ÿæˆæ‰‹å·¥åŸºç¡€ç‰¹å¾ï¼ˆå¯è§£é‡Šç‰¹å¾ï¼‰
        
        Parameters:
        -----------
        df : pd.DataFrame
            åŒ…å«OHLCVæ•°æ®çš„DataFrame
            
        Returns:
        --------
        pd.DataFrame
            åŒ…å«æ‰‹å·¥ç‰¹å¾çš„DataFrame
        """
        print("ğŸ”¨ å¼€å§‹ç”Ÿæˆæ‰‹å·¥åŸºç¡€ç‰¹å¾...")
        data = df.copy()
        
        # 1. æ”¶ç›Šç‡ç‰¹å¾ (Returns)
        print("   ğŸ“ˆ è®¡ç®—æ”¶ç›Šç‡ç‰¹å¾...")
        data['return_1d'] = data['close'].pct_change()
        data['return_5d'] = data['close'].pct_change(5)
        data['return_10d'] = data['close'].pct_change(10)
        data['return_20d'] = data['close'].pct_change(20)
        
        # 2. æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾ (Rolling Statistics)
        print("   ğŸ“Š è®¡ç®—æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾...")
        windows = [5, 10, 20, 30]
        for window in windows:
            # æ»šåŠ¨å‡å€¼
            data[f'rolling_mean_{window}d'] = data['close'].rolling(window).mean()
            # æ»šåŠ¨æ ‡å‡†å·®
            data[f'rolling_std_{window}d'] = data['close'].rolling(window).std()
            # æ»šåŠ¨ä¸­ä½æ•°
            data[f'rolling_median_{window}d'] = data['close'].rolling(window).median()
            # ä»·æ ¼ç›¸å¯¹ä½ç½®
            data[f'price_position_{window}d'] = (data['close'] - data[f'rolling_mean_{window}d']) / data[f'rolling_std_{window}d']
        
        # 3. åŠ¨é‡ç‰¹å¾ (Momentum)
        print("   ğŸš€ è®¡ç®—åŠ¨é‡ç‰¹å¾...")
        momentum_periods = [3, 5, 10, 20]
        for period in momentum_periods:
            data[f'momentum_{period}d'] = (data['close'] / data['close'].shift(period)) - 1
        
        # 4. ATRå’Œæ³¢åŠ¨ç‡ç‰¹å¾ (Volatility)
        print("   ğŸ“Š è®¡ç®—æ³¢åŠ¨ç‡ç‰¹å¾...")
        if self.use_talib:
            data['atr_14'] = talib.ATR(data['high'].values, data['low'].values, data['close'].values, timeperiod=14)
        else:
            # æ‰‹å·¥è®¡ç®—ATR
            data['tr1'] = data['high'] - data['low']
            data['tr2'] = abs(data['high'] - data['close'].shift(1))
            data['tr3'] = abs(data['low'] - data['close'].shift(1))
            data['true_range'] = data[['tr1', 'tr2', 'tr3']].max(axis=1)
            data['atr_14'] = data['true_range'].rolling(14).mean()
            data.drop(['tr1', 'tr2', 'tr3', 'true_range'], axis=1, inplace=True)
        
        # ä»·æ ¼æ³¢åŠ¨ç‡
        data['volatility_5d'] = data['return_1d'].rolling(5).std()
        data['volatility_20d'] = data['return_1d'].rolling(20).std()
        
        # ååº¦å’Œå³°åº¦
        data['skewness_20d'] = data['return_1d'].rolling(20).skew()
        data['kurtosis_20d'] = data['return_1d'].rolling(20).kurt()
        
        # 5. æˆäº¤é‡ç‰¹å¾ (Volume Features)
        print("   ğŸ’° è®¡ç®—æˆäº¤é‡ç‰¹å¾...")
        data['volume_change'] = data['volume'].pct_change()
        data['volume_ma_5'] = data['volume'].rolling(5).mean()
        data['volume_ma_20'] = data['volume'].rolling(20).mean()
        data['volume_ratio_5d'] = data['volume'] / data['volume_ma_5']
        data['volume_ratio_20d'] = data['volume'] / data['volume_ma_20']
        
        # æˆäº¤é‡å˜åŒ–ç‡
        volume_periods = [3, 5, 10]
        for period in volume_periods:
            data[f'volume_roc_{period}d'] = (data['volume'] / data['volume'].shift(period)) - 1
        
        # 6. ä»·æ ¼èŒƒå›´ç‰¹å¾ (Price Range)
        print("   ğŸ“ è®¡ç®—ä»·æ ¼èŒƒå›´ç‰¹å¾...")
        data['high_low_ratio'] = data['high'] / data['low']
        data['high_close_ratio'] = data['high'] / data['close']
        data['low_close_ratio'] = data['low'] / data['close']
        data['open_close_ratio'] = data['close'] / data['open']
        
        # ä»·æ ¼èŒƒå›´ç›¸å¯¹åŒ–
        data['price_range'] = data['high'] - data['low']
        data['price_range_pct'] = data['price_range'] / data['close']
        data['open_close_range'] = abs(data['close'] - data['open'])
        data['open_close_range_pct'] = data['open_close_range'] / data['close']
        
        # 7. æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾ (Technical Indicators)
        print("   ğŸ” è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾...")
        
        # RSI
        if self.use_talib:
            data['rsi_14'] = talib.RSI(data['close'].values, timeperiod=14)
        else:
            data['rsi_14'] = self._calculate_rsi(data['close'], window=14)
        
        # å¸ƒæ—å¸¦
        if self.use_talib:
            bb_upper, bb_middle, bb_lower = talib.BBANDS(data['close'].values, timeperiod=20, nbdevup=2, nbdevdn=2)
            data['bb_upper'] = bb_upper
            data['bb_middle'] = bb_middle
            data['bb_lower'] = bb_lower
        else:
            bb_window = 20
            bb_std = 2
            bb_ma = data['close'].rolling(bb_window).mean()
            bb_std_val = data['close'].rolling(bb_window).std()
            data['bb_upper'] = bb_ma + (bb_std_val * bb_std)
            data['bb_middle'] = bb_ma
            data['bb_lower'] = bb_ma - (bb_std_val * bb_std)
        
        data['bb_position'] = (data['close'] - data['bb_lower']) / (data['bb_upper'] - data['bb_lower'])
        data['bb_width'] = (data['bb_upper'] - data['bb_lower']) / data['bb_middle']
        
        # MACD
        if self.use_talib:
            macd, macd_signal, macd_hist = talib.MACD(data['close'].values, fastperiod=12, slowperiod=26, signalperiod=9)
            data['macd'] = macd
            data['macd_signal'] = macd_signal
            data['macd_hist'] = macd_hist
        else:
            ema_12 = data['close'].ewm(span=12).mean()
            ema_26 = data['close'].ewm(span=26).mean()
            data['macd'] = ema_12 - ema_26
            data['macd_signal'] = data['macd'].ewm(span=9).mean()
            data['macd_hist'] = data['macd'] - data['macd_signal']
        
        # æ¸…ç†æ— ç”¨åˆ—å¹¶å»é™¤ç¼ºå¤±å€¼
        feature_columns = [col for col in data.columns 
                          if col not in ['datetime', 'open', 'high', 'low', 'close', 'volume', 'turnover', 'open_interest']]
        
        result = data[['datetime', 'close'] + feature_columns].dropna()
        
        print(f"âœ… æ‰‹å·¥ç‰¹å¾ç”Ÿæˆå®Œæˆï¼Œç‰¹å¾æ•°é‡: {len(feature_columns)}")
        return result
    
    def prepare_auto_features(self, df: pd.DataFrame, window_size: int = 30, 
                            max_features: int = 100, n_jobs: int = 1) -> pd.DataFrame:
        """
        ä½¿ç”¨tsfreshè‡ªåŠ¨ç”Ÿæˆç‰¹å¾
        
        Parameters:
        -----------
        df : pd.DataFrame
            åŒ…å«OHLCVæ•°æ®çš„DataFrame
        window_size : int, default=30
            ç‰¹å¾æå–çš„çª—å£å¤§å°
        max_features : int, default=100
            æœ€å¤§ç‰¹å¾æ•°é‡ï¼ˆç”¨äºæ§åˆ¶ç»´åº¦çˆ†ç‚¸ï¼‰
        n_jobs : int, default=1
            å¹¶è¡Œå¤„ç†çš„ä½œä¸šæ•°
            
        Returns:
        --------
        pd.DataFrame
            åŒ…å«è‡ªåŠ¨ç”Ÿæˆç‰¹å¾çš„DataFrame
        """
        if not self.use_tsfresh:
            print("âŒ tsfreshä¸å¯ç”¨ï¼Œè·³è¿‡è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆ")
            return df[['datetime', 'close']].copy()
        
        print("ğŸ¤– å¼€å§‹è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆ...")
        print(f"   ğŸ“Š çª—å£å¤§å°: {window_size}")
        print(f"   ğŸ”¢ æœ€å¤§ç‰¹å¾æ•°: {max_features}")
        
        data = df.copy()
        
        # å‡†å¤‡tsfreshæ ¼å¼çš„æ•°æ®
        tsfresh_data = []
        
        # ä¸ºæ¯ä¸ªçª—å£åˆ›å»ºæ—¶åºæ•°æ®
        for i in range(window_size, len(data)):
            window_data = data.iloc[i-window_size:i].copy()
            window_id = i - window_size
            
            # ä¸ºtsfreshæ ¼å¼æ·»åŠ idå’Œsortåˆ—
            window_data['id'] = window_id
            window_data['time'] = range(len(window_data))
            
            tsfresh_data.append(window_data)
        
        if not tsfresh_data:
            print("âŒ æ•°æ®é‡ä¸è¶³ä»¥ç”Ÿæˆè‡ªåŠ¨ç‰¹å¾")
            return df[['datetime', 'close']].copy()
        
        # åˆå¹¶æ‰€æœ‰çª—å£æ•°æ®
        combined_data = pd.concat(tsfresh_data, ignore_index=True)
        
        # é€‰æ‹©è¦æå–ç‰¹å¾çš„åˆ—
        value_columns = ['close', 'volume', 'high', 'low', 'open']
        value_columns = [col for col in value_columns if col in combined_data.columns]
        
        try:
            print("   ğŸ”„ æå–ç‰¹å¾ä¸­...")
            
            # ä½¿ç”¨æœ€å°ç‰¹å¾é›†ä»¥æ§åˆ¶ç»´åº¦
            if max_features <= 50:
                fc_parameters = MinimalFCParameters()
            else:
                fc_parameters = ComprehensiveFCParameters()
            
            # æå–ç‰¹å¾
            extracted_features = extract_features(
                combined_data[['id', 'time'] + value_columns],
                column_id='id',
                column_sort='time',
                default_fc_parameters=fc_parameters,
                n_jobs=n_jobs
            )
            
            # å¤„ç†ç¼ºå¤±å€¼
            impute(extracted_features)
            
            # ç‰¹å¾é€‰æ‹©ï¼ˆæ§åˆ¶ç»´åº¦ï¼‰
            if len(extracted_features.columns) > max_features:
                print(f"   âœ‚ï¸ ç‰¹å¾é™ç»´: {len(extracted_features.columns)} -> {max_features}")
                
                # ç®€å•çš„æ–¹å·®ç­›é€‰
                feature_vars = extracted_features.var()
                selected_features = feature_vars.nlargest(max_features).index
                extracted_features = extracted_features[selected_features]
            
            # æ·»åŠ æ—¶é—´æˆ³å’Œä»·æ ¼ä¿¡æ¯
            result_indices = list(range(window_size, len(data)))
            result_data = data.iloc[result_indices][['datetime', 'close']].reset_index(drop=True)
            
            # åˆå¹¶ç‰¹å¾
            extracted_features.reset_index(drop=True, inplace=True)
            result = pd.concat([result_data, extracted_features], axis=1)
            
            print(f"âœ… è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆå®Œæˆï¼Œç‰¹å¾æ•°é‡: {len(extracted_features.columns)}")
            return result
            
        except Exception as e:
            print(f"âŒ è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆå¤±è´¥: {str(e)}")
            return df[['datetime', 'close']].copy()
    
    def prepare_combined_features(self, df: pd.DataFrame, window_size: int = 30,
                                auto_features: bool = True, max_auto_features: int = 50) -> pd.DataFrame:
        """
        ç”Ÿæˆç»„åˆç‰¹å¾ï¼ˆæ‰‹å·¥ + è‡ªåŠ¨ï¼‰
        
        Parameters:
        -----------
        df : pd.DataFrame
            åŸå§‹OHLCVæ•°æ®
        window_size : int, default=30
            è‡ªåŠ¨ç‰¹å¾æå–çš„çª—å£å¤§å°
        auto_features : bool, default=True
            æ˜¯å¦åŒ…å«è‡ªåŠ¨ç‰¹å¾
        max_auto_features : int, default=50
            æœ€å¤§è‡ªåŠ¨ç‰¹å¾æ•°é‡
            
        Returns:
        --------
        pd.DataFrame
            åŒ…å«æ‰€æœ‰ç‰¹å¾çš„DataFrame
        """
        print("ğŸ”§ å¼€å§‹ç”Ÿæˆç»„åˆç‰¹å¾...")
        
        # ç”Ÿæˆæ‰‹å·¥ç‰¹å¾
        manual_features = self.prepare_manual_features(df)
        
        if not auto_features or not self.use_tsfresh:
            print("âœ… ä»…ä½¿ç”¨æ‰‹å·¥ç‰¹å¾")
            return manual_features
        
        # ç”Ÿæˆè‡ªåŠ¨ç‰¹å¾
        auto_features_df = self.prepare_auto_features(df, window_size, max_auto_features)
        
        # åˆå¹¶ç‰¹å¾ï¼ˆä»¥æ‰‹å·¥ç‰¹å¾ä¸ºä¸»ï¼‰
        if len(auto_features_df) > 0 and len(auto_features_df.columns) > 2:
            # æ‰¾åˆ°é‡å çš„æ—¶é—´èŒƒå›´
            manual_times = set(manual_features['datetime'])
            auto_times = set(auto_features_df['datetime'])
            common_times = manual_times.intersection(auto_times)
            
            if common_times:
                # ç­›é€‰å…±åŒæ—¶é—´æ®µçš„æ•°æ®
                manual_filtered = manual_features[manual_features['datetime'].isin(common_times)].copy()
                auto_filtered = auto_features_df[auto_features_df['datetime'].isin(common_times)].copy()
                
                # æŒ‰æ—¶é—´æ’åº
                manual_filtered = manual_filtered.sort_values('datetime').reset_index(drop=True)
                auto_filtered = auto_filtered.sort_values('datetime').reset_index(drop=True)
                
                # åˆå¹¶ç‰¹å¾ï¼ˆå»é™¤é‡å¤çš„datetimeå’Œcloseåˆ—ï¼‰
                auto_features_only = auto_filtered.drop(['datetime', 'close'], axis=1, errors='ignore')
                combined = pd.concat([manual_filtered, auto_features_only], axis=1)
                
                print(f"âœ… ç»„åˆç‰¹å¾ç”Ÿæˆå®Œæˆ")
                print(f"   ğŸ“Š æ‰‹å·¥ç‰¹å¾: {len(manual_filtered.columns) - 2}")
                print(f"   ğŸ¤– è‡ªåŠ¨ç‰¹å¾: {len(auto_features_only.columns)}")
                print(f"   ğŸ¯ æ€»ç‰¹å¾æ•°: {len(combined.columns) - 2}")
                
                return combined
        
        print("âš ï¸ è‡ªåŠ¨ç‰¹å¾åˆå¹¶å¤±è´¥ï¼Œä»…è¿”å›æ‰‹å·¥ç‰¹å¾")
        return manual_features
    
    def analyze_features(self, features_df: pd.DataFrame, plot: bool = True) -> Dict:
        """
        åˆ†æç‰¹å¾åˆ†å¸ƒå’Œè´¨é‡
        
        Parameters:
        -----------
        features_df : pd.DataFrame
            ç‰¹å¾æ•°æ®
        plot : bool, default=True
            æ˜¯å¦ç»˜åˆ¶åˆ†æå›¾è¡¨
            
        Returns:
        --------
        Dict
            ç‰¹å¾åˆ†æç»“æœ
        """
        print("ğŸ“Š å¼€å§‹ç‰¹å¾åˆ†æ...")
        
        feature_cols = [col for col in features_df.columns if col not in ['datetime', 'close']]
        
        analysis = {
            'total_features': len(feature_cols),
            'missing_values': {},
            'extreme_values': {},
            'distributions': {}
        }
        
        # ç¼ºå¤±å€¼åˆ†æ
        for col in feature_cols:
            missing_count = features_df[col].isnull().sum()
            missing_pct = missing_count / len(features_df) * 100
            if missing_count > 0:
                analysis['missing_values'][col] = {
                    'count': missing_count,
                    'percentage': missing_pct
                }
        
        # æå€¼åˆ†æ
        for col in feature_cols:
            if features_df[col].dtype in ['float64', 'int64']:
                q1 = features_df[col].quantile(0.25)
                q3 = features_df[col].quantile(0.75)
                iqr = q3 - q1
                lower_bound = q1 - 1.5 * iqr
                upper_bound = q3 + 1.5 * iqr
                
                outliers = ((features_df[col] < lower_bound) | (features_df[col] > upper_bound)).sum()
                if outliers > 0:
                    analysis['extreme_values'][col] = {
                        'count': outliers,
                        'percentage': outliers / len(features_df) * 100,
                        'bounds': (lower_bound, upper_bound)
                    }
        
        # åˆ†å¸ƒç»Ÿè®¡
        numeric_features = features_df[feature_cols].select_dtypes(include=[np.number])
        analysis['distributions'] = {
            'mean': numeric_features.mean().to_dict(),
            'std': numeric_features.std().to_dict(),
            'min': numeric_features.min().to_dict(),
            'max': numeric_features.max().to_dict()
        }
        
        # æ‰“å°åˆ†æç»“æœ
        print(f"ğŸ“ˆ ç‰¹å¾åˆ†æç»“æœ:")
        print(f"   ğŸ”¢ æ€»ç‰¹å¾æ•°: {analysis['total_features']}")
        print(f"   âŒ ç¼ºå¤±å€¼ç‰¹å¾: {len(analysis['missing_values'])}")
        print(f"   âš ï¸ å¼‚å¸¸å€¼ç‰¹å¾: {len(analysis['extreme_values'])}")
        
        if analysis['missing_values']:
            print("   ğŸ“‹ ç¼ºå¤±å€¼è¯¦æƒ…:")
            for col, info in list(analysis['missing_values'].items())[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"      {col}: {info['count']} ({info['percentage']:.1f}%)")
        
        if analysis['extreme_values']:
            print("   ğŸ“‹ å¼‚å¸¸å€¼è¯¦æƒ…:")
            for col, info in list(analysis['extreme_values'].items())[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"      {col}: {info['count']} ({info['percentage']:.1f}%)")
        
        # ç»˜å›¾åˆ†æ
        if plot and len(numeric_features.columns) > 0:
            self._plot_feature_analysis(numeric_features)
        
        return analysis
    
    def _calculate_rsi(self, prices: pd.Series, window: int = 14) -> pd.Series:
        """æ‰‹å·¥è®¡ç®—RSI"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def _plot_feature_analysis(self, features_df: pd.DataFrame, max_plots: int = 12):
        """ç»˜åˆ¶ç‰¹å¾åˆ†æå›¾è¡¨"""
        try:
            plt.rcParams['font.sans-serif'] = ['SimHei']
            plt.rcParams['axes.unicode_minus'] = False
            
            n_features = min(len(features_df.columns), max_plots)
            fig, axes = plt.subplots(3, 4, figsize=(16, 12))
            axes = axes.ravel()
            
            for i, col in enumerate(features_df.columns[:n_features]):
                # åˆ†å¸ƒç›´æ–¹å›¾
                axes[i].hist(features_df[col].dropna(), bins=50, alpha=0.7, edgecolor='black')
                axes[i].set_title(f'{col}', fontsize=10)
                axes[i].tick_params(labelsize=8)
            
            # éšè—å¤šä½™çš„å­å›¾
            for i in range(n_features, len(axes)):
                axes[i].set_visible(False)
            
            plt.tight_layout()
            plt.suptitle('ç‰¹å¾åˆ†å¸ƒåˆ†æ', fontsize=14, y=0.98)
            plt.show()
            
        except Exception as e:
            print(f"âš ï¸ ç»˜å›¾å¤±è´¥: {str(e)}")


def load_real_stock_data(symbol: str = "000001", start_date: str = "2022-01-01", end_date: str = "2024-12-31") -> pd.DataFrame:
    """
    ä»InfluxDBåŠ è½½çœŸå®è‚¡ç¥¨æ•°æ®
    
    Parameters:
    -----------
    symbol : str, default="000001"
        è‚¡ç¥¨ä»£ç 
    start_date : str, default="2022-01-01"
        å¼€å§‹æ—¥æœŸ
    end_date : str, default="2024-12-31"
        ç»“æŸæ—¥æœŸ
        
    Returns:
    --------
    pd.DataFrame
        åŒ…å«OHLCVæ•°æ®çš„DataFrameï¼Œå¦‚æœåŠ è½½å¤±è´¥åˆ™è¿”å›None
    """
    if not INFLUXDB_AVAILABLE:
        print("âŒ InfluxDBæ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•åŠ è½½çœŸå®æ•°æ®")
        return None
    
    try:
        print(f"ğŸ”— ä»InfluxDBåŠ è½½ {symbol} æ•°æ®...")
        
        # è·å–InfluxDBå®¢æˆ·ç«¯
        client = utils.get_influxdb_client()
        if client is None:
            print("âŒ æ— æ³•è¿æ¥åˆ°InfluxDB")
            return None
        
        query_api = client.query_api()
        
        # è½¬æ¢æ—¥æœŸæ ¼å¼
        start_str_rfc = f"{start_date}T00:00:00Z"
        end_str_rfc = f"{end_date}T23:59:59Z"
        
        # è·å–å†å²æ•°æ®
        df = get_history_data(query_api, symbol, start_str_rfc, end_str_rfc)
        
        if df.empty:
            print(f"âŒ InfluxDBä¸­æœªæ‰¾åˆ° {symbol} çš„æ•°æ®")
            client.close()
            return None
        
        # æ ‡å‡†åŒ–åˆ—å
        column_mapping = {
            'æ—¥æœŸ': 'datetime',
            'å¼€ç›˜': 'open',
            'æœ€é«˜': 'high', 
            'æœ€ä½': 'low',
            'æ”¶ç›˜': 'close',
            'æˆäº¤é‡': 'volume',
            'æˆäº¤é¢': 'turnover'
        }
        
        df = df.rename(columns=column_mapping)
        
        # ç¡®ä¿datetimeåˆ—æ˜¯æ­£ç¡®çš„æ—¶é—´æ ¼å¼
        df['datetime'] = pd.to_datetime(df['datetime'])
        df = df.sort_values('datetime').reset_index(drop=True)
        
        # æ·»åŠ ç¼ºå¤±çš„åˆ—
        if 'turnover' not in df.columns:
            df['turnover'] = 0.0
        
        print(f"âœ… ä»InfluxDBæˆåŠŸåŠ è½½ {len(df)} æ¡ {symbol} æ•°æ®")
        print(f"ğŸ“… æ•°æ®æ—¶é—´èŒƒå›´: {df['datetime'].min().date()} åˆ° {df['datetime'].max().date()}")
        
        client.close()
        return df[['datetime', 'open', 'high', 'low', 'close', 'volume', 'turnover']]
        
    except Exception as e:
        print(f"âŒ ä»InfluxDBåŠ è½½æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return None


# æµ‹è¯•å‡½æ•°
def test_feature_engineering():
    """æµ‹è¯•ç‰¹å¾å·¥ç¨‹åŠŸèƒ½ï¼ˆåŒ…å«æ‰‹å·¥ç‰¹å¾å’Œè‡ªåŠ¨ç‰¹å¾ï¼‰"""
    print("ğŸ§ª æµ‹è¯•ç‰¹å¾å·¥ç¨‹åŠŸèƒ½")
    print("=" * 50)
    
    # åŠ è½½çœŸå®è‚¡ç¥¨æ•°æ®
    print("ğŸ“Š æ•°æ®åŠ è½½é˜¶æ®µ...")
    test_data = load_real_stock_data("000001", "2022-01-01", "2024-12-31")
    
    if test_data is None or len(test_data) < 100:
        print("âŒ æ— æ³•è·å–çœŸå®è‚¡ç¥¨æ•°æ®æˆ–æ•°æ®é‡ä¸è¶³")
        print("ğŸ’¡ è¯·æ£€æŸ¥:")
        print("   1. InfluxDBæœåŠ¡æ˜¯å¦è¿è¡Œ")
        print("   2. æ•°æ®åº“ä¸­æ˜¯å¦åŒ…å«000001è‚¡ç¥¨æ•°æ®")
        print("   3. stock_infoæ¨¡å—æ˜¯å¦æ­£ç¡®é…ç½®")
        return None
    
    data_source = "çœŸå®è‚¡ç¥¨æ•°æ®"
    # å¦‚æœçœŸå®æ•°æ®å¤ªå¤šï¼Œå–æœ€è¿‘çš„æ•°æ®
    if len(test_data) > 500:
        test_data = test_data.tail(500).reset_index(drop=True)
    
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ ({data_source})")
    print(f"ğŸ“Š æ•°æ®ç‚¹æ•°: {len(test_data)}")
    print(f"ğŸ“… æ—¶é—´èŒƒå›´: {test_data['datetime'].min().date()} åˆ° {test_data['datetime'].max().date()}")
    
    # æ£€æŸ¥tsfreshå¯ç”¨æ€§
    print("\nğŸ” æ£€æŸ¥tsfreshåº“å¯ç”¨æ€§...")
    if TSFRESH_AVAILABLE:
        print("âœ… tsfreshåº“å·²å®‰è£…ï¼Œå°†æµ‹è¯•è‡ªåŠ¨ç‰¹å¾åŠŸèƒ½")
        try:
            from tsfresh import extract_features
            # ç®€å•åŠŸèƒ½æµ‹è¯•
            simple_data = pd.DataFrame({
                'id': [1, 1, 1], 'time': [1, 2, 3], 'value': [1, 2, 3]
            })
            test_extract = extract_features(simple_data, column_id='id', column_sort='time')
            print(f"   ğŸ§ª tsfreshåŸºæœ¬åŠŸèƒ½æ­£å¸¸ï¼Œæµ‹è¯•æå–äº† {len(test_extract.columns)} ä¸ªç‰¹å¾")
        except Exception as e:
            print(f"   âš ï¸ tsfreshåŠŸèƒ½å¼‚å¸¸: {str(e)}")
    else:
        print("âŒ tsfreshåº“æœªå®‰è£…")
        print("ğŸ’¡ å®‰è£…æç¤º: pip install tsfresh")
    
    # åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹å™¨
    engineer = FeatureEngineer(use_tsfresh=True)
    
    # 1. æµ‹è¯•æ‰‹å·¥ç‰¹å¾
    print("\nğŸ“Š æµ‹è¯•æ‰‹å·¥ç‰¹å¾ç”Ÿæˆ...")
    manual_features = engineer.prepare_manual_features(test_data)
    print(f"âœ… æ‰‹å·¥ç‰¹å¾æµ‹è¯•å®Œæˆï¼Œç‰¹å¾æ•°é‡: {len(manual_features.columns) - 2}")
    
    # åˆ†ææ‰‹å·¥ç‰¹å¾
    print("\nğŸ“ˆ åˆ†ææ‰‹å·¥ç‰¹å¾...")
    manual_analysis = engineer.analyze_features(manual_features, plot=False)
    
    # 2. æµ‹è¯•è‡ªåŠ¨ç‰¹å¾ï¼ˆä»…å½“tsfreshå¯ç”¨æ—¶ï¼‰
    auto_features = None
    auto_analysis = None
    combined_features = None
    combined_analysis = None
    
    if engineer.use_tsfresh:
        print("\nğŸ¤– æµ‹è¯•è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆ...")
        try:
            auto_features = engineer.prepare_auto_features(
                test_data, 
                window_size=30, 
                max_features=20,
                n_jobs=1
            )
            
            if auto_features is not None and len(auto_features.columns) > 2:
                print(f"âœ… è‡ªåŠ¨ç‰¹å¾ç”ŸæˆæˆåŠŸï¼Œç‰¹å¾æ•°é‡: {len(auto_features.columns) - 2}")
                
                # æ˜¾ç¤ºéƒ¨åˆ†ç‰¹å¾åç§°
                feature_names = [col for col in auto_features.columns if col not in ['datetime', 'close']]
                if feature_names:
                    print(f"   ğŸ·ï¸ ç‰¹å¾ç¤ºä¾‹: {feature_names[:3]}")
                
                # åˆ†æè‡ªåŠ¨ç‰¹å¾
                print("\nğŸ“ˆ åˆ†æè‡ªåŠ¨ç‰¹å¾...")
                auto_analysis = engineer.analyze_features(auto_features, plot=False)
                
        except Exception as e:
            print(f"âŒ è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆå‡ºé”™: {str(e)}")
        
        # 3. æµ‹è¯•ç»„åˆç‰¹å¾
        print("\nğŸ”§ æµ‹è¯•ç»„åˆç‰¹å¾ç”Ÿæˆ...")
        try:
            combined_features = engineer.prepare_combined_features(
                test_data, 
                window_size=30,
                auto_features=True,
                max_auto_features=15
            )
            
            if combined_features is not None:
                print(f"âœ… ç»„åˆç‰¹å¾ç”ŸæˆæˆåŠŸï¼Œæ€»ç‰¹å¾æ•°: {len(combined_features.columns) - 2}")
                
                # åˆ†æç»„åˆç‰¹å¾
                print("\nğŸ“ˆ åˆ†æç»„åˆç‰¹å¾...")
                combined_analysis = engineer.analyze_features(combined_features, plot=False)
                
                # ç»Ÿè®¡ç‰¹å¾ç±»å‹
                all_feature_cols = [col for col in combined_features.columns if col not in ['datetime', 'close']]
                manual_feature_cols = [col for col in manual_features.columns if col not in ['datetime', 'close']]
                auto_feature_count = len(all_feature_cols) - len(manual_feature_cols)
                
                print(f"\nğŸ“‹ ç‰¹å¾ç»„æˆç»Ÿè®¡:")
                print(f"   ğŸ“Š æ‰‹å·¥ç‰¹å¾: {len(manual_feature_cols)}")
                print(f"   ğŸ¤– è‡ªåŠ¨ç‰¹å¾: {auto_feature_count}")
                print(f"   ğŸ¯ æ€»è®¡ç‰¹å¾: {len(all_feature_cols)}")
                
        except Exception as e:
            print(f"âŒ ç»„åˆç‰¹å¾ç”Ÿæˆå‡ºé”™: {str(e)}")
    
    else:
        print("\nâš ï¸ è·³è¿‡è‡ªåŠ¨ç‰¹å¾å’Œç»„åˆç‰¹å¾æµ‹è¯•ï¼ˆtsfreshä¸å¯ç”¨ï¼‰")
    
    # è¿”å›æµ‹è¯•ç»“æœ
    print("\n" + "=" * 50)
    print(f"ğŸ‰ ç‰¹å¾å·¥ç¨‹æµ‹è¯•å®Œæˆï¼(æ•°æ®æº: {data_source})")
    
    results = {
        'data_source': data_source,
        'test_data': test_data,
        'manual_features': manual_features,
        'manual_analysis': manual_analysis,
        'auto_features': auto_features,
        'auto_analysis': auto_analysis,
        'combined_features': combined_features,
        'combined_analysis': combined_analysis
    }
    
    return results


if __name__ == "__main__":
    # è¿è¡Œç»¼åˆæµ‹è¯•
    results = test_feature_engineering()
    
    if results is None:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼šæ— æ³•è·å–çœŸå®è‚¡ç¥¨æ•°æ®")
        print("ğŸ”§ è¯·æ£€æŸ¥InfluxDBé…ç½®å’Œæ•°æ®")
        exit(1)
    
    # ç®€å•çš„ç»“æœæŠ¥å‘Š
    print(f"\nğŸ“‹ æµ‹è¯•ç»“æœæ€»ç»“ (æ•°æ®æº: {results['data_source']}):")
    if results['manual_features'] is not None:
        print(f"   âœ… æ‰‹å·¥ç‰¹å¾: {len(results['manual_features'].columns) - 2} ä¸ª")
    if results['auto_features'] is not None and len(results['auto_features'].columns) > 2:
        print(f"   âœ… è‡ªåŠ¨ç‰¹å¾: {len(results['auto_features'].columns) - 2} ä¸ª")
    if results['combined_features'] is not None:
        print(f"   âœ… ç»„åˆç‰¹å¾: {len(results['combined_features'].columns) - 2} ä¸ª")
    
    print("\nğŸ’¡ è¯´æ˜:")
    print("   ğŸ¯ ä½¿ç”¨äº†çœŸå®çš„è‚¡ç¥¨å†å²æ•°æ®è¿›è¡Œç‰¹å¾å·¥ç¨‹æµ‹è¯•")
    print("   ğŸ“Š ç‰¹å¾è´¨é‡æ›´é«˜ï¼Œæ›´é€‚åˆå®é™…åº”ç”¨")
    print("   ï¿½ æ•°æ®æ¥æºï¼šInfluxDBæ•°æ®åº“")
    
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")