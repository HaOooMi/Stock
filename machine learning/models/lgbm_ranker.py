#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LightGBM æ’åºæ¨¡å‹ - LGBMRanker

åŠŸèƒ½ï¼š
1. å°è£… LightGBM æ’åºæ¨¡å‹ï¼ˆLambdaRank / Pairwiseï¼‰
2. æ”¯æŒ group åˆ†ç»„ï¼ˆæŒ‰æ—¥æœŸï¼‰
3. ä¸ BaseModel æ¥å£å…¼å®¹
4. æä¾› NDCG è¯„ä¼°ä¸æ—©åœ

è®¾è®¡åŸåˆ™ï¼š
- å¤ç”¨ BaseModel çš„ save/load/get_feature_importance æ¥å£
- å‚æ•°é¢„è®¾åå‘ç¨³å®šï¼ˆé˜²æ­¢æ’åºæ¨¡å‹è¿‡æ‹Ÿåˆï¼‰
- è®­ç»ƒæ•°æ®å¿…é¡»æŒ‰æ—¥æœŸæ’åºï¼Œgroup ä¸æ ·æœ¬é¡ºåºä¸€è‡´

LambdaRank æ ¸å¿ƒï¼š
- ä¼˜åŒ–ç›®æ ‡ï¼šæœ€å¤§åŒ– NDCGï¼ˆå¤´éƒ¨æ’åºè´¨é‡ï¼‰
- æŸå¤±å‡½æ•°ï¼šåŸºäº pairwise çš„æ¢¯åº¦æå‡
- é€‚ç”¨åœºæ™¯ï¼šå…³æ³¨ Top-K è‚¡ç¥¨çš„æ’åºå‡†ç¡®æ€§

åˆ›å»º: 2025-12-04 | ç‰ˆæœ¬: v1.0
"""

import time
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Union
import warnings
warnings.filterwarnings('ignore')

from .base_model import BaseModel

# å°è¯•å¯¼å…¥ LightGBM
try:
    import lightgbm as lgb
    HAS_LIGHTGBM = True
except ImportError:
    HAS_LIGHTGBM = False
    print("âš ï¸ LightGBM æœªå®‰è£…ï¼Œæ’åºæ¨¡å‹å°†ä¸å¯ç”¨")


class LightGBMRanker(BaseModel):
    """
    LightGBM æ’åºæ¨¡å‹
    
    ç‰¹ç‚¹ï¼š
    - ä½¿ç”¨ LambdaRank ç›®æ ‡å‡½æ•°ï¼Œç›´æ¥ä¼˜åŒ–æ’åºè´¨é‡
    - æ”¯æŒ group åˆ†ç»„ï¼Œé€‚åˆæ¨ªæˆªé¢è‚¡ç¥¨æ’åº
    - å‚æ•°é¢„è®¾åå‘ç¨³å®šï¼Œå‡å°‘è¿‡æ‹Ÿåˆé£é™©
    - è¯„ä¼°æŒ‡æ ‡ï¼šNDCG@K
    
    ä½¿ç”¨è¦æ±‚ï¼š
    - è®­ç»ƒæ•°æ®å¿…é¡»æŒ‰æ—¥æœŸï¼ˆgroupï¼‰æ’åº
    - æ ‡ç­¾å¿…é¡»æ˜¯ç¦»æ•£æ•´æ•°ï¼ˆ0, 1, 2, ... n_bins-1ï¼‰
    - å¿…é¡»æä¾› group å‘é‡
    """
    
    def __init__(self, params: Optional[Dict] = None):
        """
        åˆå§‹åŒ– LightGBM æ’åºæ¨¡å‹
        
        Parameters:
        -----------
        params : dict, optional
            æ¨¡å‹å‚æ•°ï¼Œæ”¯æŒçš„å…³é”®å‚æ•°ï¼š
            - objective: æ’åºç›®æ ‡ï¼ˆé»˜è®¤ 'lambdarank'ï¼‰
            - metric: è¯„ä¼°æŒ‡æ ‡ï¼ˆé»˜è®¤ 'ndcg'ï¼‰
            - ndcg_eval_at: NDCG@K çš„ K å€¼åˆ—è¡¨
            - label_gain: å„ç­‰çº§çš„å¢ç›Šæƒé‡
            - n_estimators: è¿­ä»£æ¬¡æ•°
            - learning_rate: å­¦ä¹ ç‡
            - num_leaves: å¶å­æ•°
            - max_depth: æœ€å¤§æ·±åº¦
            - feature_fraction: ç‰¹å¾é‡‡æ ·æ¯”ä¾‹
            - bagging_fraction: æ ·æœ¬é‡‡æ ·æ¯”ä¾‹
            - min_data_in_leaf: å¶å­æœ€å°æ ·æœ¬æ•°
            - lambda_l1/l2: L1/L2 æ­£åˆ™åŒ–
        """
        if not HAS_LIGHTGBM:
            raise ImportError("LightGBM æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install lightgbm")
        
        super().__init__(name='LightGBMRanker', params=params)
        
        # é»˜è®¤å‚æ•°ï¼ˆåå‘ç¨³å®šï¼‰
        default_params = {
            # æ’åºç›®æ ‡
            'objective': 'lambdarank',
            'metric': 'ndcg',
            'ndcg_eval_at': [10, 30, 50],  # å…³æ³¨å¤´éƒ¨
            'label_gain': None,  # è‡ªåŠ¨è®¡ç®—ï¼š[0, 1, 3, 7, 15, ...]
            
            # æ ‘ç»“æ„ï¼ˆä¿å®ˆè®¾ç½®ï¼‰
            'n_estimators': 500,
            'learning_rate': 0.05,
            'num_leaves': 31,
            'max_depth': 6,  # æ¯”å›å½’æ›´æµ…
            'min_data_in_leaf': 50,  # æ¯”å›å½’æ›´å¤§
            
            # é‡‡æ ·ï¼ˆå¼ºåˆ¶æ­£åˆ™åŒ–ï¼‰
            'feature_fraction': 0.7,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            
            # æ­£åˆ™åŒ–
            'lambda_l1': 0.1,
            'lambda_l2': 0.1,
            
            # å…¶ä»–
            'random_state': 42,
            'verbose': -1,
            'force_row_wise': True  # æ’åºæ¨¡å‹æ¨è
        }
        
        # ç”¨æˆ·å‚æ•°è¦†ç›–é»˜è®¤å€¼
        default_params.update(self.params)
        self.params = default_params
        
        # è®°å½• group ä¿¡æ¯
        self._train_groups = None
        self._valid_groups = None
    
    def fit(self,
            X: pd.DataFrame,
            y: pd.Series,
            X_valid: Optional[pd.DataFrame] = None,
            y_valid: Optional[pd.Series] = None,
            groups: Optional[List[int]] = None,
            valid_groups: Optional[List[int]] = None,
            early_stopping_rounds: int = 50) -> Dict:
        """
        è®­ç»ƒ LightGBM æ’åºæ¨¡å‹
        
        Parameters:
        -----------
        X : pd.DataFrame
            è®­ç»ƒç‰¹å¾ï¼ˆå¿…é¡»æŒ‰æ—¥æœŸæ’åºï¼‰
        y : pd.Series
            è®­ç»ƒæ ‡ç­¾ï¼ˆç¦»æ•£æ•´æ•° 0 ~ n_bins-1ï¼‰
        X_valid : pd.DataFrame, optional
            éªŒè¯ç‰¹å¾ï¼ˆç”¨äºæ—©åœï¼‰
        y_valid : pd.Series, optional
            éªŒè¯æ ‡ç­¾
        groups : List[int]
            è®­ç»ƒé›† group å‘é‡ï¼Œgroups[i] = ç¬¬ i ä¸ªæ—¥æœŸçš„æ ·æœ¬æ•°
        valid_groups : List[int], optional
            éªŒè¯é›† group å‘é‡
        early_stopping_rounds : int
            æ—©åœè½®æ•°
            
        Returns:
        --------
        dict
            è®­ç»ƒç»“æœ
        """
        if not HAS_LIGHTGBM:
            raise ImportError("LightGBM æœªå®‰è£…")
        
        # æ ¡éªŒ group
        if groups is None:
            raise ValueError("æ’åºæ¨¡å‹å¿…é¡»æä¾› groups å‚æ•°")
        
        if sum(groups) != len(X):
            raise ValueError(f"groups æ€»å’Œ ({sum(groups)}) ä¸æ ·æœ¬æ•° ({len(X)}) ä¸åŒ¹é…")
        
        print(f"   ğŸ¯ è®­ç»ƒ {self.name} æ¨¡å‹...")
        print(f"      ğŸ“Š è¿­ä»£æ¬¡æ•°: {self.params['n_estimators']}, å­¦ä¹ ç‡: {self.params['learning_rate']}")
        print(f"      ğŸ“Š æ—¥æœŸæ•°(groups): {len(groups)}, æ ·æœ¬æ•°: {len(X)}")
        print(f"      ğŸ“Š æ ‡ç­¾åˆ†å¸ƒ: {dict(y.value_counts().sort_index())}")
        
        start_time = time.time()
        
        # ä¿å­˜ç‰¹å¾åç§°
        self.feature_names = list(X.columns)
        self._train_groups = groups
        
        # å‡†å¤‡å‚æ•°ï¼ˆç§»é™¤é LightGBM åŸç”Ÿå‚æ•°ï¼‰
        train_params = self.params.copy()
        n_estimators = train_params.pop('n_estimators', 500)
        
        # è‡ªåŠ¨è®¡ç®— label_gainï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
        if train_params.get('label_gain') is None:
            n_labels = int(y.max()) + 1
            # æŒ‡æ•°å¢ç›Šï¼šç­‰çº§è¶Šé«˜ï¼Œå¢ç›Šè¶Šå¤§
            train_params['label_gain'] = [2**i - 1 for i in range(n_labels)]
            print(f"      ğŸ“Š è‡ªåŠ¨ label_gain: {train_params['label_gain']}")
        
        # åˆ›å»ºè®­ç»ƒæ•°æ®é›†
        train_data = lgb.Dataset(
            X.values if isinstance(X, pd.DataFrame) else X,
            label=y.values if isinstance(y, pd.Series) else y,
            group=groups,
            feature_name=self.feature_names
        )
        
        # å‡†å¤‡éªŒè¯é›†
        valid_sets = [train_data]
        valid_names = ['train']
        
        callbacks = []
        
        if X_valid is not None and y_valid is not None:
            if valid_groups is None:
                raise ValueError("æä¾›éªŒè¯é›†æ—¶å¿…é¡»åŒæ—¶æä¾› valid_groups")
            
            if sum(valid_groups) != len(X_valid):
                raise ValueError(f"valid_groups æ€»å’Œ ({sum(valid_groups)}) ä¸éªŒè¯æ ·æœ¬æ•° ({len(X_valid)}) ä¸åŒ¹é…")
            
            self._valid_groups = valid_groups
            
            valid_data = lgb.Dataset(
                X_valid.values if isinstance(X_valid, pd.DataFrame) else X_valid,
                label=y_valid.values if isinstance(y_valid, pd.Series) else y_valid,
                group=valid_groups,
                reference=train_data
            )
            valid_sets.append(valid_data)
            valid_names.append('valid')
            
            # æ—©åœ
            callbacks.append(lgb.early_stopping(stopping_rounds=early_stopping_rounds, verbose=False))
            callbacks.append(lgb.log_evaluation(period=100))  # æ¯ 100 è½®æ‰“å°
            
            print(f"      ğŸ“Š éªŒè¯é›†: {len(X_valid)} æ ·æœ¬, {len(valid_groups)} æ—¥æœŸ")
        
        # è®­ç»ƒæ¨¡å‹
        self.model = lgb.train(
            train_params,
            train_data,
            num_boost_round=n_estimators,
            valid_sets=valid_sets,
            valid_names=valid_names,
            callbacks=callbacks
        )
        
        self.is_fitted = True
        
        training_time = time.time() - start_time
        
        # æ„å»ºç»“æœ
        results = {
            'model_name': self.name,
            'training_time': training_time,
            'train_samples': len(X),
            'train_groups': len(groups),
            'n_features': len(self.feature_names),
            'n_estimators': self.model.num_trees(),
            'best_iteration': self.model.best_iteration
        }
        
        # éªŒè¯é›†ç»“æœ
        if X_valid is not None and y_valid is not None:
            results['valid_samples'] = len(X_valid)
            results['valid_groups'] = len(valid_groups)
            
            # è·å–æœ€ä½³ NDCG
            if hasattr(self.model, 'best_score') and self.model.best_score:
                best_scores = self.model.best_score.get('valid', {})
                for metric_name, score in best_scores.items():
                    results[f'valid_{metric_name}'] = score
                    print(f"      ğŸ“Š éªŒè¯é›† {metric_name}: {score:.6f}")
            
            print(f"      ğŸ¯ æœ€ä½³è¿­ä»£: {self.model.best_iteration}")
        
        print(f"      â±ï¸  è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
        
        return results
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """
        é¢„æµ‹æ’åºåˆ†æ•°
        
        Parameters:
        -----------
        X : pd.DataFrame
            ç‰¹å¾æ•°æ®
            
        Returns:
        --------
        np.ndarray
            é¢„æµ‹åˆ†æ•°ï¼ˆè¶Šé«˜è¡¨ç¤ºé¢„æœŸæ’åè¶Šé å‰ï¼‰
        """
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œæ— æ³•é¢„æµ‹")
        
        # ç¡®ä¿ç‰¹å¾é¡ºåºä¸€è‡´
        if self.feature_names is not None:
            if isinstance(X, pd.DataFrame):
                X = X[self.feature_names]
        
        return self.model.predict(
            X.values if isinstance(X, pd.DataFrame) else X
        )
    
    def get_feature_importance(self, importance_type: str = 'gain') -> Optional[pd.Series]:
        """
        è·å–ç‰¹å¾é‡è¦æ€§
        
        Parameters:
        -----------
        importance_type : str
            é‡è¦æ€§ç±»å‹: 'gain' æˆ– 'split'
            
        Returns:
        --------
        pd.Series
            ç‰¹å¾é‡è¦æ€§
        """
        if not self.is_fitted:
            return None
        
        importance = self.model.feature_importance(importance_type=importance_type)
        
        return pd.Series(importance, index=self.feature_names).sort_values(ascending=False)
    
    def get_group_info(self) -> Dict:
        """
        è·å– group ä¿¡æ¯
        
        Returns:
        --------
        dict
            åŒ…å« train_groups, valid_groups ç­‰ä¿¡æ¯
        """
        return {
            'train_groups': self._train_groups,
            'valid_groups': self._valid_groups,
            'train_n_groups': len(self._train_groups) if self._train_groups else 0,
            'valid_n_groups': len(self._valid_groups) if self._valid_groups else 0
        }


# ==================== ä¾¿æ·å‡½æ•° ====================

def prepare_ranking_data(features: pd.DataFrame,
                         labels: pd.Series,
                         groups: Optional[List[int]] = None) -> tuple:
    """
    å‡†å¤‡æ’åºæ¨¡å‹è®­ç»ƒæ•°æ®
    
    ç¡®ä¿ï¼š
    1. æ•°æ®æŒ‰æ—¥æœŸæ’åº
    2. ç‰¹å¾ä¸æ ‡ç­¾å¯¹é½
    3. å¦‚æœæœªæä¾› groupsï¼Œè‡ªåŠ¨è®¡ç®—
    
    Parameters:
    -----------
    features : pd.DataFrame
        ç‰¹å¾ï¼ŒMultiIndex [date, ticker]
    labels : pd.Series
        æ ‡ç­¾ï¼ŒMultiIndex [date, ticker]
    groups : List[int], optional
        group å‘é‡
        
    Returns:
    --------
    tuple
        (X, y, groups)
    """
    # å¯¹é½
    common_idx = features.index.intersection(labels.index)
    X = features.loc[common_idx].sort_index(level='date')
    y = labels.loc[common_idx].sort_index(level='date')
    
    # è®¡ç®— groups
    if groups is None:
        groups = X.groupby(level='date').size().tolist()
    
    return X, y, groups


# ==================== æµ‹è¯• ====================

if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯• LightGBM æ’åºæ¨¡å‹")
    print("=" * 60)
    
    if not HAS_LIGHTGBM:
        print("âŒ LightGBM æœªå®‰è£…ï¼Œè·³è¿‡æµ‹è¯•")
        print("   å®‰è£…å‘½ä»¤: pip install lightgbm")
    else:
        # æ„é€ æ¨¡æ‹Ÿæ•°æ®
        np.random.seed(42)
        
        n_dates = 20
        n_stocks = 50
        n_features = 10
        n_bins = 5
        
        # åˆ›å»º MultiIndex
        dates = pd.date_range('2023-01-01', periods=n_dates, freq='D')
        tickers = [f'{i:06d}' for i in range(1, n_stocks + 1)]
        index = pd.MultiIndex.from_product([dates, tickers], names=['date', 'ticker'])
        
        # æ¨¡æ‹Ÿç‰¹å¾
        X = pd.DataFrame(
            np.random.randn(len(index), n_features),
            columns=[f'feature_{i}' for i in range(n_features)],
            index=index
        )
        
        # æ¨¡æ‹Ÿæ ‡ç­¾ï¼ˆç¦»æ•£ç­‰çº§ 0-4ï¼‰
        y = pd.Series(
            np.random.randint(0, n_bins, len(index)),
            index=index,
            name='label'
        )
        
        # æŒ‰æ—¥æœŸæ’åº
        X = X.sort_index(level='date')
        y = y.sort_index(level='date')
        
        # è®¡ç®— groups
        groups = X.groupby(level='date').size().tolist()
        
        print(f"æ¨¡æ‹Ÿæ•°æ®: {len(index)} æ ·æœ¬, {n_dates} æ—¥æœŸ, {n_stocks} è‚¡ç¥¨")
        print(f"Groups: {groups[:5]}... (å…± {len(groups)} ä¸ª)")
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆæŒ‰æ—¥æœŸï¼‰
        split_date = dates[int(n_dates * 0.7)]
        train_mask = X.index.get_level_values('date') < split_date
        valid_mask = ~train_mask
        
        X_train, y_train = X[train_mask], y[train_mask]
        X_valid, y_valid = X[valid_mask], y[valid_mask]
        
        train_groups = X_train.groupby(level='date').size().tolist()
        valid_groups = X_valid.groupby(level='date').size().tolist()
        
        print(f"\nè®­ç»ƒé›†: {len(X_train)} æ ·æœ¬, {len(train_groups)} æ—¥æœŸ")
        print(f"éªŒè¯é›†: {len(X_valid)} æ ·æœ¬, {len(valid_groups)} æ—¥æœŸ")
        
        # è®­ç»ƒæ¨¡å‹
        print("\nğŸ’¡ è®­ç»ƒ LightGBM æ’åºæ¨¡å‹")
        model = LightGBMRanker(params={
            'n_estimators': 100,
            'learning_rate': 0.1,
            'num_leaves': 15
        })
        
        results = model.fit(
            X_train, y_train,
            X_valid, y_valid,
            groups=train_groups,
            valid_groups=valid_groups
        )
        
        print(f"\nâœ… è®­ç»ƒç»“æœ: {results}")
        
        # æµ‹è¯•é¢„æµ‹
        print("\nğŸ¯ æµ‹è¯•é¢„æµ‹")
        scores = model.predict(X_valid)
        print(f"é¢„æµ‹å½¢çŠ¶: {scores.shape}")
        print(f"é¢„æµ‹åˆ†æ•°èŒƒå›´: [{scores.min():.4f}, {scores.max():.4f}]")
        
        # æµ‹è¯•ç‰¹å¾é‡è¦æ€§
        print("\nğŸ“Š ç‰¹å¾é‡è¦æ€§ (gain)")
        importance = model.get_feature_importance('gain')
        print(f"Top 5:\n{importance.head()}")
        
        # Group ä¿¡æ¯
        print("\nğŸ“Š Group ä¿¡æ¯")
        print(model.get_group_info())
        
        print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡")
