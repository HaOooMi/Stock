#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
èšç±»ä¸çŠ¶æ€æ”¶ç›Šè¯„ä¼°æ¨¡å—

åŠŸèƒ½ï¼š
1. åŸºäºPCAçŠ¶æ€è¿›è¡ŒKMeansèšç±»åˆ†æ
2. è¯„ä¼°ä¸åŒèšç±»çš„æœªæ¥æ”¶ç›Šè¡¨ç°
3. ç”Ÿæˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„èšç±»æ€§èƒ½æŠ¥å‘Š
4. éªŒè¯èšç±»æ•ˆæœçš„æ˜¾è‘—æ€§


"""

import os
import sys
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import pickle
import warnings
warnings.filterwarnings('ignore')

# æœºå™¨å­¦ä¹ åº“
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, calinski_harabasz_score

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
# evaluation/cluster/ -> evaluation/ -> machine learning/
ml_root = os.path.dirname(os.path.dirname(current_dir))
project_root = os.path.dirname(ml_root)
if ml_root not in sys.path:
    sys.path.insert(0, ml_root)


class ClusterEvaluator:
    """
    èšç±»æ”¶ç›Šè¯„ä¼°å™¨
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. å¯¹PCAçŠ¶æ€è¿›è¡ŒKMeansèšç±»
    2. åˆ†ææ¯ä¸ªèšç±»çš„æœªæ¥æ”¶ç›Šè¡¨ç°
    3. ç”Ÿæˆèšç±»æ€§èƒ½æŠ¥å‘Š
    4. éªŒè¯èšç±»çš„é¢„æµ‹èƒ½åŠ›
    """
    
    def __init__(self, reports_dir: str = "ML output/reports"):
        """
        åˆå§‹åŒ–èšç±»è¯„ä¼°å™¨
        
        Parameters:
        -----------
        reports_dir : str
            æŠ¥å‘Šä¿å­˜ç›®å½•
        """
        # è®¾ç½®æŠ¥å‘Šç›®å½•
        if os.path.isabs(reports_dir):
            self.reports_dir = reports_dir
        else:
            self.reports_dir = os.path.join(ml_root, reports_dir)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.reports_dir, exist_ok=True)
        
        # èšç±»é…ç½®
        self.k_values = [4, 5, 6]
        self.random_state = 42
        
        # å­˜å‚¨èšç±»ç»“æœ
        self.cluster_models = {}
        self.train_results = {}
        self.test_results = {}

    def load_pca_states_and_targets(self, states_train_path: str, states_test_path: str,
                                   targets_path: str) -> Tuple[np.ndarray, np.ndarray, pd.DataFrame]:
        """
        åŠ è½½PCAçŠ¶æ€å’Œç›®æ ‡æ”¶ç›Šæ•°æ®
        
        Parameters:
        -----------
        states_train_path : str
            è®­ç»ƒé›†PCAçŠ¶æ€æ–‡ä»¶è·¯å¾„
        states_test_path : str
            æµ‹è¯•é›†PCAçŠ¶æ€æ–‡ä»¶è·¯å¾„
        targets_path : str
            ç›®æ ‡æ”¶ç›Šæ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
        --------
        tuple
            (è®­ç»ƒçŠ¶æ€, æµ‹è¯•çŠ¶æ€, ç›®æ ‡æ•°æ®)
        """
        print("Loading PCA states and targets...")
        
        # åŠ è½½PCAçŠ¶æ€
        states_train = np.load(states_train_path)
        states_test = np.load(states_test_path)
        
        # åŠ è½½ç›®æ ‡æ”¶ç›Šæ•°æ®
        targets_df = pd.read_csv(targets_path, index_col=0, parse_dates=True)
        
        print(f"Training states: {states_train.shape}")
        print(f"Test states: {states_test.shape}")
        print(f"Targets: {targets_df.shape}")
        
        return states_train, states_test, targets_df



    def perform_kmeans_clustering(self, states_train: np.ndarray, k: int) -> KMeans:
        """
        æ‰§è¡ŒKMeansèšç±»
        
        Parameters:
        -----------
        states_train : np.ndarray
            è®­ç»ƒé›†PCAçŠ¶æ€
        k : int
            èšç±»æ•°é‡
            
        Returns:
        --------
        KMeans
            è®­ç»ƒå¥½çš„KMeansæ¨¡å‹
        """
        kmeans = KMeans(
            n_clusters=k,
            random_state=self.random_state,
            n_init=20,
            max_iter=500
        )
        
        kmeans.fit(states_train)
        
        print(f"K={k} clustering completed")
        
        return kmeans

    def evaluate_cluster_returns(self, states: np.ndarray, targets_df: pd.DataFrame,
                                kmeans: KMeans, phase: str = "train") -> pd.DataFrame:
        """
        è¯„ä¼°èšç±»çš„æ”¶ç›Šè¡¨ç°
        
        Parameters:
        -----------
        states : np.ndarray
            PCAçŠ¶æ€æ•°æ®
        targets_df : pd.DataFrame
            ç›®æ ‡æ”¶ç›Šæ•°æ®
        kmeans : KMeans
            èšç±»æ¨¡å‹
        phase : str
            æ•°æ®é˜¶æ®µ ("train" æˆ– "test")
            
        Returns:
        --------
        pd.DataFrame
            èšç±»æ€§èƒ½è¯„ä¼°ç»“æœ
        """
        # è·å–èšç±»æ ‡ç­¾
        cluster_labels = kmeans.predict(states)
        
        # ç¡®ä¿ç´¢å¼•å¯¹é½
        if phase == "train":
            # è®­ç»ƒé›†ä½¿ç”¨å‰70%çš„ç›®æ ‡æ•°æ®
            n_train = len(states)
            target_data = targets_df.iloc[:n_train]
        else:
            # æµ‹è¯•é›†ä½¿ç”¨å30%çš„ç›®æ ‡æ•°æ®
            n_train = len(states)
            target_data = targets_df.iloc[-n_train:]
        
        # é‡ç½®ç´¢å¼•ç¡®ä¿å¯¹é½
        if len(target_data) != len(cluster_labels):
            print(f"Warning: Length mismatch - targets: {len(target_data)}, clusters: {len(cluster_labels)}")
            min_len = min(len(target_data), len(cluster_labels))
            target_data = target_data.iloc[:min_len]
            cluster_labels = cluster_labels[:min_len]
        
        # è®¡ç®—æ¯ä¸ªèšç±»çš„æœªæ¥æ”¶ç›Šç»Ÿè®¡
        results = []
        
        for cluster_id in range(kmeans.n_clusters):
            mask = cluster_labels == cluster_id
            cluster_returns = target_data.loc[mask, 'future_return_5d']
            cluster_states = states[mask]
            
            if len(cluster_returns) > 0 and len(cluster_states) > 0:
                # åŸºæœ¬æ”¶ç›Šç»Ÿè®¡
                stats = {
                    'cluster_id': cluster_id,
                    'count': len(cluster_returns),
                    'mean_return': cluster_returns.mean(),
                    'std_return': cluster_returns.std(),
                    'median_return': cluster_returns.median(),
                    'min_return': cluster_returns.min(),
                    'max_return': cluster_returns.max(),
                    'positive_ratio': (cluster_returns > 0).mean(),
                    'phase': phase,
                    'k_value': kmeans.n_clusters
                }
                
                # æ·»åŠ ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯
                for pc_idx in range(cluster_states.shape[1]):
                    pc_values = cluster_states[:, pc_idx]
                    stats[f'PC{pc_idx+1}_mean'] = pc_values.mean()
                    stats[f'PC{pc_idx+1}_std'] = pc_values.std()
                    stats[f'PC{pc_idx+1}_min'] = pc_values.min()
                    stats[f'PC{pc_idx+1}_max'] = pc_values.max()
                
                results.append(stats)
        
        results_df = pd.DataFrame(results)
        
        # æŒ‰å¹³å‡æ”¶ç›Šæ’åº
        results_df = results_df.sort_values('mean_return', ascending=False)
        results_df['rank'] = range(1, len(results_df) + 1)
        
        return results_df

    def validate_cluster_performance(self, train_results: pd.DataFrame, 
                                   test_results: pd.DataFrame,
                                   global_std: float) -> Dict:
        """
        éªŒè¯èšç±»æ€§èƒ½
        
        Parameters:
        -----------
        train_results : pd.DataFrame
            è®­ç»ƒé›†èšç±»ç»“æœ
        test_results : pd.DataFrame
            æµ‹è¯•é›†èšç±»ç»“æœ
        global_std : float
            å…¨å±€æ”¶ç›Šæ ‡å‡†å·®
            
        Returns:
        --------
        dict
            éªŒè¯ç»“æœ
        """
        validation = {}
        
        # è®­ç»ƒé›†éªŒè¯ï¼šæœ€ä½³vsæœ€å·®èšç±»å·®å¼‚ > å…¨å±€std * 0.4
        best_train = train_results.iloc[0]['mean_return']
        worst_train = train_results.iloc[-1]['mean_return']
        train_diff = best_train - worst_train
        threshold = global_std * 0.4
        
        validation['train_best_return'] = best_train
        validation['train_worst_return'] = worst_train
        validation['train_difference'] = train_diff
        validation['threshold'] = threshold
        validation['train_significant'] = train_diff > threshold
        
        # æµ‹è¯•é›†éªŒè¯ï¼šæœ€ä½³èšç±»ä»åœ¨å‰50%
        best_train_cluster = train_results.iloc[0]['cluster_id']
        test_cluster_match = test_results[test_results['cluster_id'] == best_train_cluster]
        
        if len(test_cluster_match) > 0:
            test_best_rank = test_cluster_match['rank'].iloc[0]
            total_clusters = len(test_results)
            validation['test_best_cluster_rank'] = test_best_rank
            validation['total_clusters'] = total_clusters
            validation['test_top_50_percent'] = test_best_rank <= (total_clusters * 0.5)
        else:
            # è®­ç»ƒé›†æœ€ä½³ç°‡åœ¨æµ‹è¯•é›†ä¸­ä¸å­˜åœ¨
            print(f"   âš ï¸ è­¦å‘Š: è®­ç»ƒé›†æœ€ä½³ç°‡ {best_train_cluster} åœ¨æµ‹è¯•é›†ä¸­æ— æ ·æœ¬")
            validation['test_best_cluster_rank'] = None
            validation['total_clusters'] = len(test_results)
            validation['test_top_50_percent'] = False
        
        return validation
    
    
    def generate_comprehensive_report(self, all_train_results: List, all_test_results: List, 
                                    validations: Dict, global_std: float):
        """
        ç”Ÿæˆç»¼åˆèšç±»åˆ†ææŠ¥å‘Š - åˆå¹¶æ‰€æœ‰æŠ¥å‘Šç”ŸæˆåŠŸèƒ½
        
        Parameters:
        -----------
        all_train_results : List
            æ‰€æœ‰kå€¼çš„è®­ç»ƒç»“æœ
        all_test_results : List
            æ‰€æœ‰kå€¼çš„æµ‹è¯•ç»“æœ
        validations : dict
            æ‰€æœ‰kå€¼çš„éªŒè¯ç»“æœ
        global_std : float
            å…¨å±€æ ‡å‡†å·®
        """
        print("\nğŸ“Š Generating comprehensive clustering reports...")
        
        # === 1. æ€»ç»“æŠ¥å‘Šæ•°æ® ===
        summary = {
            'global_std': global_std,
            'threshold': global_std * 0.4,
            'total_k_values': len(self.k_values),
            'passed_train_validation': 0,
            'passed_test_validation': 0,
            'passed_both_validation': 0,
            'best_k': None,
            'best_performance': None
        }
        
        best_score = -1
        
        for k, validation in validations.items():
            # ç»Ÿè®¡é€šè¿‡éªŒæ”¶çš„kå€¼
            if validation['train_significant']:
                summary['passed_train_validation'] += 1
            
            if validation['test_top_50_percent']:
                summary['passed_test_validation'] += 1
            
            if validation['train_significant'] and validation['test_top_50_percent']:
                summary['passed_both_validation'] += 1
                
                # è®¡ç®—ç»¼åˆåˆ†æ•° (è®­ç»ƒå·®å¼‚ + æµ‹è¯•æ’åæƒé‡)
                score = validation['train_difference'] - (validation['test_best_cluster_rank'] - 1) * 0.01
                if score > best_score:
                    best_score = score
                    summary['best_k'] = k
                    summary['best_performance'] = validation
        
        # è®¡ç®—æˆåŠŸç‡
        summary['train_success_rate'] = summary['passed_train_validation'] / summary['total_k_values']
        summary['test_success_rate'] = summary['passed_test_validation'] / summary['total_k_values']
        summary['overall_success_rate'] = summary['passed_both_validation'] / summary['total_k_values']
        
        # === 2. ç”Ÿæˆä¸»æŠ¥å‘Šæ–‡ä»¶ ===
        main_report_lines = []
        main_report_lines.append("K-Meansèšç±»åˆ†æç»¼åˆæŠ¥å‘Š")
        main_report_lines.append("=" * 60)
        main_report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        main_report_lines.append(f"å…¨å±€æ”¶ç›Šæ ‡å‡†å·®: {global_std:.6f}")
        main_report_lines.append(f"è®­ç»ƒæ˜¾è‘—æ€§é˜ˆå€¼: {global_std * 0.4:.6f}")
        main_report_lines.append("")
        
        # éªŒè¯ç»Ÿè®¡
        main_report_lines.append("ï¿½ éªŒè¯ç»Ÿè®¡:")
        main_report_lines.append(f"  æµ‹è¯•kå€¼æ€»æ•°: {summary['total_k_values']}")
        main_report_lines.append(f"  è®­ç»ƒæ˜¾è‘—æ€§é€šè¿‡: {summary['passed_train_validation']}/{summary['total_k_values']} ({summary['train_success_rate']:.1%})")
        main_report_lines.append(f"  æµ‹è¯•å‰50%é€šè¿‡: {summary['passed_test_validation']}/{summary['total_k_values']} ({summary['test_success_rate']:.1%})")
        main_report_lines.append(f"  åŒé‡éªŒè¯é€šè¿‡: {summary['passed_both_validation']}/{summary['total_k_values']} ({summary['overall_success_rate']:.1%})")
        
        if summary['best_k']:
            main_report_lines.append(f"\nğŸ† æœ€ä½³kå€¼: {summary['best_k']}")
            best_perf = summary['best_performance']
            main_report_lines.append(f"  è®­ç»ƒå·®å¼‚: {best_perf['train_difference']:+.6f}")
            main_report_lines.append(f"  æµ‹è¯•æœ€ä½³æ’å: {best_perf['test_best_cluster_rank']}")
        
        main_report_lines.append("\n" + "=" * 60)
        
        # === 3. ä¸ºæ¯ä¸ªkå€¼ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š ===
        all_summary_data = []
        
        for k in self.k_values:
            validation = validations[k]
            
            # è·å–è¯¥kå€¼çš„ç»“æœ
            train_k = pd.concat([df for df in all_train_results if df['k_value'].iloc[0] == k], ignore_index=True)
            test_k = pd.concat([df for df in all_test_results if df['k_value'].iloc[0] == k], ignore_index=True)
            kmeans = self.cluster_models[k]
            
            # æ·»åŠ åˆ°ä¸»æŠ¥å‘Š
            main_report_lines.append(f"\nğŸ” K={k} è¯¦ç»†åˆ†æ {'âœ…' if validation['train_significant'] and validation['test_top_50_percent'] else 'âŒ'}")
            main_report_lines.append("-" * 40)
            main_report_lines.append(f"éªŒè¯: è®­ç»ƒæ˜¾è‘—æ€§={validation['train_significant']}, æµ‹è¯•å‰50%={validation['test_top_50_percent']}")
            
            # åªåœ¨æœ‰è´¨é‡æŒ‡æ ‡æ—¶æ˜¾ç¤º
            if 'silhouette_score' in validation and 'calinski_score' in validation:
                main_report_lines.append(f"è´¨é‡åˆ†æ•°: Silhouette={validation['silhouette_score']:.4f}, Calinski-Harabasz={validation['calinski_score']:.2f}")
            else:
                main_report_lines.append("è´¨é‡åˆ†æ•°: æœªè®¡ç®—")
            
            # è¯¦ç»†èšç±»ä¿¡æ¯
            detailed_csv_data = []
            
            for cluster_id in range(k):
                train_row = train_k[train_k['cluster_id'] == cluster_id]
                test_row = test_k[test_k['cluster_id'] == cluster_id]
                
                if len(train_row) > 0 and len(test_row) > 0:
                    train_data = train_row.iloc[0]
                    test_data = test_row.iloc[0]
                    cluster_center = kmeans.cluster_centers_[cluster_id]
                    
                    # æ·»åŠ åˆ°ä¸»æŠ¥å‘Š
                    main_report_lines.append(f"  èšç±»{cluster_id}: è®­ç»ƒæ”¶ç›Š={train_data['mean_return']:+.6f}(æ’å{train_data['rank']}), æµ‹è¯•æ”¶ç›Š={test_data['mean_return']:+.6f}(æ’å{test_data['rank']})")
                    
                    # å‡†å¤‡CSVæ•°æ®
                    row_data = {
                        'k_value': k,
                        'cluster_id': cluster_id,
                        'train_mean_return': train_data['mean_return'],
                        'test_mean_return': test_data['mean_return'],
                        'train_rank': train_data['rank'],
                        'test_rank': test_data['rank'],
                        'train_samples': train_data['count'],
                        'test_samples': test_data['count'],
                        'train_positive_ratio': train_data['positive_ratio'],
                        'test_positive_ratio': test_data['positive_ratio']
                    }
                    
                    # æ·»åŠ èšç±»ä¸­å¿ƒç‰¹å¾
                    for i, center_val in enumerate(cluster_center):
                        row_data[f'center_PC{i+1}'] = center_val
                    
                    # æ·»åŠ è®­ç»ƒé›†ç‰¹å¾ç»Ÿè®¡
                    feature_cols = [col for col in train_data.index if col.startswith('PC')]
                    for col in feature_cols:
                        row_data[f'train_{col}'] = train_data[col]
                    
                    detailed_csv_data.append(row_data)
                elif len(train_row) > 0:
                    # åªåœ¨è®­ç»ƒé›†ä¸­å­˜åœ¨
                    train_data = train_row.iloc[0]
                    main_report_lines.append(f"  èšç±»{cluster_id}: è®­ç»ƒæ”¶ç›Š={train_data['mean_return']:+.6f}(æ’å{train_data['rank']}), æµ‹è¯•é›†æ— æ ·æœ¬")
                elif len(test_row) > 0:
                    # åªåœ¨æµ‹è¯•é›†ä¸­å­˜åœ¨
                    test_data = test_row.iloc[0]
                    main_report_lines.append(f"  èšç±»{cluster_id}: è®­ç»ƒé›†æ— æ ·æœ¬, æµ‹è¯•æ”¶ç›Š={test_data['mean_return']:+.6f}(æ’å{test_data['rank']})")
                else:
                    # è®­ç»ƒé›†å’Œæµ‹è¯•é›†éƒ½æ²¡æœ‰æ ·æœ¬ï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰
                    main_report_lines.append(f"  èšç±»{cluster_id}: è®­ç»ƒé›†å’Œæµ‹è¯•é›†éƒ½æ— æ ·æœ¬")
            
            # ä¿å­˜å•ç‹¬çš„kå€¼è¯¦ç»†CSV
            if detailed_csv_data:
                detailed_csv = pd.DataFrame(detailed_csv_data)
                csv_file = os.path.join(self.reports_dir, f"cluster_features_k{k}.csv")
                detailed_csv.to_csv(csv_file, index=False)
                
                # æ·»åŠ åˆ°æ±‡æ€»æ•°æ®
                all_summary_data.extend(detailed_csv_data)
        
        # === 4. ä¿å­˜ä¸»æŠ¥å‘Šæ–‡ä»¶ ===
        main_report_file = os.path.join(self.reports_dir, "clustering_analysis_report.txt")
        with open(main_report_file, 'w', encoding='utf-8') as f:
            f.write("\n".join(main_report_lines))
        
        # === 5. ç”Ÿæˆèšç±»æ¯”è¾ƒè¡¨ ===
        comparison_data = []
        for k in self.k_values:
            train_k = pd.concat([df for df in all_train_results if df['k_value'].iloc[0] == k], ignore_index=True)
            test_k = pd.concat([df for df in all_test_results if df['k_value'].iloc[0] == k], ignore_index=True)
            validation = validations[k]
            
            for cluster_id in range(k):
                train_row = train_k[train_k['cluster_id'] == cluster_id]
                test_row = test_k[test_k['cluster_id'] == cluster_id]
                
                if len(train_row) > 0 and len(test_row) > 0:
                    train_data = train_row.iloc[0]
                    test_data = test_row.iloc[0]
                    
                    comparison_data.append({
                        'k_value': k,
                        'cluster_id': cluster_id,
                        'train_samples': train_data['count'],
                        'test_samples': test_data['count'],
                        'train_mean_return': train_data['mean_return'],
                        'test_mean_return': test_data['mean_return'],
                        'train_rank': train_data['rank'],
                        'test_rank': test_data['rank'],
                        'overall_return': train_data['mean_return'] + test_data['mean_return'],
                        'validation_passed': validation['train_significant'] and validation['test_top_50_percent'],
                        'is_best_in_k': train_data['rank'] == 1,
                        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    })
                elif len(train_row) > 0:
                    # åªåœ¨è®­ç»ƒé›†ä¸­å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œæ ·æœ¬å¤–éªŒè¯
                    train_data = train_row.iloc[0]
                    comparison_data.append({
                        'k_value': k,
                        'cluster_id': cluster_id,
                        'train_samples': train_data['count'],
                        'test_samples': 0,
                        'train_mean_return': train_data['mean_return'],
                        'test_mean_return': np.nan,
                        'train_rank': train_data['rank'],
                        'test_rank': np.nan,
                        'overall_return': train_data['mean_return'],
                        'validation_passed': False,  # æ— æµ‹è¯•é›†æ ·æœ¬è§†ä¸ºæœªé€šè¿‡éªŒè¯
                        'is_best_in_k': train_data['rank'] == 1,
                        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    })
                # å¦‚æœåªåœ¨æµ‹è¯•é›†ä¸­å­˜åœ¨æˆ–éƒ½ä¸å­˜åœ¨ï¼Œåˆ™è·³è¿‡ï¼ˆè¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿåœ¨æ­£å¸¸èšç±»ä¸­ï¼‰
        
        # ä¿å­˜èšç±»æ¯”è¾ƒè¡¨
        if comparison_data:
            comparison_df = pd.DataFrame(comparison_data)
            comparison_df = comparison_df.sort_values(['validation_passed', 'overall_return'], ascending=[False, False])
            comparison_df['global_rank'] = range(1, len(comparison_df) + 1)
            comparison_csv_file = os.path.join(self.reports_dir, "cluster_comparison.csv")
            comparison_df.to_csv(comparison_csv_file, index=False)
        
        # === 6. ä¿å­˜æ±‡æ€»CSVæ–‡ä»¶ ===
        if all_summary_data:
            all_summary_csv = pd.DataFrame(all_summary_data)
            summary_csv_file = os.path.join(self.reports_dir, "clustering_summary_all_k.csv")
            all_summary_csv.to_csv(summary_csv_file, index=False)
        
        # === 7. ä¿å­˜éªŒè¯ç»“æœCSV ===
        validation_data = []
        for k, validation in validations.items():
            validation_data.append({
                'k_value': k,
                'silhouette_score': validation['silhouette_score'],
                'calinski_score': validation['calinski_score'],
                'train_difference': validation['train_difference'],
                'train_significant': validation['train_significant'],
                'test_best_cluster_rank': validation['test_best_cluster_rank'],
                'test_top_50_percent': validation['test_top_50_percent'],
                'overall_valid': validation['train_significant'] and validation['test_top_50_percent']
            })
        
        validation_csv = pd.DataFrame(validation_data)
        validation_csv_file = os.path.join(self.reports_dir, "clustering_validation_results.csv")
        validation_csv.to_csv(validation_csv_file, index=False)
        
        # === 8. è¾“å‡ºç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨ ===
        print(f"ğŸ“„ ä¸»æŠ¥å‘Š: {main_report_file}")
        print(f"ğŸ† èšç±»æ¯”è¾ƒ: {comparison_csv_file}")
        print(f"ğŸ“Š æ±‡æ€»æ•°æ®: {summary_csv_file}")
        print(f"âœ… éªŒè¯ç»“æœ: {validation_csv_file}")
        print(f"ğŸ“ è¯¦ç»†ç‰¹å¾: {len(self.k_values)}ä¸ªkå€¼çš„å•ç‹¬CSVæ–‡ä»¶")
        print(f"\næŠ¥å‘Šç”Ÿæˆå®Œæˆï¼æ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨: {self.reports_dir}")
        
        return summary

    def run_clustering_analysis(self, states_train_path: str, states_test_path: str,
                              targets_path: str) -> Dict:
        """
        è¿è¡Œå®Œæ•´çš„èšç±»åˆ†æ
        
        Parameters:
        -----------
        states_train_path : str
            è®­ç»ƒé›†PCAçŠ¶æ€æ–‡ä»¶è·¯å¾„
        states_test_path : str
            æµ‹è¯•é›†PCAçŠ¶æ€æ–‡ä»¶è·¯å¾„
        targets_path : str
            ç›®æ ‡æ”¶ç›Šæ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
        --------
        dict
            åˆ†æç»“æœæ‘˜è¦
        """
        print("Starting clustering analysis...")
        
        # åŠ è½½æ•°æ®
        states_train, states_test, targets_df = self.load_pca_states_and_targets(
            states_train_path, states_test_path, targets_path
        )
        
        # è®¡ç®—å…¨å±€æ”¶ç›Šæ ‡å‡†å·®
        global_std = targets_df['future_return_5d'].std()
        print(f"Global return std: {global_std:.4f}")
        
        all_train_results = []
        all_test_results = []
        all_validations = {}
        
        # å¯¹æ¯ä¸ªkå€¼è¿›è¡Œèšç±»åˆ†æ
        for k in self.k_values:
            print(f"\n--- K-Means with k={k} ---")
            
            # è®­ç»ƒKMeansæ¨¡å‹
            kmeans = self.perform_kmeans_clustering(states_train, k)
            self.cluster_models[k] = kmeans
            
            # è¯„ä¼°è®­ç»ƒé›†æ€§èƒ½
            train_results = self.evaluate_cluster_returns(
                states_train, targets_df, kmeans, phase="train"
            )
            train_results['timestamp'] = datetime.now()
            all_train_results.append(train_results)
            
            # è¯„ä¼°æµ‹è¯•é›†æ€§èƒ½
            test_results = self.evaluate_cluster_returns(
                states_test, targets_df, kmeans, phase="test"
            )
            test_results['timestamp'] = datetime.now()
            all_test_results.append(test_results)
            
            # éªŒè¯èšç±»æ€§èƒ½
            validation = self.validate_cluster_performance(
                train_results, test_results, global_std
            )
            validation['k_value'] = k
            
            # æ·»åŠ èšç±»è´¨é‡æŒ‡æ ‡
            silhouette_avg = silhouette_score(states_train, kmeans.labels_)
            calinski_score_val = calinski_harabasz_score(states_train, kmeans.labels_)
            validation['silhouette_score'] = silhouette_avg
            validation['calinski_score'] = calinski_score_val
            
            all_validations[k] = validation
            
            print(f"Training: Best={train_results.iloc[0]['mean_return']:.4f}, "
                  f"Worst={train_results.iloc[-1]['mean_return']:.4f}")
            print(f"Validation: Train significant={validation['train_significant']}, "
                  f"Test top 50%={validation['test_top_50_percent']}")
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        train_combined = pd.concat(all_train_results, ignore_index=True)
        test_combined = pd.concat(all_test_results, ignore_index=True)
        
        # === è®¡ç®—å¹¶ä¿å­˜æœ€ä½³PCä¿¡æ¯ï¼ˆåŸºäºè®­ç»ƒé›†å†å²æ•°æ®ï¼‰ ===
        print("\nğŸ“Š è®¡ç®—æœ€ä½³PCï¼ˆåŸºäºè®­ç»ƒé›†ï¼‰...")
        from scipy import stats

        pc_metadata = {
            'best_pc': None,
            'best_pc_index': None,
            'pc_direction': 1.0,
            'pc_threshold': 0.0,
            'threshold_quantile': 0.6,
            'ic_value': 0.0,
            'all_ic_values': [],
            'calculated_time': datetime.now().isoformat()
        }

        if 'future_return_5d' in targets_df.columns and len(states_train) > 10:
            ret_values = targets_df.iloc[:len(states_train)]['future_return_5d'].fillna(0).values
            ic_list = []

            for idx in range(states_train.shape[1]):
                pc_values = states_train[:, idx]
                pc_t1 = np.roll(pc_values, 1)
                pc_t1[0] = 0  # ç¬¬ä¸€ä¸ªä½ç½®æ— T+1

                ic, _ = stats.spearmanr(pc_t1, ret_values)
                ic_list.append(0.0 if np.isnan(ic) else float(ic))

            pc_metadata['all_ic_values'] = ic_list

            if ic_list:
                abs_ic = np.abs(ic_list)
                best_pc_idx = int(np.argmax(abs_ic))
                best_ic = ic_list[best_pc_idx]
                direction = 1.0 if best_ic >= 0 else -1.0

                strength = states_train[:, best_pc_idx] * direction
                threshold = float(np.quantile(strength, pc_metadata['threshold_quantile']))

                pc_metadata.update({
                    'best_pc': f'PC{best_pc_idx + 1}',
                    'best_pc_index': best_pc_idx,
                    'pc_direction': direction,
                    'pc_threshold': threshold,
                    'ic_value': best_ic
                })

                print(f"   âœ… æœ€ä½³PC: {pc_metadata['best_pc']} (IC={best_ic:.4f}, é—¨æ§›={threshold:.4f})")
            else:
                print("   âš ï¸ æœªèƒ½è®¡ç®—PCçš„ICï¼Œä½¿ç”¨é»˜è®¤PC1")
        else:
            print("   âš ï¸ è®­ç»ƒæ•°æ®ç¼ºå°‘future_return_5dæˆ–æ ·æœ¬ä¸è¶³ï¼Œä½¿ç”¨é»˜è®¤PC1")

        # è‹¥æœªæˆåŠŸç¡®å®šæœ€ä½³PCï¼Œä½¿ç”¨é»˜è®¤é…ç½®
        if pc_metadata['best_pc'] is None:
            pc_metadata.update({
                'best_pc': 'PC1',
                'best_pc_index': 0,
                'pc_direction': 1.0,
                'pc_threshold': 0.0,
                'ic_value': 0.0
            })

        # ä¿å­˜PCå…ƒæ•°æ®
        pc_metadata_file = os.path.join(self.reports_dir, "pc_metadata.pkl")
        with open(pc_metadata_file, 'wb') as f:
            pickle.dump(pc_metadata, f)
        print(f"   ğŸ’¾ PCå…ƒæ•°æ®å·²ä¿å­˜: {pc_metadata_file}")
        
        # ä¿å­˜èšç±»æ¨¡å‹
        models_file = os.path.join(self.reports_dir, "cluster_models.pkl")
        with open(models_file, 'wb') as f:
            pickle.dump(self.cluster_models, f)
        print(f"ğŸ’¾ èšç±»æ¨¡å‹å·²ä¿å­˜: {models_file}")
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Šï¼ˆåˆå¹¶æ‰€æœ‰æŠ¥å‘Šç”ŸæˆåŠŸèƒ½ï¼‰
        summary = self.generate_comprehensive_report(all_train_results, all_test_results, 
                                                   all_validations, global_std)
        
        return {
            'train_results': train_combined,
            'test_results': test_combined,
            'validations': all_validations,
            'summary': summary
        }


    def print_analysis_summary(self, results: Dict):
        """
        æ‰“å°åˆ†ææ‘˜è¦
        
        Parameters:
        -----------
        results : dict
            åˆ†æç»“æœ
        """
        summary = results['summary']
        
        print("\n" + "="*60)
        print("CLUSTERING ANALYSIS SUMMARY")
        print("="*60)
        
        print(f"Global return std: {summary['global_std']:.4f}")
        print(f"Significance threshold: {summary['threshold']:.4f}")
        print(f"K values tested: {self.k_values}")
        
        print(f"\nValidation Results:")
        print(f"  Training significance: {summary['passed_train_validation']}/{summary['total_k_values']} "
              f"({summary['train_success_rate']:.1%})")
        print(f"  Test top 50%: {summary['passed_test_validation']}/{summary['total_k_values']} "
              f"({summary['test_success_rate']:.1%})")
        print(f"  Both criteria: {summary['passed_both_validation']}/{summary['total_k_values']} "
              f"({summary['overall_success_rate']:.1%})")
        
        if summary['best_k']:
            best = summary['best_performance']
            print(f"\nBest performing k={summary['best_k']}:")
            print(f"  Training difference: {best['train_difference']:.4f} > {best['threshold']:.4f} âœ“")
            print(f"  Test rank: {best['test_best_cluster_rank']}/{best['total_clusters']} "
                  f"(top {best['test_best_cluster_rank']/best['total_clusters']:.1%}) âœ“")
        else:
            print(f"\nNo k value passed both validation criteria")
        
        print("="*60)



def find_latest_files(evaluator, states_dir: str = None, targets_dir: str = None):
    """
    æŸ¥æ‰¾æœ€æ–°çš„PCAçŠ¶æ€å’Œç›®æ ‡æ–‡ä»¶
    
    Parameters:
    -----------
    evaluator : ClusterEvaluator
        è¯„ä¼°å™¨å¯¹è±¡
    states_dir : str, optional
        çŠ¶æ€æ–‡ä»¶ç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
    targets_dir : str, optional
        ç›®æ ‡æ–‡ä»¶ç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
    """
    if states_dir is None:
        states_dir = os.path.join(ml_root, "ML output/states/baseline_v1")
    if targets_dir is None:
        targets_dir = os.path.join(ml_root, "ML output/datasets/baseline_v1")
    
    if not (os.path.exists(states_dir) and os.path.exists(targets_dir)):
        return None, None, None
    
    # æŸ¥æ‰¾çŠ¶æ€æ–‡ä»¶
    state_files = [f for f in os.listdir(states_dir) if f.startswith('states_pca_train_') and f.endswith('.npy')]
    target_files = [f for f in os.listdir(targets_dir) if f.startswith('with_targets_') and f.endswith('.csv')]
    
    if not (state_files and target_files):
        return None, None, None
    
    # è·å–æœ€æ–°æ–‡ä»¶
    latest_state_train = max(state_files, key=lambda x: os.path.getctime(os.path.join(states_dir, x)))
    latest_state_test = latest_state_train.replace('states_pca_train_', 'states_pca_test_')
    latest_target = max(target_files, key=lambda x: os.path.getctime(os.path.join(targets_dir, x)))
    
    train_path = os.path.join(states_dir, latest_state_train)
    test_path = os.path.join(states_dir, latest_state_test)
    targets_path = os.path.join(targets_dir, latest_target)
    
    # æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§
    if all(os.path.exists(p) for p in [train_path, test_path, targets_path]):
        return train_path, test_path, targets_path
    else:
        return None, None, None


def main(config: dict = None):
    """
    ä¸»å‡½æ•°ï¼šèšç±»+çŠ¶æ€æ”¶ç›Šè¯„ä¼°
    
    Parameters:
    -----------
    config : dict, optional
        é…ç½®å­—å…¸ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
    """
    print("="*60)
    print("èšç±» + çŠ¶æ€æ”¶ç›Šè¯„ä¼°")
    print("="*60)
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # å¦‚æœæ²¡æœ‰æä¾›é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    if config is None:
        config = {
            'paths': {
                'reports_clustering': 'ML output/reports',
                'states_dir': 'ML output/states'
            },
            'clustering': {
                'k_range': [4, 5, 6],
                'method': 'kmeans',
                'random_state': 42
            }
        }
    
    # ä»é…ç½®ä¸­æå–å‚æ•°
    reports_dir = config.get('paths', {}).get('reports_clustering', 'ML output/reports/baseline_v1/clustering')
    states_dir = config.get('paths', {}).get('states_dir', 'ML output/states/baseline_v1')
    k_range = config.get('clustering', {}).get('k_range', [4, 5, 6])
    random_state = config.get('clustering', {}).get('random_state', 42)
    
    # è½¬æ¢ç›¸å¯¹è·¯å¾„ä¸ºç»å¯¹è·¯å¾„
    if not os.path.isabs(reports_dir):
        reports_dir = os.path.join(ml_root, reports_dir)
    if not os.path.isabs(states_dir):
        states_dir = os.path.join(ml_root, states_dir)
    
    print(f"\nğŸ“‹ é…ç½®:")
    print(f"   reports_dir: {reports_dir}")
    print(f"   states_dir: {states_dir}")
    print(f"   k_range: {k_range}")
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨ï¼ˆä½¿ç”¨é…ç½®çš„ç›®å½•ï¼‰
    evaluator = ClusterEvaluator(reports_dir=reports_dir)
    evaluator.k_values = k_range
    evaluator.random_state = random_state
    
    # æŸ¥æ‰¾å®é™…æ•°æ®æ–‡ä»¶ï¼ˆä¼ å…¥é…ç½®çš„ç›®å½•ï¼‰
    train_path, test_path, targets_path = find_latest_files(
        evaluator, 
        states_dir=states_dir,
        targets_dir=os.path.join(ml_root, "ML output/datasets/baseline_v1")
    )
    
    if train_path is None:
        print("\nâŒ æœªæ‰¾åˆ°æ‰€éœ€çš„æ•°æ®æ–‡ä»¶ï¼")
        print("è¯·ç¡®ä¿ä»¥ä¸‹ç›®å½•å­˜åœ¨ç›¸åº”æ–‡ä»¶ï¼š")
        print(f"  - {states_dir} (states_pca_train_*.npy)")
        print(f"  - {os.path.join(ml_root, 'ML output/datasets/baseline_v1')} (with_targets_*.csv)")
        return
    else:
        print(f"\nâœ… æ‰¾åˆ°æ•°æ®æ–‡ä»¶:")
        print(f"  è®­ç»ƒçŠ¶æ€: {os.path.basename(train_path)}")
        print(f"  æµ‹è¯•çŠ¶æ€: {os.path.basename(test_path)}")
        print(f"  ç›®æ ‡æ•°æ®: {os.path.basename(targets_path)}")
    
    try:
        print(f"\nå¼€å§‹èšç±»åˆ†æ...")
        print("-" * 40)
        
        # è¿è¡Œèšç±»åˆ†æ
        results = evaluator.run_clustering_analysis(
            train_path, test_path, targets_path
        )
        
        # æ‰“å°æ‘˜è¦
        evaluator.print_analysis_summary(results)
        
        # éªŒæ”¶ç»“æœæ£€æŸ¥
        print("\n" + "="*60)
        print("éªŒæ”¶æ ‡å‡†æ£€æŸ¥")
        print("="*60)
        
        summary = results['summary']
        
        print("1. KMeansèšç±» (k=4,5,6) â†’ future_return_5dæ’åº: âœ…")
        print("2. ç”Ÿæˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†æ€§èƒ½æŠ¥å‘Š: âœ…")
        
        if summary['passed_train_validation'] > 0:
            print(f"3. è®­ç»ƒé›†éªŒæ”¶ (æœ€ä½³vsæœ€å·® > å…¨å±€std*0.4): âœ… ({summary['passed_train_validation']}/{summary['total_k_values']})")
        else:
            print(f"3. è®­ç»ƒé›†éªŒæ”¶ (æœ€ä½³vsæœ€å·® > å…¨å±€std*0.4): âŒ (0/{summary['total_k_values']})")
        
        if summary['passed_test_validation'] > 0:
            print(f"4. æµ‹è¯•é›†éªŒæ”¶ (æœ€ä½³èšç±»ä¿æŒå‰50%): âœ… ({summary['passed_test_validation']}/{summary['total_k_values']})")
        else:
            print(f"4. æµ‹è¯•é›†éªŒæ”¶ (æœ€ä½³èšç±»ä¿æŒå‰50%): âŒ (0/{summary['total_k_values']})")
        
        overall_success = summary['overall_success_rate'] > 0
        print(f"\næ€»ä½“è¯„ä»·: {'âœ… é€šè¿‡' if overall_success else 'âŒ æœªé€šè¿‡'} ({summary['passed_both_validation']}/{summary['total_k_values']} æ»¡è¶³å…¨éƒ¨æ¡ä»¶)")
        
        print("\n" + "="*60)
        print("é˜¶æ®µ10å®Œæˆï¼")
        print("="*60)
        
    except Exception as e:
        print(f"\nâŒ èšç±»åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()