#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å› å­è´¨é‡æ£€æŸ¥å™¨ - å› å­ä½“æ£€æµç¨‹

ã€æ¨¡å—å®šä½ã€‘
æœ¬æ¨¡å—ä¸“æ³¨äºã€Œå› å­å·¥å‚ã€ç‰¹æœ‰çš„è´¨é‡æ£€æŸ¥ï¼Œå¤ç”¨ cross_section_metrics.py çš„ICè®¡ç®—ã€‚

ç‹¬ç‰¹åŠŸèƒ½ï¼š
1. âœ… ICåŠè¡°æœŸä¸IC Decayæ›²çº¿ï¼ˆæ—¶é—´è¡°å‡ç‰¹æ€§ï¼‰
2. âœ… PSI/KSæµ‹è¯•ï¼ˆåˆ†å¸ƒç¨³å®šæ€§æ£€æµ‹ï¼‰
3. âœ… ç›¸å…³æ€§æ£€æŸ¥ï¼ˆé¿å…å†—ä½™å› å­ï¼‰
4. âœ… ç»¼åˆè´¨é‡è¯„åˆ†ï¼ˆ6å±‚æ£€æŸ¥æ‰“åˆ†ï¼‰

å¤ç”¨åŠŸèƒ½ï¼š
- IC/ICIRè®¡ç®— â†’ ä½¿ç”¨ cross_section_metrics.calculate_daily_ic()
- å•è°ƒæ€§æ£€éªŒ â†’ ä½¿ç”¨ cross_section_metrics çš„åˆ†æ¡¶åŠŸèƒ½

éªŒæ”¶æ ‡å‡†ï¼š
- ICå‡å€¼ > 0.02, ICIR > 0.5 (å¹´åŒ–)
- PSI < 0.25 (åˆ†å¸ƒç¨³å®š)
- ä¸å·²æœ‰å› å­ç›¸å…³æ€§ < 0.7
- è‡³å°‘é€šè¿‡ 4/6 é¡¹æ£€æŸ¥

åˆ›å»º: 2025-01-20 | ç‰ˆæœ¬: v1.0
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple
from scipy import stats
from scipy.stats import ks_2samp
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥å·²æœ‰çš„ICè®¡ç®—åŠŸèƒ½
from evaluation.cross_section_metrics import (
    calculate_daily_ic,
    calculate_ic_summary
)


class FactorQualityChecker:
    """
    å› å­è´¨é‡æ£€æŸ¥å™¨
    
    æä¾›å®Œæ•´çš„å› å­"ä½“æ£€"æµç¨‹
    """
    
    def __init__(self, 
                 ic_threshold: float = 0.02,
                 icir_threshold: float = 0.5,
                 psi_threshold: float = 0.25,
                 corr_threshold: float = 0.7):
        """
        åˆå§‹åŒ–è´¨é‡æ£€æŸ¥å™¨
        
        Parameters:
        -----------
        ic_threshold : float
            ICå‡å€¼é˜ˆå€¼
        icir_threshold : float
            ICIRå¹´åŒ–é˜ˆå€¼
        psi_threshold : float
            PSIé˜ˆå€¼
        corr_threshold : float
            ç›¸å…³æ€§é˜ˆå€¼
        """
        self.ic_threshold = ic_threshold
        self.icir_threshold = icir_threshold
        self.psi_threshold = psi_threshold
        self.corr_threshold = corr_threshold
        
        print("ğŸ”¬ å› å­è´¨é‡æ£€æŸ¥å™¨åˆå§‹åŒ–")
        print(f"   ICé˜ˆå€¼: {ic_threshold}")
        print(f"   ICIRé˜ˆå€¼: {icir_threshold}")
        print(f"   PSIé˜ˆå€¼: {psi_threshold}")
        print(f"   ç›¸å…³æ€§é˜ˆå€¼: {corr_threshold}")
    
    def calculate_ic_metrics(self, 
                            factor_values: pd.Series, 
                            target_values: pd.Series,
                            method: str = 'spearman') -> Dict:
        """
        è®¡ç®—ICæŒ‡æ ‡ï¼ˆå¤ç”¨ cross_section_metrics çš„åŠŸèƒ½ï¼‰
        
        æ³¨æ„ï¼šæœ¬æ–¹æ³•å·²æ”¹ä¸ºè½»é‡çº§åŒ…è£…å™¨ï¼Œæ ¸å¿ƒè®¡ç®—ç”± cross_section_metrics å®Œæˆã€‚
        
        Parameters:
        -----------
        factor_values : pd.Series
            å› å­å€¼ï¼ŒMultiIndex[date, ticker]
        target_values : pd.Series
            ç›®æ ‡å€¼ï¼ˆè¿œæœŸæ”¶ç›Šï¼‰ï¼ŒMultiIndex[date, ticker]
        method : str
            'spearman' æˆ– 'pearson'
            
        Returns:
        --------
        dict
            ICæŒ‡æ ‡å­—å…¸ï¼ˆæ ¼å¼å…¼å®¹æ—§ç‰ˆï¼‰
        """
        # è½¬æ¢ä¸ºDataFrameæ ¼å¼ä»¥é€‚é… cross_section_metrics
        factor_df = pd.DataFrame({'factor': factor_values})
        target_df = pd.DataFrame({'target': target_values})
        
        # ä½¿ç”¨ cross_section_metrics è®¡ç®—æ¯æ—¥IC
        daily_ic_df = calculate_daily_ic(factor_df, target_df, method=method)
        
        if daily_ic_df.empty:
            return {
                'ic_mean': np.nan,
                'ic_std': np.nan,
                'icir': np.nan,
                'icir_annual': np.nan,
                't_stat': np.nan,
                'p_value': np.nan,
                'positive_ratio': np.nan,
                'pass_ic': False
            }
        
        # æå–ICåºåˆ—ï¼ˆç¬¬ä¸€åˆ—ï¼‰
        ic_series = daily_ic_df.iloc[:, 0].dropna()
        
        # ä½¿ç”¨ cross_section_metrics è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        ic_summary = calculate_ic_summary(ic_series, annualize=True)
        
        # åˆ¤æ–­æ˜¯å¦é€šè¿‡
        pass_ic = abs(ic_summary['mean']) > self.ic_threshold and ic_summary['p_value'] < 0.05
        
        # è½¬æ¢ä¸ºå…¼å®¹æ ¼å¼
        return {
            'ic_mean': ic_summary['mean'],
            'ic_std': ic_summary['std'],
            'icir': ic_summary['icir'],
            'icir_annual': ic_summary['icir_annual'],
            't_stat': ic_summary['t_stat'],
            'p_value': ic_summary['p_value'],
            'positive_ratio': ic_summary['positive_ratio'],
            'pass_ic': pass_ic
        }
    
    def calculate_ic_decay(self, 
                          factor: pd.Series,
                          prices: pd.DataFrame,
                          max_period: int = 20) -> pd.DataFrame:
        """
        è®¡ç®—ICè¡°å‡æ›²çº¿
        
        Parameters:
        -----------
        factor : pd.Series
            å› å­å€¼
        prices : pd.DataFrame
            ä»·æ ¼æ•°æ®ï¼ŒåŒ…å«'close'åˆ—
        max_period : int
            æœ€å¤§å‰ç»æœŸ
            
        Returns:
        --------
        pd.DataFrame
            ICè¡°å‡æ›²çº¿
        """
        ic_decay = []
        
        for period in range(1, max_period + 1):
            forward_return = prices['close'].pct_change(period).shift(-period)
            
            valid_mask = factor.notna() & forward_return.notna()
            if valid_mask.sum() < 30:
                continue
            
            ic, _ = stats.spearmanr(
                factor[valid_mask],
                forward_return[valid_mask]
            )
            
            ic_decay.append({
                'period': period,
                'ic': ic,
                'abs_ic': abs(ic)
            })
        
        decay_df = pd.DataFrame(ic_decay)
        
        # è®¡ç®—åŠè¡°æœŸï¼ˆICä¸‹é™åˆ°åˆå§‹å€¼50%çš„æœŸæ•°ï¼‰
        if not decay_df.empty:
            initial_ic = abs(decay_df.iloc[0]['ic'])
            half_ic = initial_ic * 0.5
            
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªä½äºhalf_icçš„ç‚¹
            below_half = decay_df[decay_df['abs_ic'] < half_ic]
            half_life = below_half.iloc[0]['period'] if not below_half.empty else max_period
        else:
            half_life = np.nan
        
        return decay_df, half_life
    
    def calculate_psi(self, 
                     factor: pd.Series, 
                     train_end_idx: int,
                     n_bins: int = 10) -> float:
        """
        è®¡ç®—PSI (Population Stability Index)
        
        ç”¨äºæ£€æµ‹å› å­åˆ†å¸ƒæ¼‚ç§»
        
        Parameters:
        -----------
        factor : pd.Series
            å› å­å€¼
        train_end_idx : int
            è®­ç»ƒé›†ç»“æŸç´¢å¼•
        n_bins : int
            åˆ†ç®±æ•°é‡
            
        Returns:
        --------
        float
            PSIå€¼
        """
        train_factor = factor.iloc[:train_end_idx].dropna()
        test_factor = factor.iloc[train_end_idx:].dropna()
        
        if len(train_factor) < 30 or len(test_factor) < 30:
            return np.nan
        
        # åŸºäºè®­ç»ƒé›†ç¡®å®šåˆ†ç®±è¾¹ç•Œ
        _, bin_edges = pd.qcut(train_factor, q=n_bins, retbins=True, duplicates='drop')
        
        # è®¡ç®—è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åˆ†å¸ƒ
        train_dist, _ = np.histogram(train_factor, bins=bin_edges)
        test_dist, _ = np.histogram(test_factor, bins=bin_edges)
        
        # è½¬æ¢ä¸ºæ¯”ä¾‹
        train_pct = train_dist / len(train_factor)
        test_pct = test_dist / len(test_factor)
        
        # é¿å…é›¶å€¼
        train_pct = np.where(train_pct == 0, 0.0001, train_pct)
        test_pct = np.where(test_pct == 0, 0.0001, test_pct)
        
        # è®¡ç®—PSI
        psi = np.sum((test_pct - train_pct) * np.log(test_pct / train_pct))
        
        return psi
    
    def calculate_ks_statistic(self, 
                               factor: pd.Series,
                               train_end_idx: int) -> Tuple[float, float]:
        """
        è®¡ç®—KSç»Ÿè®¡é‡
        
        ç”¨äºæ£€æµ‹åˆ†å¸ƒå·®å¼‚
        
        Parameters:
        -----------
        factor : pd.Series
            å› å­å€¼
        train_end_idx : int
            è®­ç»ƒé›†ç»“æŸç´¢å¼•
            
        Returns:
        --------
        tuple
            (KSç»Ÿè®¡é‡, på€¼)
        """
        train_factor = factor.iloc[:train_end_idx].dropna()
        test_factor = factor.iloc[train_end_idx:].dropna()
        
        if len(train_factor) < 30 or len(test_factor) < 30:
            return np.nan, np.nan
        
        ks_stat, p_value = ks_2samp(train_factor, test_factor)
        
        return ks_stat, p_value
    
    def check_correlation_with_existing(self,
                                       new_factor: pd.Series,
                                       existing_factors: pd.DataFrame) -> Dict:
        """
        æ£€æŸ¥æ–°å› å­ä¸å·²æœ‰å› å­çš„ç›¸å…³æ€§
        
        Parameters:
        -----------
        new_factor : pd.Series
            æ–°å› å­
        existing_factors : pd.DataFrame
            å·²æœ‰å› å­é›†åˆ
            
        Returns:
        --------
        dict
            ç›¸å…³æ€§åˆ†æç»“æœ
        """
        if existing_factors.empty:
            return {
                'max_corr': 0.0,
                'max_corr_factor': None,
                'high_corr_count': 0,
                'pass_corr': True
            }
        
        # è®¡ç®—ä¸æ‰€æœ‰å·²æœ‰å› å­çš„ç›¸å…³æ€§
        correlations = {}
        for col in existing_factors.columns:
            valid_mask = new_factor.notna() & existing_factors[col].notna()
            if valid_mask.sum() < 30:
                continue
            
            corr, _ = stats.spearmanr(
                new_factor[valid_mask],
                existing_factors[col][valid_mask]
            )
            correlations[col] = abs(corr)
        
        if not correlations:
            return {
                'max_corr': 0.0,
                'max_corr_factor': None,
                'high_corr_count': 0,
                'pass_corr': True
            }
        
        max_corr = max(correlations.values())
        max_corr_factor = max(correlations, key=correlations.get)
        high_corr_count = sum(1 for c in correlations.values() if c > self.corr_threshold)
        
        pass_corr = max_corr < self.corr_threshold
        
        return {
            'max_corr': max_corr,
            'max_corr_factor': max_corr_factor,
            'high_corr_count': high_corr_count,
            'correlations': correlations,
            'pass_corr': pass_corr
        }
    
    def check_monotonicity(self,
                          factor: pd.Series,
                          forward_return: pd.Series,
                          n_quantiles: int = 5) -> Dict:
        """
        æ£€æŸ¥å•è°ƒæ€§ï¼ˆåˆ†ä½æ•°ç»„åˆæ”¶ç›Šæ˜¯å¦å•è°ƒï¼‰
        
        æ³¨æ„ï¼šcross_section_metrics ä¹Ÿæœ‰åˆ†æ¡¶åŠŸèƒ½ï¼Œä½†æ­¤å¤„ä¸ºç®€åŒ–ç‰ˆä¸“ç”¨äºå¿«é€Ÿæ£€æŸ¥ã€‚
        
        Parameters:
        -----------
        factor : pd.Series
            å› å­å€¼ï¼Œæ”¯æŒå•å±‚æˆ–MultiIndex
        forward_return : pd.Series
            è¿œæœŸæ”¶ç›Šç‡ï¼Œæ”¯æŒå•å±‚æˆ–MultiIndex
        n_quantiles : int
            åˆ†ä½æ•°æ•°é‡
            
        Returns:
        --------
        dict
            å•è°ƒæ€§æ£€éªŒç»“æœ
        """
        # å¤„ç†MultiIndexæƒ…å†µï¼šå±•å¹³åè®¡ç®—ï¼ˆè·¨æ—¥æœŸæ¨ªæˆªé¢ï¼‰
        if isinstance(factor.index, pd.MultiIndex):
            factor = factor.reset_index(drop=True)
            forward_return = forward_return.reset_index(drop=True)
        
        # å¯¹é½æ•°æ®
        valid_mask = factor.notna() & forward_return.notna()
        factor_clean = factor[valid_mask]
        return_clean = forward_return[valid_mask]
        
        if len(factor_clean) < n_quantiles * 10:
            return {
                'kendall_tau': np.nan,
                'kendall_p': np.nan,
                'monotonic_ratio': np.nan,
                'pass_monotonicity': False
            }
        
        # åˆ†ä½æ•°åˆ†ç»„
        quantiles = pd.qcut(factor_clean, q=n_quantiles, labels=False, duplicates='drop')
        
        # è®¡ç®—å„åˆ†ä½æ•°çš„å¹³å‡æ”¶ç›Š
        quantile_returns = []
        for q in range(n_quantiles):
            q_mask = quantiles == q
            if q_mask.sum() > 0:
                mean_return = return_clean[q_mask].mean()
                quantile_returns.append(mean_return)
        
        if len(quantile_returns) < n_quantiles:
            return {
                'kendall_tau': np.nan,
                'kendall_p': np.nan,
                'monotonic_ratio': np.nan,
                'pass_monotonicity': False
            }
        
        # Kendall Ï„æ£€éªŒ
        expected_ranks = np.arange(n_quantiles)
        actual_ranks = stats.rankdata(quantile_returns) - 1
        
        kendall_tau, kendall_p = stats.kendalltau(expected_ranks, actual_ranks)
        
        # å•è°ƒæ€§æ¯”ä¾‹ï¼ˆç›¸é‚»åˆ†ä½æ•°æ”¶ç›Šé€’å¢çš„æ¯”ä¾‹ï¼‰
        monotonic_count = sum(1 for i in range(len(quantile_returns) - 1) 
                            if quantile_returns[i] < quantile_returns[i + 1])
        monotonic_ratio = monotonic_count / (len(quantile_returns) - 1)
        
        # åˆ¤æ–­æ˜¯å¦é€šè¿‡ï¼ˆKendall Ï„ > 0.5 æˆ– å•è°ƒæ€§æ¯”ä¾‹ > 0.6ï¼‰
        pass_monotonicity = kendall_tau > 0.5 or monotonic_ratio > 0.6
        
        return {
            'kendall_tau': kendall_tau,
            'kendall_p': kendall_p,
            'monotonic_ratio': monotonic_ratio,
            'quantile_returns': quantile_returns,
            'pass_monotonicity': pass_monotonicity
        }
    
    def comprehensive_check(self,
                           factor_values: pd.Series,
                           target_values: pd.Series,
                           prices: Optional[pd.DataFrame] = None,
                           existing_factors: Optional[pd.DataFrame] = None,
                           train_ratio: float = 0.8) -> Dict:
        """
        ç»¼åˆè´¨é‡æ£€æŸ¥ï¼ˆå®Œæ•´ä½“æ£€æµç¨‹ï¼‰
        
        å¤ç”¨ cross_section_metrics çš„ICè®¡ç®—ï¼Œä¸“æ³¨äºå› å­å·¥å‚ç‰¹æœ‰çš„æ£€æŸ¥é¡¹ã€‚
        
        Parameters:
        -----------
        factor_values : pd.Series
            å¾…æ£€æŸ¥å› å­ï¼ŒMultiIndex[date, ticker]
        target_values : pd.Series
            ç›®æ ‡å€¼ï¼ˆè¿œæœŸæ”¶ç›Šï¼‰ï¼ŒMultiIndex[date, ticker]
        prices : pd.DataFrame, optional
            ä»·æ ¼æ•°æ®ï¼ˆç”¨äºICè¡°å‡ï¼Œå¦‚æ— åˆ™è·³è¿‡ï¼‰
        existing_factors : pd.DataFrame, optional
            å·²æœ‰å› å­ï¼ˆç”¨äºç›¸å…³æ€§æ£€æŸ¥ï¼‰
        train_ratio : float
            è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆç”¨äºPSI/KSæ£€æŸ¥ï¼‰
            
        Returns:
        --------
        dict
            å®Œæ•´çš„è´¨é‡æ£€æŸ¥æŠ¥å‘Š
        """
        print(f"\nğŸ”¬ ç»¼åˆè´¨é‡æ£€æŸ¥")
        print("=" * 60)
        
        train_end_idx = int(len(factor_values) * train_ratio)
        
        # 1. ICæŒ‡æ ‡ï¼ˆå¤ç”¨ cross_section_metricsï¼‰
        print("1ï¸âƒ£  ICæŒ‡æ ‡ï¼ˆå¤ç”¨ cross_section_metricsï¼‰...")
        ic_metrics = self.calculate_ic_metrics(factor_values, target_values)
        print(f"   ICå‡å€¼: {ic_metrics['ic_mean']:.4f} ({'âœ…' if ic_metrics['pass_ic'] else 'âŒ'})")
        print(f"   ICIRå¹´åŒ–: {ic_metrics['icir_annual']:.2f}")
        print(f"   p-value: {ic_metrics['p_value']:.4f}")
        
        # 2. ICè¡°å‡ï¼ˆç‹¬ç‰¹åŠŸèƒ½ï¼‰
        if prices is not None and not prices.empty:
            print("2ï¸âƒ£  ICè¡°å‡æ›²çº¿...")
            ic_decay_df, half_life = self.calculate_ic_decay(factor_values, prices, max_period=20)
            print(f"   ICåŠè¡°æœŸ: {half_life:.1f} å¤©")
        else:
            print("2ï¸âƒ£  ICè¡°å‡æ›²çº¿...ï¼ˆæ— ä»·æ ¼æ•°æ®ï¼Œè·³è¿‡ï¼‰")
            ic_decay_df, half_life = pd.DataFrame(), np.nan
        
        # 3. PSIï¼ˆç‹¬ç‰¹åŠŸèƒ½ï¼‰
        print("3ï¸âƒ£  PSIæµ‹è¯•...")
        psi = self.calculate_psi(factor_values, train_end_idx)
        pass_psi = psi < self.psi_threshold if not np.isnan(psi) else False
        print(f"   PSI: {psi:.4f} ({'âœ…' if pass_psi else 'âŒ'})")
        
        # 4. KSç»Ÿè®¡é‡ï¼ˆç‹¬ç‰¹åŠŸèƒ½ï¼‰
        print("4ï¸âƒ£  KSæµ‹è¯•...")
        ks_stat, ks_p = self.calculate_ks_statistic(factor_values, train_end_idx)
        pass_ks = ks_p > 0.05 if not np.isnan(ks_p) else False
        print(f"   KSç»Ÿè®¡é‡: {ks_stat:.4f}, p-value: {ks_p:.4f} ({'âœ…' if pass_ks else 'âŒ'})")
        
        # 5. ç›¸å…³æ€§æ£€æŸ¥ï¼ˆç‹¬ç‰¹åŠŸèƒ½ï¼‰
        print("5ï¸âƒ£  ç›¸å…³æ€§æ£€æŸ¥...")
        if existing_factors is not None and not existing_factors.empty:
            corr_check = self.check_correlation_with_existing(factor_values, existing_factors)
            print(f"   æœ€å¤§ç›¸å…³æ€§: {corr_check['max_corr']:.4f} ({'âœ…' if corr_check['pass_corr'] else 'âŒ'})")
            if corr_check['max_corr_factor']:
                print(f"   æœ€ç›¸å…³å› å­: {corr_check['max_corr_factor']}")
        else:
            corr_check = {'pass_corr': True, 'max_corr': 0.0}
            print(f"   æ— å·²æœ‰å› å­ï¼Œè·³è¿‡")
        
        # 6. å•è°ƒæ€§æ£€éªŒ
        print("6ï¸âƒ£  å•è°ƒæ€§æ£€éªŒ...")
        monotonicity = self.check_monotonicity(factor_values, target_values)
        print(f"   Kendall Ï„: {monotonicity['kendall_tau']:.4f} ({'âœ…' if monotonicity['pass_monotonicity'] else 'âŒ'})")
        print(f"   å•è°ƒæ€§æ¯”ä¾‹: {monotonicity['monotonic_ratio']:.2%}")
        
        # æ€»ä½“åˆ¤æ–­
        checks_passed = [
            ic_metrics['pass_ic'],
            pass_psi,
            pass_ks,
            corr_check['pass_corr'],
            monotonicity['pass_monotonicity']
        ]
        
        # è®¡ç®—å¤±è´¥åŸå› 
        fail_reasons = []
        if not ic_metrics['pass_ic']:
            fail_reasons.append(f"ICä¸è¶³({ic_metrics['ic_mean']:.4f}<{self.ic_threshold})")
        if not pass_psi:
            fail_reasons.append(f"PSIè¿‡å¤§({psi:.4f}>{self.psi_threshold})")
        if not pass_ks:
            fail_reasons.append(f"KSæ£€éªŒå¤±è´¥(p={ks_p:.4f})")
        if not corr_check['pass_corr']:
            fail_reasons.append(f"é«˜ç›¸å…³æ€§({corr_check['max_corr']:.4f}>{self.corr_threshold})")
        if not monotonicity['pass_monotonicity']:
            fail_reasons.append(f"å•è°ƒæ€§å¼±(Ï„={monotonicity['kendall_tau']:.4f})")
        
        overall_pass = sum(checks_passed) >= 4  # è‡³å°‘é€šè¿‡4é¡¹
        
        print(f"\n{'='*60}")
        print(f"æ€»ä½“è¯„åˆ†: {'âœ… é€šè¿‡' if overall_pass else 'âŒ ä¸é€šè¿‡'} ({sum(checks_passed)}/5)")
        if fail_reasons:
            print(f"å¤±è´¥åŸå› : {', '.join(fail_reasons)}")
        print(f"{'='*60}")
        
        return {
            'ic_metrics': ic_metrics,
            'ic_decay': ic_decay_df,
            'ic_half_life': half_life,
            'psi': psi,
            'pass_psi': pass_psi,
            'ks_stat': ks_stat,
            'ks_p': ks_p,
            'pass_ks': pass_ks,
            'corr_check': corr_check,
            'monotonicity': monotonicity,
            'checks_passed': checks_passed,
            'overall_pass': overall_pass,
            'fail_reasons': fail_reasons
        }


if __name__ == "__main__":
    """æµ‹è¯•å› å­è´¨é‡æ£€æŸ¥å™¨"""
    print("=" * 70)
    print("å› å­è´¨é‡æ£€æŸ¥å™¨æµ‹è¯•")
    print("=" * 70)
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    np.random.seed(42)
    n = 500
    
    dates = pd.date_range('2022-01-01', periods=n, freq='D')
    
    # æ¨¡æ‹Ÿä»·æ ¼æ•°æ®
    prices = pd.DataFrame({
        'close': 100 + np.random.randn(n).cumsum()
    }, index=dates)
    
    # æ¨¡æ‹Ÿå› å­ï¼ˆæœ‰ä¸€å®šé¢„æµ‹èƒ½åŠ›ï¼‰
    returns = prices['close'].pct_change().shift(-5)
    factor = returns.shift(1) + np.random.randn(n) * 0.01  # æ·»åŠ å™ªå£°
    factor.index = dates
    
    # è¿œæœŸæ”¶ç›Šç‡
    forward_return = prices['close'].pct_change(5).shift(-5)
    forward_return.index = dates
    
    # åˆ›å»ºè´¨é‡æ£€æŸ¥å™¨
    checker = FactorQualityChecker(
        ic_threshold=0.02,
        icir_threshold=0.5,
        psi_threshold=0.25,
        corr_threshold=0.7
    )
    
    # ç»¼åˆæ£€æŸ¥
    report = checker.comprehensive_check(
        factor=factor,
        forward_return=forward_return,
        prices=prices,
        existing_factors=None,
        train_ratio=0.8
    )
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼")
