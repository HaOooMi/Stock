#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¼‚ç§»æ£€æµ‹ä¸åˆ†å‰²å¯¹æ¯”æ¨¡å—

åŠŸèƒ½ï¼š
1. æ¯”è¾ƒ Train / Valid / Test é›†çš„ IC å’Œ Spread
2. æ£€æµ‹æ¼‚ç§»ï¼ˆéªŒè¯ vs æµ‹è¯•å·®å¼‚ < 20%ï¼‰
3. ç”Ÿæˆæ¼‚ç§»æŠ¥å‘Šï¼ˆJSON + HTMLï¼‰
4. ä¸ CrossSectionAnalyzer æ— ç¼é›†æˆ

éªŒæ”¶æ ‡å‡†ï¼ˆæ¥è‡ªç ”ç©¶å®ªç« ï¼‰ï¼š
- éªŒè¯ vs æµ‹è¯•ï¼šRank ICã€ICIRã€åˆ†å±‚æ”¶ç›Šå·®å¼‚ < 20%
- ç»Ÿè®¡æ£€éªŒä¸å›¾å½¢åŒ–ï¼ˆåˆ†å¸ƒã€æ—¶åºï¼‰

è¾“å‡ºç›®å½•ï¼š
/ML output/reports/baseline_vX/cv/
â”œâ”€â”€ drift_report.json        # æ¼‚ç§»æ£€æµ‹ç»“æœ
â”œâ”€â”€ drift_tearsheet.html     # å¯è§†åŒ–æŠ¥å‘Š
â””â”€â”€ split_comparison.csv     # åˆ†å‰²å¯¹æ¯”è¯¦æƒ…

åˆ›å»º: 2025-12-02 | ç‰ˆæœ¬: v1.0
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
ml_root = os.path.dirname(current_dir)
if ml_root not in sys.path:
    sys.path.insert(0, ml_root)


class DriftDetector:
    """
    æ¼‚ç§»æ£€æµ‹å™¨
    
    æ¯”è¾ƒä¸åŒæ•°æ®é›†ï¼ˆTrain/Valid/Testï¼‰çš„å› å­è¡¨ç°å·®å¼‚
    """
    
    def __init__(self, 
                 drift_threshold: float = 0.2,
                 significance_level: float = 0.05):
        """
        åˆå§‹åŒ–æ¼‚ç§»æ£€æµ‹å™¨
        
        Parameters:
        -----------
        drift_threshold : float
            æ¼‚ç§»é˜ˆå€¼ï¼ˆé»˜è®¤ 0.2ï¼Œå³ 20%ï¼‰
        significance_level : float
            ç»Ÿè®¡æ˜¾è‘—æ€§æ°´å¹³ï¼ˆé»˜è®¤ 0.05ï¼‰
        """
        self.drift_threshold = drift_threshold
        self.significance_level = significance_level
        
        print(f"ğŸ” æ¼‚ç§»æ£€æµ‹å™¨åˆå§‹åŒ–")
        print(f"   æ¼‚ç§»é˜ˆå€¼: {drift_threshold:.0%}")
        print(f"   æ˜¾è‘—æ€§æ°´å¹³: {significance_level}")
    
    def compare_ic_summaries(self,
                            train_summary: Dict,
                            valid_summary: Dict,
                            test_summary: Dict) -> Dict:
        """
        æ¯”è¾ƒ IC æ±‡æ€»ç»Ÿè®¡
        
        Parameters:
        -----------
        train_summary, valid_summary, test_summary : Dict
            IC æ±‡æ€»ç»Ÿè®¡ï¼ˆæ¥è‡ª calculate_ic_summaryï¼‰
            
        Returns:
        --------
        Dict
            æ¯”è¾ƒç»“æœ
        """
        comparison = {}
        
        metrics = ['mean', 'std', 'icir', 'icir_annual', 'positive_ratio']
        
        for metric in metrics:
            train_val = train_summary.get(metric, np.nan)
            valid_val = valid_summary.get(metric, np.nan)
            test_val = test_summary.get(metric, np.nan)
            
            # è®¡ç®—ç›¸å¯¹å·®å¼‚
            if train_val != 0 and not np.isnan(train_val):
                valid_vs_train = abs(valid_val - train_val) / abs(train_val)
                test_vs_train = abs(test_val - train_val) / abs(train_val)
                valid_vs_test = abs(valid_val - test_val) / abs(train_val)
            else:
                valid_vs_train = np.nan
                test_vs_train = np.nan
                valid_vs_test = np.nan
            
            comparison[metric] = {
                'train': train_val,
                'valid': valid_val,
                'test': test_val,
                'valid_vs_train_pct': valid_vs_train,
                'test_vs_train_pct': test_vs_train,
                'valid_vs_test_pct': valid_vs_test,
                'drift_detected': valid_vs_test > self.drift_threshold if not np.isnan(valid_vs_test) else None
            }
        
        return comparison
    
    def compare_spreads(self,
                       train_spread: pd.Series,
                       valid_spread: pd.Series,
                       test_spread: pd.Series) -> Dict:
        """
        æ¯”è¾ƒ Spread
        
        Parameters:
        -----------
        train_spread, valid_spread, test_spread : pd.Series
            Spread æ—¶é—´åºåˆ—
            
        Returns:
        --------
        Dict
            æ¯”è¾ƒç»“æœ
        """
        def calc_stats(s):
            s = s.dropna()
            if len(s) == 0:
                return {'mean': np.nan, 'std': np.nan, 'sharpe': np.nan, 'positive_ratio': np.nan}
            return {
                'mean': s.mean(),
                'std': s.std(),
                'sharpe': s.mean() / s.std() if s.std() > 0 else np.nan,
                'positive_ratio': (s > 0).mean()
            }
        
        train_stats = calc_stats(train_spread)
        valid_stats = calc_stats(valid_spread)
        test_stats = calc_stats(test_spread)
        
        comparison = {}
        
        for metric in ['mean', 'sharpe', 'positive_ratio']:
            train_val = train_stats[metric]
            valid_val = valid_stats[metric]
            test_val = test_stats[metric]
            
            if train_val != 0 and not np.isnan(train_val):
                valid_vs_test = abs(valid_val - test_val) / abs(train_val)
            else:
                valid_vs_test = np.nan
            
            comparison[f'spread_{metric}'] = {
                'train': train_val,
                'valid': valid_val,
                'test': test_val,
                'valid_vs_test_pct': valid_vs_test,
                'drift_detected': valid_vs_test > self.drift_threshold if not np.isnan(valid_vs_test) else None
            }
        
        return comparison
    
    def statistical_test_ic(self,
                           ic_series_1: pd.Series,
                           ic_series_2: pd.Series,
                           test_type: str = 'mannwhitneyu') -> Dict:
        """
        IC åˆ†å¸ƒç»Ÿè®¡æ£€éªŒ
        
        Parameters:
        -----------
        ic_series_1, ic_series_2 : pd.Series
            ä¸¤ä¸ª IC åºåˆ—
        test_type : str
            æ£€éªŒç±»å‹ï¼š'ttest', 'mannwhitneyu', 'ks'
            
        Returns:
        --------
        Dict
            æ£€éªŒç»“æœ
        """
        s1 = ic_series_1.dropna()
        s2 = ic_series_2.dropna()
        
        if len(s1) < 5 or len(s2) < 5:
            return {'test': test_type, 'statistic': np.nan, 'p_value': np.nan, 'significant': None}
        
        if test_type == 'ttest':
            stat, p = stats.ttest_ind(s1, s2)
        elif test_type == 'mannwhitneyu':
            stat, p = stats.mannwhitneyu(s1, s2, alternative='two-sided')
        elif test_type == 'ks':
            stat, p = stats.ks_2samp(s1, s2)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ£€éªŒç±»å‹: {test_type}")
        
        return {
            'test': test_type,
            'statistic': stat,
            'p_value': p,
            'significant': p < self.significance_level
        }
    
    def detect_drift(self,
                    train_results: Dict,
                    valid_results: Dict,
                    test_results: Dict,
                    factor_name: str = 'factor',
                    period: str = '5d') -> Dict:
        """
        ç»¼åˆæ¼‚ç§»æ£€æµ‹
        
        Parameters:
        -----------
        train_results, valid_results, test_results : Dict
            CrossSectionAnalyzer çš„åˆ†æç»“æœ
        factor_name : str
            å› å­åç§°
        period : str
            æ”¶ç›Šå‘¨æœŸ
            
        Returns:
        --------
        Dict
            æ¼‚ç§»æ£€æµ‹æŠ¥å‘Š
        """
        print(f"\nğŸ” æ¼‚ç§»æ£€æµ‹: {factor_name} @ {period}")
        
        report = {
            'factor': factor_name,
            'period': period,
            'timestamp': datetime.now().isoformat(),
            'threshold': self.drift_threshold,
            'checks': {},
            'overall_pass': True
        }
        
        # 1. IC æ¯”è¾ƒ
        key = (factor_name, f'ret_{period}')
        
        train_ic_summary = train_results.get('ic_summary', {}).get(key, {})
        valid_ic_summary = valid_results.get('ic_summary', {}).get(key, {})
        test_ic_summary = test_results.get('ic_summary', {}).get(key, {})
        
        ic_comparison = self.compare_ic_summaries(train_ic_summary, valid_ic_summary, test_ic_summary)
        report['checks']['ic_comparison'] = ic_comparison
        
        # æ£€æŸ¥ IC æ¼‚ç§»
        ic_drift = ic_comparison.get('mean', {}).get('drift_detected', False)
        icir_drift = ic_comparison.get('icir', {}).get('drift_detected', False)
        
        if ic_drift:
            print(f"   âš ï¸  IC å‡å€¼æ¼‚ç§»: Valid vs Test > {self.drift_threshold:.0%}")
            report['overall_pass'] = False
        
        if icir_drift:
            print(f"   âš ï¸  ICIR æ¼‚ç§»: Valid vs Test > {self.drift_threshold:.0%}")
            report['overall_pass'] = False
        
        # 2. Spread æ¯”è¾ƒ
        train_spread = train_results.get('spreads', {}).get(key, pd.Series())
        valid_spread = valid_results.get('spreads', {}).get(key, pd.Series())
        test_spread = test_results.get('spreads', {}).get(key, pd.Series())
        
        spread_comparison = self.compare_spreads(train_spread, valid_spread, test_spread)
        report['checks']['spread_comparison'] = spread_comparison
        
        # æ£€æŸ¥ Spread æ¼‚ç§»
        spread_drift = spread_comparison.get('spread_mean', {}).get('drift_detected', False)
        
        if spread_drift:
            print(f"   âš ï¸  Spread å‡å€¼æ¼‚ç§»: Valid vs Test > {self.drift_threshold:.0%}")
            report['overall_pass'] = False
        
        # 3. ç»Ÿè®¡æ£€éªŒ
        train_ic_series = train_results.get('daily_ic', pd.DataFrame())
        valid_ic_series = valid_results.get('daily_ic', pd.DataFrame())
        test_ic_series = test_results.get('daily_ic', pd.DataFrame())
        
        if key in train_ic_series.columns and key in valid_ic_series.columns and key in test_ic_series.columns:
            valid_test_test = self.statistical_test_ic(
                valid_ic_series[key], 
                test_ic_series[key],
                test_type='ks'
            )
            report['checks']['statistical_test'] = valid_test_test
            
            if valid_test_test.get('significant', False):
                print(f"   âš ï¸  IC åˆ†å¸ƒæ˜¾è‘—ä¸åŒ (KS p={valid_test_test['p_value']:.4f})")
        
        # æ€»ç»“
        if report['overall_pass']:
            print(f"   âœ… æ¼‚ç§»æ£€æµ‹é€šè¿‡")
        else:
            print(f"   âŒ æ¼‚ç§»æ£€æµ‹æœªé€šè¿‡")
        
        return report
    
    def generate_drift_report(self,
                             drift_reports: List[Dict],
                             output_dir: str) -> str:
        """
        ç”Ÿæˆæ¼‚ç§»æŠ¥å‘Š
        
        Parameters:
        -----------
        drift_reports : List[Dict]
            æ¼‚ç§»æ£€æµ‹ç»“æœåˆ—è¡¨
        output_dir : str
            è¾“å‡ºç›®å½•
            
        Returns:
        --------
        str
            æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. ä¿å­˜ JSON æŠ¥å‘Š
        json_path = os.path.join(output_dir, 'drift_report.json')
        
        # è½¬æ¢ numpy ç±»å‹ä¸º Python åŸç”Ÿç±»å‹
        def convert_to_native(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, pd.Series):
                return obj.to_dict()
            elif isinstance(obj, dict):
                return {k: convert_to_native(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_to_native(i) for i in obj]
            return obj
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(convert_to_native(drift_reports), f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“Š æ¼‚ç§»æŠ¥å‘Šå·²ä¿å­˜: {json_path}")
        
        # 2. ç”Ÿæˆæ±‡æ€» CSV
        summary_rows = []
        for report in drift_reports:
            row = {
                'factor': report['factor'],
                'period': report['period'],
                'overall_pass': report['overall_pass']
            }
            
            # IC æ¯”è¾ƒ
            ic_comp = report.get('checks', {}).get('ic_comparison', {})
            for metric in ['mean', 'icir']:
                m = ic_comp.get(metric, {})
                row[f'ic_{metric}_train'] = m.get('train')
                row[f'ic_{metric}_valid'] = m.get('valid')
                row[f'ic_{metric}_test'] = m.get('test')
                row[f'ic_{metric}_drift_pct'] = m.get('valid_vs_test_pct')
            
            # Spread æ¯”è¾ƒ
            spread_comp = report.get('checks', {}).get('spread_comparison', {})
            for metric in ['spread_mean', 'spread_sharpe']:
                m = spread_comp.get(metric, {})
                row[f'{metric}_train'] = m.get('train')
                row[f'{metric}_valid'] = m.get('valid')
                row[f'{metric}_test'] = m.get('test')
                row[f'{metric}_drift_pct'] = m.get('valid_vs_test_pct')
            
            summary_rows.append(row)
        
        csv_path = os.path.join(output_dir, 'split_comparison.csv')
        pd.DataFrame(summary_rows).to_csv(csv_path, index=False)
        print(f"ğŸ“Š åˆ†å‰²å¯¹æ¯”å·²ä¿å­˜: {csv_path}")
        
        # 3. ç”Ÿæˆ HTML æŠ¥å‘Š
        html_path = self._generate_html_report(drift_reports, output_dir)
        
        return json_path
    
    def _generate_html_report(self, drift_reports: List[Dict], output_dir: str) -> str:
        """ç”Ÿæˆ HTML æ¼‚ç§»æŠ¥å‘Š"""
        html_path = os.path.join(output_dir, 'drift_tearsheet.html')
        
        # æ„å»º HTML
        html = """<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ¼‚ç§»æ£€æµ‹æŠ¥å‘Š</title>
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; margin: 20px; background: #f5f5f5; }
        .container { max-width: 1200px; margin: 0 auto; background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
        h1 { color: #333; border-bottom: 2px solid #007bff; padding-bottom: 10px; }
        h2 { color: #555; margin-top: 30px; }
        table { width: 100%; border-collapse: collapse; margin: 20px 0; }
        th, td { padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }
        th { background: #007bff; color: white; }
        tr:hover { background: #f1f1f1; }
        .pass { color: #28a745; font-weight: bold; }
        .fail { color: #dc3545; font-weight: bold; }
        .warning { color: #ffc107; font-weight: bold; }
        .metric-card { background: #f8f9fa; padding: 15px; border-radius: 8px; margin: 10px 0; }
        .summary { display: flex; gap: 20px; flex-wrap: wrap; }
        .summary-item { flex: 1; min-width: 200px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 8px; text-align: center; }
        .summary-item h3 { margin: 0; font-size: 2em; }
        .summary-item p { margin: 5px 0 0; opacity: 0.9; }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ“Š æ¼‚ç§»æ£€æµ‹æŠ¥å‘Š</h1>
        <p>ç”Ÿæˆæ—¶é—´: """ + datetime.now().strftime('%Y-%m-%d %H:%M:%S') + """</p>
        <p>æ¼‚ç§»é˜ˆå€¼: """ + f"{self.drift_threshold:.0%}" + """</p>
        
        <div class="summary">
            <div class="summary-item">
                <h3>""" + str(len(drift_reports)) + """</h3>
                <p>æ£€æµ‹å› å­æ•°</p>
            </div>
            <div class="summary-item">
                <h3>""" + str(sum(1 for r in drift_reports if r['overall_pass'])) + """</h3>
                <p>é€šè¿‡æ•°</p>
            </div>
            <div class="summary-item">
                <h3>""" + str(sum(1 for r in drift_reports if not r['overall_pass'])) + """</h3>
                <p>æœªé€šè¿‡æ•°</p>
            </div>
        </div>
        
        <h2>è¯¦ç»†ç»“æœ</h2>
        <table>
            <tr>
                <th>å› å­</th>
                <th>å‘¨æœŸ</th>
                <th>ICå‡å€¼ (Train/Valid/Test)</th>
                <th>ICIR (Train/Valid/Test)</th>
                <th>Valid vs Test å·®å¼‚</th>
                <th>çŠ¶æ€</th>
            </tr>
"""
        
        for report in drift_reports:
            ic_comp = report.get('checks', {}).get('ic_comparison', {})
            ic_mean = ic_comp.get('mean', {})
            icir = ic_comp.get('icir', {})
            
            status_class = 'pass' if report['overall_pass'] else 'fail'
            status_text = 'âœ… é€šè¿‡' if report['overall_pass'] else 'âŒ æœªé€šè¿‡'
            
            drift_pct = ic_mean.get('valid_vs_test_pct', 0)
            drift_text = f"{drift_pct*100:.1f}%" if drift_pct and not np.isnan(drift_pct) else 'N/A'
            
            html += f"""
            <tr>
                <td>{report['factor']}</td>
                <td>{report['period']}</td>
                <td>{ic_mean.get('train', 'N/A'):.4f} / {ic_mean.get('valid', 'N/A'):.4f} / {ic_mean.get('test', 'N/A'):.4f}</td>
                <td>{icir.get('train', 'N/A'):.4f} / {icir.get('valid', 'N/A'):.4f} / {icir.get('test', 'N/A'):.4f}</td>
                <td>{drift_text}</td>
                <td class="{status_class}">{status_text}</td>
            </tr>
"""
        
        html += """
        </table>
        
        <h2>è¯´æ˜</h2>
        <div class="metric-card">
            <p><strong>æ¼‚ç§»æ£€æµ‹æ ‡å‡†ï¼š</strong></p>
            <ul>
                <li>Valid vs Test çš„ IC/ICIR/Spread å·®å¼‚ < 20%</li>
                <li>IC åˆ†å¸ƒç»Ÿè®¡æ£€éªŒæ— æ˜¾è‘—å·®å¼‚ (p > 0.05)</li>
            </ul>
            <p><strong>çº¢çº¿æ ‡å‡†ï¼ˆè§¦å‘å›æ»šï¼‰ï¼š</strong></p>
            <ul>
                <li>æµ‹è¯•é›† Spread â‰¤ 0</li>
                <li>ICIR æ˜¾è‘—å›è½ > 50%</li>
            </ul>
        </div>
    </div>
</body>
</html>
"""
        
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html)
        
        print(f"ğŸ“Š HTML æŠ¥å‘Šå·²ä¿å­˜: {html_path}")
        
        return html_path


def compare_splits_with_analyzer(
    factors: pd.DataFrame,
    forward_returns: pd.DataFrame,
    train_idx: pd.Index,
    valid_idx: pd.Index,
    test_idx: pd.Index,
    output_dir: str,
    drift_threshold: float = 0.2
) -> Dict:
    """
    ä¾¿æ·å‡½æ•°ï¼šä½¿ç”¨ CrossSectionAnalyzer åˆ†æå„ä¸ªåˆ†å‰²å¹¶æ¯”è¾ƒ
    
    Parameters:
    -----------
    factors : pd.DataFrame
        å› å­æ•°æ®ï¼ŒMultiIndex [date, ticker]
    forward_returns : pd.DataFrame
        è¿œæœŸæ”¶ç›Šç‡
    train_idx, valid_idx, test_idx : pd.Index
        åˆ†å‰²ç´¢å¼•
    output_dir : str
        è¾“å‡ºç›®å½•
    drift_threshold : float
        æ¼‚ç§»é˜ˆå€¼
        
    Returns:
    --------
    Dict
        æ¼‚ç§»æ£€æµ‹ç»“æœ
    """
    from evaluation.cross_section_analyzer import CrossSectionAnalyzer
    
    # åˆ†åˆ«åˆ›å»ºå„åˆ†å‰²çš„åˆ†æå™¨
    print("\n" + "=" * 70)
    print("åˆ†å‰²å¯¹æ¯”åˆ†æ")
    print("=" * 70)
    
    # Train
    print("\nğŸ“Š åˆ†æ Train é›†...")
    train_analyzer = CrossSectionAnalyzer(
        factors=factors.loc[train_idx],
        forward_returns=forward_returns.loc[train_idx]
    )
    train_analyzer.analyze()
    train_results = train_analyzer.results
    
    # Valid
    print("\nğŸ“Š åˆ†æ Valid é›†...")
    valid_analyzer = CrossSectionAnalyzer(
        factors=factors.loc[valid_idx],
        forward_returns=forward_returns.loc[valid_idx]
    )
    valid_analyzer.analyze()
    valid_results = valid_analyzer.results
    
    # Test
    print("\nğŸ“Š åˆ†æ Test é›†...")
    test_analyzer = CrossSectionAnalyzer(
        factors=factors.loc[test_idx],
        forward_returns=forward_returns.loc[test_idx]
    )
    test_analyzer.analyze()
    test_results = test_analyzer.results
    
    # æ¼‚ç§»æ£€æµ‹
    detector = DriftDetector(drift_threshold=drift_threshold)
    
    drift_reports = []
    for factor_col in factors.columns:
        for ret_col in forward_returns.columns:
            period = ret_col.replace('ret_', '')
            report = detector.detect_drift(
                train_results, valid_results, test_results,
                factor_name=factor_col,
                period=period
            )
            drift_reports.append(report)
    
    # ç”ŸæˆæŠ¥å‘Š
    detector.generate_drift_report(drift_reports, output_dir)
    
    return {
        'train_results': train_results,
        'valid_results': valid_results,
        'test_results': test_results,
        'drift_reports': drift_reports
    }


if __name__ == '__main__':
    """æµ‹è¯•ä»£ç """
    print("=" * 70)
    print("æ¼‚ç§»æ£€æµ‹æ¨¡å—æµ‹è¯•")
    print("=" * 70)
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    np.random.seed(42)
    
    dates = pd.date_range('2020-01-01', '2024-12-31', freq='D')
    dates = dates[dates.dayofweek < 5]
    tickers = ['000001', '000002', '000003', '000004', '000005']
    
    index = pd.MultiIndex.from_product([dates, tickers], names=['date', 'ticker'])
    
    # æ¨¡æ‹Ÿå› å­å’Œæ”¶ç›Š
    factors = pd.DataFrame({
        'factor_1': np.random.randn(len(index))
    }, index=index)
    
    forward_returns = pd.DataFrame({
        'ret_5d': np.random.randn(len(index)) * 0.05
    }, index=index)
    
    # æ¨¡æ‹Ÿåˆ†å‰²
    n = len(dates)
    train_end = int(n * 0.6)
    valid_end = int(n * 0.8)
    
    train_dates = dates[:train_end]
    valid_dates = dates[train_end:valid_end]
    test_dates = dates[valid_end:]
    
    train_idx = factors.index[factors.index.get_level_values('date').isin(train_dates)]
    valid_idx = factors.index[factors.index.get_level_values('date').isin(valid_dates)]
    test_idx = factors.index[factors.index.get_level_values('date').isin(test_dates)]
    
    print(f"\nğŸ“Š æµ‹è¯•æ•°æ®:")
    print(f"   Train: {len(train_idx)} æ ·æœ¬")
    print(f"   Valid: {len(valid_idx)} æ ·æœ¬")
    print(f"   Test: {len(test_idx)} æ ·æœ¬")
    
    # æµ‹è¯•æ¼‚ç§»æ£€æµ‹
    output_dir = os.path.join(ml_root, 'ML output', 'reports', 'test_drift')
    
    try:
        results = compare_splits_with_analyzer(
            factors, forward_returns,
            train_idx, valid_idx, test_idx,
            output_dir
        )
        print("\nâœ… æ¼‚ç§»æ£€æµ‹æµ‹è¯•å®Œæˆï¼")
    except Exception as e:
        print(f"\nâš ï¸  æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
