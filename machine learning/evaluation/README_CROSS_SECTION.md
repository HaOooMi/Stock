# æ¨ªæˆªé¢è¯„ä¼°æ¡†æ¶ï¼ˆAlphalensé£æ ¼ï¼‰

## ğŸ“– ç®€ä»‹

æœ¬æ¡†æ¶æä¾›äº†å®Œæ•´çš„æ¨ªæˆªé¢å› å­è¯„ä¼°å·¥å…·ï¼Œéµå¾ªAlphalensé£æ ¼ï¼Œä¸“ä¸ºé‡åŒ–æŠ•èµ„ä¸­çš„å› å­ç ”ç©¶è®¾è®¡ã€‚å®Œå…¨ç¬¦åˆã€Šæœºå™¨å­¦ä¹ é‡åŒ–ç ”ç©¶å®ªç«  v1.0ã€‹ç¬¬2ç« è¦æ±‚ã€‚

### ğŸ¯ å®ç°æ¦‚è§ˆ

å·²æˆåŠŸæ„å»ºå®Œæ•´çš„æ¨ªæˆªé¢å› å­è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å«**6ä¸ªæ ¸å¿ƒæ¨¡å—**å’Œ**1ä¸ªç¤ºä¾‹è„šæœ¬**ï¼Œå…±çº¦2500è¡Œä»£ç ã€‚

### âœ¨ æ ¸å¿ƒç‰¹æ€§

âœ… **å®Œæ•´çš„ICåˆ†æ**
- æ¯æ—¥æ¨ªæˆªé¢Rank ICï¼ˆSpearmanç›¸å…³ï¼‰
- ICç»Ÿè®¡æ‘˜è¦ï¼ˆå‡å€¼ã€æ ‡å‡†å·®ã€ICIRã€tæ£€éªŒã€p-valueï¼‰
- ICèƒœç‡ï¼ˆæ­£ICæ¯”ä¾‹ï¼‰
- ICæ—¶é—´åºåˆ—å¯è§†åŒ–

âœ… **åˆ†ä½æ•°ç»„åˆåˆ†æ**
- 5åˆ†ä½ç­‰é¢‘åˆ†æ¡¶ï¼ˆæ¨ªæˆªé¢ï¼‰
- è®¡ç®—å„åˆ†ä½æ•°çš„è¿œæœŸæ”¶ç›Š
- ç´¯è®¡å‡€å€¼æ›²çº¿
- å•è°ƒæ€§æ£€éªŒï¼ˆKendall Ï„ï¼‰

âœ… **Spreadåˆ†æ**
- Top-Mean Spreadï¼ˆå®ç›˜æ¨èï¼‰
- Top-Bottom Spreadï¼ˆå­¦æœ¯å¸¸è§ï¼‰
- Spreadç´¯è®¡æ”¶ç›Šä¸å¤æ™®æ¯”
- å¹´åŒ–å¤æ™®æ¯”è®¡ç®—

âœ… **å› å­é¢„å¤„ç†**
- Winsorizeï¼ˆ1%-99%æå€¼å¤„ç†ï¼‰
- Z-scoreæ ‡å‡†åŒ–ï¼ˆæ¨ªæˆªé¢ï¼‰
- å¸‚å€¼ä¸­æ€§åŒ–ï¼ˆå›å½’æ®‹å·®æ³•ï¼‰
- è¡Œä¸šä¸­æ€§åŒ–ï¼ˆå›å½’æ®‹å·®æ³•ï¼‰
- ç»¼åˆä¸­æ€§åŒ–ï¼ˆå¸‚å€¼+è¡Œä¸šï¼‰

âœ… **æ¢æ‰‹ç‡åˆ†æ**
- Topåˆ†ä½æ•°æ¢æ‰‹ç‡è·Ÿè¸ª
- æ¢æ‰‹ç‡æ—¶é—´åºåˆ—
- æŒä»“å˜åŒ–ç»Ÿè®¡

âœ… **å®Œå–„çš„å¯è§†åŒ–**ï¼ˆ7ç§ä¸“ä¸šå›¾è¡¨ï¼‰
- ICæ—¶é—´åºåˆ—å›¾ï¼ˆèµ°å»Šå›¾ï¼Œå«Â±1ÏƒåŒºé—´ï¼‰
- ICåˆ†å¸ƒç›´æ–¹å›¾ï¼ˆå«æ­£æ€æ‹Ÿåˆï¼‰
- åˆ†ä½æ•°ç´¯è®¡æ”¶ç›Šå›¾ï¼ˆå½©è‰²å‡€å€¼æ›²çº¿ï¼‰
- åˆ†ä½æ•°å¹³å‡æ”¶ç›ŠæŸ±çŠ¶å›¾
- Spreadç´¯è®¡æ”¶ç›Šå›¾
- æ¢æ‰‹ç‡æ—¶é—´åºåˆ—å›¾
- æœˆåº¦ICçƒ­åŠ›å›¾

âœ… **ä¸“ä¸šæŠ¥å‘Šè¾“å‡º**
- HTMLæ ¼å¼TearsheetæŠ¥å‘Šï¼ˆå“åº”å¼è®¾è®¡ï¼‰
- è‡ªåŠ¨å› å­è¯„ä¼°ï¼ˆä¼˜ç§€/åˆæ ¼/å¼±ï¼‰
- CSVæ•°æ®å¯¼å‡º
- å›¾è¡¨è‡ªåŠ¨ç”Ÿæˆï¼ˆ300 DPIï¼‰

---

## ğŸ“¦ æ¨¡å—æ¶æ„

### æ ¸å¿ƒæ¨¡å—æ¸…å•

| æ¨¡å— | æ–‡ä»¶ | åŠŸèƒ½ | ä»£ç é‡ |
|------|------|------|--------|
| æ ¸å¿ƒåº¦é‡ | `cross_section_metrics.py` | Forward Returns, IC, ICIR, Spread, æ¢æ‰‹ç‡ç­‰ | ~600è¡Œ |
| å› å­é¢„å¤„ç† | `factor_preprocessing.py` | Winsorize, æ ‡å‡†åŒ–, ä¸­æ€§åŒ– | ~400è¡Œ |
| åˆ†æå™¨ | `cross_section_analyzer.py` | ç»Ÿä¸€è¯„ä¼°æ¥å£, æµç¨‹ç¼–æ’ | ~500è¡Œ |
| å¯è§†åŒ– | `visualization.py` | 7ç§ä¸“ä¸šå›¾è¡¨ | ~600è¡Œ |
| æŠ¥å‘Šç”Ÿæˆ | `tearsheet.py` | HTMLæŠ¥å‘Š, CSVå¯¼å‡º | ~400è¡Œ |
| ç¤ºä¾‹è„šæœ¬ | `run_cross_section_analysis.py` | ç«¯åˆ°ç«¯ç¤ºä¾‹ | ~300è¡Œ |

### è¾“å‡ºç›®å½•ç»“æ„

```
ML output/reports/baseline_vX/factors/
â””â”€â”€ {factor_name}/
    â”œâ”€â”€ tearsheet_{factor_name}_{period}.html      # HTMLç»¼åˆæŠ¥å‘Š
    â”œâ”€â”€ ic_{factor_name}_{period}.csv              # ICæ—¶é—´åºåˆ—
    â”œâ”€â”€ quantile_returns_{factor_name}_{period}.csv # åˆ†ä½æ•°æ”¶ç›Š
    â”œâ”€â”€ ic_series_{factor_name}_{period}.png       # ICèµ°å»Šå›¾
    â”œâ”€â”€ ic_dist_{factor_name}_{period}.png         # ICåˆ†å¸ƒ
    â”œâ”€â”€ quantile_cumret_{factor_name}_{period}.png # ç´¯è®¡æ”¶ç›Š
    â”œâ”€â”€ quantile_meanret_{factor_name}_{period}.png # å¹³å‡æ”¶ç›Š
    â”œâ”€â”€ spread_cumret_{factor_name}_{period}.png   # Spread
    â”œâ”€â”€ ic_heatmap_{factor_name}_{period}.png      # ICçƒ­åŠ›å›¾
    â””â”€â”€ turnover_{factor_name}.png                  # æ¢æ‰‹ç‡
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºæœ¬ä½¿ç”¨

```python
from evaluation import CrossSectionAnalyzer

# åˆ›å»ºåˆ†æå™¨
analyzer = CrossSectionAnalyzer(
    factors=factors_df,      # å› å­å€¼ï¼ŒMultiIndex[date, ticker]
    prices=prices_df,        # ä»·æ ¼æ•°æ®
    market_cap=mktcap_df,    # å¸‚å€¼ï¼ˆå¯é€‰ï¼‰
    industry=industry_df     # è¡Œä¸šï¼ˆå¯é€‰ï¼‰
)

# é¢„å¤„ç†
analyzer.preprocess(
    winsorize=True,
    standardize=True,
    neutralize=True  # å¸‚å€¼+è¡Œä¸šä¸­æ€§åŒ–
)

# è®¡ç®—è¿œæœŸæ”¶ç›Š
analyzer.calculate_returns(periods=[1, 5, 10, 20])

# æ‰§è¡Œåˆ†æ
analyzer.analyze(
    n_quantiles=5,
    ic_method='spearman',
    spread_method='top_minus_mean'
)

# æŸ¥çœ‹ç»“æœ
analyzer.summary()
```

### 2. ç”Ÿæˆå®Œæ•´æŠ¥å‘Š

```python
from evaluation.visualization import create_factor_tearsheet_plots
from evaluation.tearsheet import generate_full_tearsheet

# è·å–ç»“æœ
results = analyzer.get_results()

# ç”Ÿæˆå›¾è¡¨
plot_paths = create_factor_tearsheet_plots(
    results,
    factor_name='factor_momentum',
    return_period='ret_5d',
    output_dir='./output/factors'
)

# ç”ŸæˆHTMLæŠ¥å‘Šå’ŒCSV
generate_full_tearsheet(
    results,
    factor_name='factor_momentum',
    return_period='ret_5d',
    output_dir='./output/factors',
    plot_paths=plot_paths
)
```

### 3. è¿è¡Œç¤ºä¾‹è„šæœ¬

```bash
cd "machine learning/pipelines"
python run_cross_section_analysis.py
```

---

## ğŸ“˜ è¯¦ç»†ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šç«¯åˆ°ç«¯å®Œæ•´æµç¨‹

```python
from evaluation import CrossSectionAnalyzer
from evaluation.tearsheet import generate_full_tearsheet
import pandas as pd

# 1. å‡†å¤‡æ•°æ®ï¼ˆMultiIndexæ ¼å¼ï¼‰
# å‡è®¾å·²åŠ è½½: factors_df, prices_df, market_cap_df, industry_df

# 2. åˆ›å»ºåˆ†æå™¨
analyzer = CrossSectionAnalyzer(
    factors=factors_df,              # MultiIndex[date, ticker]
    prices=prices_df,                # MultiIndex[date, ticker]
    market_cap=market_cap_df,        # å¯é€‰
    industry=industry_df,            # å¯é€‰
    tradable_mask=tradable_mask_df,  # å¯é€‰
    forward_periods=[1, 5, 10, 20],  # è®¡ç®—1/5/10/20æ—¥è¿œæœŸæ”¶ç›Š
    quantiles=5,                     # 5åˆ†ä½
    return_type='simple'             # 'simple' æˆ– 'log'
)

# 3. å› å­é¢„å¤„ç†ï¼ˆé“¾å¼è°ƒç”¨ï¼‰
analyzer.preprocess(
    winsorize=True,
    standardize=True,
    neutralize=True
)

# 4. æ‰§è¡Œåˆ†æ
results = analyzer.analyze()

# 5. æŸ¥çœ‹æ‘˜è¦
analyzer.summary()

# 6. ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
generate_full_tearsheet(
    results,
    factor_name='momentum_factor',
    output_dir='./output/factors',
    show_plots=True
)
```

**è¾“å‡ºå†…å®¹**ï¼š
- `momentum_factor_tearsheet.html` - HTMLç»¼åˆæŠ¥å‘Š
- `momentum_factor_ic.csv` - ICæ—¶é—´åºåˆ—
- `momentum_factor_quantile_returns.csv` - åˆ†ä½æ•°æ”¶ç›Š
- `*.png` - 7å¼ é«˜æ¸…å›¾è¡¨ï¼ˆ300 DPIï¼‰

---

### ç¤ºä¾‹2ï¼šæ¨¡å—åŒ–ä½¿ç”¨ï¼ˆä»…è®¡ç®—ICï¼‰

```python
from evaluation.cross_section_metrics import (
    calculate_forward_returns,
    calculate_daily_ic,
    calculate_ic_summary
)

# 1. è®¡ç®—5æ—¥è¿œæœŸæ”¶ç›Š
forward_returns_5d = calculate_forward_returns(
    prices_df,
    periods=5,
    return_type='simple'
)

# 2. è®¡ç®—æ¯æ—¥IC
daily_ic = calculate_daily_ic(
    factors_df,
    forward_returns_5d,
    method='spearman'  # æˆ– 'pearson'
)

# 3. ICç»Ÿè®¡æ‘˜è¦
ic_summary = calculate_ic_summary(daily_ic, annualize=True, periods_per_year=252)

# 4. è¾“å‡ºç»“æœ
print(f"ICå‡å€¼: {ic_summary['ic_mean']:.4f}")
print(f"ICæ ‡å‡†å·®: {ic_summary['ic_std']:.4f}")
print(f"ICIR: {ic_summary['ic_ir']:.4f}")
print(f"ICIR(å¹´åŒ–): {ic_summary['ic_ir_annual']:.4f}")
print(f"ICèƒœç‡: {ic_summary['ic_win_rate']:.2%}")
print(f"tç»Ÿè®¡é‡: {ic_summary['t_stat']:.2f}")
print(f"p-value: {ic_summary['p_value']:.4f}")
```

---

### ç¤ºä¾‹3ï¼šåˆ†ä½æ•°æ”¶ç›Šåˆ†æ

```python
from evaluation.cross_section_metrics import (
    calculate_quantile_returns,
    calculate_cumulative_returns,
    calculate_spread,
    calculate_monotonicity
)

# 1. è®¡ç®—10åˆ†ä½æ”¶ç›Š
quantile_returns = calculate_quantile_returns(
    factors_df,
    forward_returns_5d,
    quantiles=10,
    labels=[f'Q{i}' for i in range(1, 11)]
)

# 2. è®¡ç®—ç´¯è®¡æ”¶ç›Šï¼ˆå‡€å€¼æ›²çº¿ï¼‰
cumulative_returns = calculate_cumulative_returns(quantile_returns)

# 3. è®¡ç®—Spread
spread_top_mean = calculate_spread(
    quantile_returns,
    method='top_minus_mean'
)
spread_top_bottom = calculate_spread(
    quantile_returns,
    method='top_minus_bottom'
)

# 4. å•è°ƒæ€§æ£€éªŒ
monotonicity = calculate_monotonicity(quantile_returns)
print(f"Kendall Ï„: {monotonicity['kendall_tau']:.4f}")
print(f"p-value: {monotonicity['kendall_p']:.4f}")
print(f"å•è°ƒé¡ºåºæ¯”ä¾‹: {monotonicity['correct_order_ratio']:.2%}")

# 5. Spreadå¤æ™®æ¯”
spread_sharpe = spread_top_mean.mean() / spread_top_mean.std() * (252 ** 0.5)
print(f"Spread Sharpe(å¹´åŒ–): {spread_sharpe:.2f}")
```

---

### ç¤ºä¾‹4ï¼šå› å­é¢„å¤„ç†è¯¦è§£

```python
from evaluation.factor_preprocessing import (
    winsorize_factor,
    standardize_factor,
    neutralize_factor,
    preprocess_factor_pipeline
)

# æ–¹å¼1: åˆ†æ­¥å¤„ç†
# Step 1: Winsorize
factors_winsorized = winsorize_factor(
    factors_df,
    lower_quantile=0.01,
    upper_quantile=0.99,
    cross_section=True  # æŒ‰æ—¥æ¨ªæˆªé¢
)

# Step 2: æ ‡å‡†åŒ–
factors_standardized = standardize_factor(
    factors_winsorized,
    method='z_score',  # 'z_score', 'min_max', æˆ– 'rank'
    cross_section=True
)

# Step 3: ä¸­æ€§åŒ–
factors_neutralized = neutralize_factor(
    factors_standardized,
    market_cap=market_cap_df,
    industry=industry_df,
    neutralize_market_cap=True,
    neutralize_industry=True
)

# æ–¹å¼2: ä¸€é”®æµæ°´çº¿ï¼ˆæ¨èï¼‰
factors_processed = preprocess_factor_pipeline(
    factors_df,
    market_cap=market_cap_df,
    industry=industry_df,
    winsorize=True,
    standardize=True,
    neutralize=True,
    winsorize_params={
        'lower_quantile': 0.01,
        'upper_quantile': 0.99,
        'cross_section': True
    },
    standardize_params={
        'method': 'z_score',
        'cross_section': True
    },
    neutralize_params={
        'neutralize_market_cap': True,
        'neutralize_industry': True
    }
)
```

---

### ç¤ºä¾‹5ï¼šæ‰¹é‡å› å­è¯„ä¼°

```python
from evaluation import CrossSectionAnalyzer
from evaluation.tearsheet import generate_full_tearsheet

# å› å­åˆ—è¡¨
factor_names = ['momentum', 'value', 'quality', 'volatility', 'size']

# æ‰¹é‡è¯„ä¼°
results_dict = {}

for factor_name in factor_names:
    print(f"\n{'='*50}")
    print(f"æ­£åœ¨è¯„ä¼°å› å­: {factor_name}")
    print(f"{'='*50}")
    
    # æå–å•ä¸ªå› å­
    factor_single = factors_df[[factor_name]].copy()
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = CrossSectionAnalyzer(
        factors=factor_single,
        prices=prices_df,
        market_cap=market_cap_df,
        industry=industry_df,
        forward_periods=[5],
        quantiles=5
    )
    
    # é¢„å¤„ç†+åˆ†æ
    results = analyzer.preprocess(
        winsorize=True,
        standardize=True,
        neutralize=True
    ).analyze()
    
    # å­˜å‚¨ç»“æœ
    results_dict[factor_name] = results
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_full_tearsheet(
        results,
        factor_name=factor_name,
        output_dir=f'./output/factors/{factor_name}',
        show_plots=False
    )
    
    # å¿«é€Ÿè¯„ä¼°
    ic_summary = results['ic_summary_5']
    ic_mean = ic_summary['ic_mean']
    icir = ic_summary['ic_ir']
    
    if icir > 1.5 and ic_mean > 0.02:
        print(f"âœ… {factor_name}: ä¼˜ç§€å› å­ (IC={ic_mean:.4f}, ICIR={icir:.2f})")
    elif icir > 0.5 and ic_mean > 0.01:
        print(f"âš ï¸  {factor_name}: åˆæ ¼å› å­ (IC={ic_mean:.4f}, ICIR={icir:.2f})")
    else:
        print(f"âŒ {factor_name}: å¼±å› å­ (IC={ic_mean:.4f}, ICIR={icir:.2f})")

# æ±‡æ€»å¯¹æ¯”
import pandas as pd
summary_df = pd.DataFrame({
    factor: {
        'ICå‡å€¼': results_dict[factor]['ic_summary_5']['ic_mean'],
        'ICIR': results_dict[factor]['ic_summary_5']['ic_ir'],
        'ICèƒœç‡': results_dict[factor]['ic_summary_5']['ic_win_rate'],
        'Spreadå‡å€¼': results_dict[factor]['spreads_5']['top_minus_mean'].mean()
    }
    for factor in factor_names
}).T

print("\nå› å­å¯¹æ¯”æ±‡æ€»:")
print(summary_df.round(4))
```

---

### ç¤ºä¾‹6ï¼šä¸ç°æœ‰Pipelineé›†æˆ

```python
# åœ¨ train_models.py æˆ–å…¶ä»–pipelineä¸­é›†æˆ

from data.data_loader import load_market_data
from evaluation import CrossSectionAnalyzer

# 1. åŠ è½½ç‰¹å¾ï¼ˆæ¥è‡ªä½ çš„æ•°æ®åŠ è½½å™¨ï¼‰
features_df = load_market_data(...)  # è¿”å› MultiIndex[date, ticker]

# 2. é€‰æ‹©è¦è¯„ä¼°çš„ç‰¹å¾åˆ—
target_features = ['PE_ratio', 'PB_ratio', 'ROE', 'momentum_20d']

# 3. é€ä¸ªè¯„ä¼°ç‰¹å¾æœ‰æ•ˆæ€§
qualified_features = []

for feature_name in target_features:
    factor = features_df[[feature_name]].copy()
    
    analyzer = CrossSectionAnalyzer(
        factors=factor,
        prices=prices_df,
        forward_periods=[5],
        quantiles=5
    )
    
    results = analyzer.analyze()
    ic_summary = results['ic_summary_5']
    
    # æ ¹æ®ICæ ‡å‡†ç­›é€‰
    if ic_summary['ic_ir'] > 0.5 and ic_summary['p_value'] < 0.05:
        qualified_features.append(feature_name)
        print(f"âœ… ä¿ç•™ç‰¹å¾: {feature_name}")
    else:
        print(f"âŒ å‰”é™¤ç‰¹å¾: {feature_name}")

# 4. ä½¿ç”¨ç­›é€‰åçš„ç‰¹å¾ç»§ç»­è®­ç»ƒ
features_filtered = features_df[qualified_features]
# ... ç»§ç»­åç»­çš„æ¨¡å‹è®­ç»ƒæµç¨‹
```

---

### ç¤ºä¾‹7ï¼šè‡ªå®šä¹‰å¯è§†åŒ–

```python
from evaluation.visualization import (
    plot_ic_time_series,
    plot_ic_distribution,
    plot_quantile_cumulative_returns,
    plot_quantile_mean_returns,
    plot_spread_cumulative_returns,
    plot_turnover_time_series,
    plot_monthly_ic_heatmap
)

# è·å–åˆ†æç»“æœ
results = analyzer.get_results()

# å•ç‹¬ç»˜åˆ¶ICèµ°å»Šå›¾
fig = plot_ic_time_series(
    results['ic_series_5'],
    factor_name='momentum_factor',
    figsize=(14, 5)
)
fig.savefig('ic_corridor.png', dpi=300, bbox_inches='tight')

# å•ç‹¬ç»˜åˆ¶ICåˆ†å¸ƒ
fig = plot_ic_distribution(
    results['ic_series_5'],
    factor_name='momentum_factor'
)
fig.savefig('ic_distribution.png', dpi=300, bbox_inches='tight')

# ç»˜åˆ¶ç´¯è®¡æ”¶ç›Š
fig = plot_quantile_cumulative_returns(
    results['cumulative_returns_5'],
    factor_name='momentum_factor'
)
fig.savefig('quantile_cumulative.png', dpi=300, bbox_inches='tight')

# ç»˜åˆ¶ICçƒ­åŠ›å›¾
fig = plot_monthly_ic_heatmap(
    results['ic_series_5'],
    factor_name='momentum_factor'
)
fig.savefig('ic_heatmap.png', dpi=300, bbox_inches='tight')
```

---

## ğŸ“Š æ•°æ®æ ¼å¼è¦æ±‚

### è¾“å…¥æ•°æ®æ ¼å¼

æ‰€æœ‰è¾“å…¥DataFrameéƒ½éœ€è¦**MultiIndex[date, ticker]**æ ¼å¼ï¼š

```python
# ç¤ºä¾‹
import pandas as pd

dates = pd.date_range('2023-01-01', '2023-12-31', freq='D')
tickers = ['000001.SZ', '000002.SZ', '600000.SH']

index = pd.MultiIndex.from_product(
    [dates, tickers],
    names=['date', 'ticker']
)

# å› å­æ•°æ®
factors = pd.DataFrame({
    'factor_1': [...],
    'factor_2': [...]
}, index=index)

# ä»·æ ¼æ•°æ®
prices = pd.DataFrame({
    'close': [...]
}, index=index)

# å¸‚å€¼æ•°æ®ï¼ˆå¯é€‰ï¼‰
market_cap = pd.DataFrame({
    'market_cap': [...]
}, index=index)

# è¡Œä¸šæ•°æ®ï¼ˆå¯é€‰ï¼‰
industry = pd.DataFrame({
    'industry': ['é‡‘è', 'ç§‘æŠ€', ...]
}, index=index)
```

---

## ğŸ“ˆ æ ¸å¿ƒåº¦é‡è¯´æ˜

### 1. Forward Returnsï¼ˆè¿œæœŸæ”¶ç›Šï¼‰

**å…¬å¼**ï¼š
- **Simple Return**: $r_{t \rightarrow t+H} = \frac{P_{t+H}}{P_t} - 1$
- **Log Return**: $r_{t \rightarrow t+H} = \log(P_{t+H}) - \log(P_t)$

**å®ç°**ï¼š
```python
# cross_section_metrics.py
calculate_forward_returns(prices, periods=[1, 5, 10, 20], method='simple')
# ä½¿ç”¨ groupby(level='ticker').shift(-period) ä¿è¯æŒ‰è‚¡ç¥¨åˆ†ç»„è®¡ç®—
```

### 2. Rank ICï¼ˆæ’åºä¿¡æ¯ç³»æ•°ï¼‰

**å…¬å¼**ï¼šæ¯æ—¥æ¨ªæˆªé¢Spearmanç›¸å…³ç³»æ•°

$$\text{IC}_t = \text{Spearman}(\text{factor}_t, \text{forward\_return}_{t \rightarrow t+H})$$

**å®ç°**ï¼š
```python
calculate_daily_ic(factors, forward_returns, method='spearman')
# æ¯æ—¥æ¨ªæˆªé¢ç‹¬ç«‹è®¡ç®—ï¼Œä½¿ç”¨ scipy.stats.spearmanr()
# è‡ªåŠ¨è®¡ç®— p-value å’Œç»Ÿè®¡æ˜¾è‘—æ€§
```

**å®ªç« è¦æ±‚**ï¼š
- ç›®æ ‡å€¼: |Rank IC| â‰¥ 0.02
- ç»Ÿè®¡æ˜¾è‘—æ€§: p-value < 0.05

### 3. ICIRï¼ˆICä¿¡æ¯æ¯”ç‡ï¼‰

**å…¬å¼**ï¼š

$$\text{ICIR} = \frac{\text{Mean}(\text{IC})}{\text{Std}(\text{IC})}$$

å¹´åŒ–ï¼š$\text{ICIR}_{\text{annual}} = \text{ICIR} \times \sqrt{252}$

**å®ç°**ï¼š
```python
calculate_ic_summary(ic_series, annualize=True, periods_per_year=252)
# è¿”å›: mean, std, icir, icir_annual, t_stat, p_value, positive_ratio
```

**å®ªç« è¦æ±‚**ï¼š
- ç›®æ ‡å€¼: ICIR â‰¥ 0.5
- ä¼˜ç§€å€¼: ICIR â‰¥ 1.0

### 4. ICèƒœç‡

**å®šä¹‰**ï¼šæ—¥åº¦IC > 0çš„æ¯”ä¾‹

**å®ç°**ï¼š
```python
# åŒ…å«åœ¨ calculate_ic_summary() è¿”å›å€¼ä¸­
{
    'positive_ratio': (ic_clean > 0).sum() / len(ic_clean),
    ...
}
```

**å®ªç« è¦æ±‚**ï¼š
- ç›®æ ‡å€¼: ICèƒœç‡ â‰¥ 55%
- ä¼˜ç§€å€¼: ICèƒœç‡ â‰¥ 60%

### 5. åˆ†ä½æ•°æ”¶ç›Š

**å®ç°æµç¨‹**ï¼š
1. æ¯æ—¥æ¨ªæˆªé¢æŒ‰å› å­å€¼æ’åº
2. ä½¿ç”¨`pd.qcut()`ç­‰åˆ†ä½åˆ†ç»„ï¼ˆ5æ¡£ï¼‰
3. è®¡ç®—å„ç»„å¹³å‡æ”¶ç›Š
4. ç´¯è®¡è®¡ç®—å‡€å€¼æ›²çº¿

**ä»£ç **ï¼š
```python
calculate_quantile_returns(factors, forward_returns, n_quantiles=5)
# è¿”å›: DataFrame[date x quantile] çš„æ—¥æ”¶ç›Šç‡
```

### 6. Spread

**å…¬å¼**ï¼š
- **Top-Mean**: $\text{Spread} = R_{\text{top}} - \text{Mean}(R_{\text{all}})$ ï¼ˆå®ç›˜æ¨èï¼‰
- **Top-Bottom**: $\text{Spread} = R_{\text{top}} - R_{\text{bottom}}$ ï¼ˆå­¦æœ¯å¸¸è§ï¼‰

**å®ç°**ï¼š
```python
calculate_spread(quantile_returns, method='top_minus_mean')
# è¿”å›: æ¯æ—¥Spreadåºåˆ—
```

**å®ªç« è¦æ±‚**ï¼š
- æµ‹è¯•é›† Spread > 0ï¼ˆç¡¬çº¦æŸï¼‰
- Spread Sharpe(å¹´åŒ–) > 1.0

### 7. å•è°ƒæ€§ï¼ˆKendall Ï„ï¼‰

**å®šä¹‰**ï¼šæ£€éªŒåˆ†ä½æ•°æ”¶ç›Šæ˜¯å¦å•è°ƒé€’å¢

**å®ç°**ï¼š
```python
calculate_monotonicity(quantile_returns)
# è¿”å›: kendall_tau, kendall_p_value, correct_order_ratio
```

### 8. æ¢æ‰‹ç‡

**å…¬å¼**ï¼š

$$\text{Turnover}_t = 1 - \frac{|\text{Holdings}_t \cap \text{Holdings}_{t-1}|}{|\text{Holdings}_t|}$$

**å®ç°**ï¼š
```python
calculate_turnover(factors, quantile=4, n_quantiles=5)
# è¿½è¸ªTopåˆ†ä½æ•°æŒä»“å˜åŒ–
```

---

## ğŸ¯ å› å­é¢„å¤„ç†æµç¨‹

### æ ‡å‡†æµç¨‹ï¼ˆæ¨èï¼‰

#### 1. Winsorizeï¼ˆæå€¼å¤„ç†ï¼‰

**æ–¹æ³•**ï¼šæŒ‰æˆªé¢1%-99%åˆ†ä½æ•°è£å‰ª

**å®ç°**ï¼š
```python
# factor_preprocessing.py
winsorize_factor(factors, lower_quantile=0.01, upper_quantile=0.99, cross_section=True)

# æŒ‰æ—¥æ¨ªæˆªé¢å¤„ç†
for date in dates:
    lower = quantile(0.01)
    upper = quantile(0.99)
    factor_winsorized = factor.clip(lower, upper)
```

#### 2. Z-scoreæ ‡å‡†åŒ–ï¼ˆæ¨ªæˆªé¢ï¼‰

**å…¬å¼**ï¼šæŒ‰æ—¥æœŸæˆªé¢æ ‡å‡†åŒ–

$$z = \frac{x - \mu_{\text{cross}}}{\sigma_{\text{cross}}}$$

**å®ç°**ï¼š
```python
standardize_factor(factors, method='z_score', cross_section=True)

# æŒ‰æ—¥æ¨ªæˆªé¢å¤„ç†
for date in dates:
    mean = factor.mean()
    std = factor.std()
    factor_zscore = (factor - mean) / std
```

**å…¶ä»–æ ‡å‡†åŒ–æ–¹æ³•**ï¼š
- `'min_max'`: Min-Maxæ ‡å‡†åŒ–åˆ°[0, 1]
- `'rank'`: æ’åæ ‡å‡†åŒ–åˆ°[0, 1]

#### 3. ä¸­æ€§åŒ–ï¼ˆå›å½’æ®‹å·®æ³•ï¼‰

**æ–¹æ³•**ï¼šæˆªé¢å›å½’å–æ®‹å·®

$$\text{factor} \sim \alpha + \beta_1 \log(\text{mkt\_cap}) + \beta_2 \text{industry\_dummies}$$

**å®ç°**ï¼š
```python
neutralize_factor(factors, market_cap=market_cap, industry=industry)

# æŒ‰æ—¥æ¨ªæˆªé¢å¤„ç†
for date in dates:
    # æ„å»ºå›å½’
    X = [log(market_cap), industry_dummies]
    y = factor
    
    # OLS: Î² = (X'X)^-1 X'y
    beta = np.linalg.lstsq(X, y, rcond=None)[0]
    residuals = y - X @ beta
    
    factor_neutralized = residuals
```

**æ”¯æŒçš„ä¸­æ€§åŒ–**ï¼š
- å¸‚å€¼ä¸­æ€§åŒ–
- è¡Œä¸šä¸­æ€§åŒ–
- ç»¼åˆä¸­æ€§åŒ–ï¼ˆå¸‚å€¼+è¡Œä¸šï¼‰

#### å®Œæ•´æµæ°´çº¿

```python
# ä¸€é”®é¢„å¤„ç†
processed_factors = preprocess_factor_pipeline(
    factors,
    market_cap=market_cap,
    industry=industry,
    winsorize=True,
    standardize=True,
    neutralize=True,
    winsorize_params={'lower_quantile': 0.01, 'upper_quantile': 0.99},
    standardize_params={'method': 'z_score', 'cross_section': True}
)
```

---

## ğŸ“ è¾“å‡ºç›®å½•ç»“æ„

```
ML output/reports/baseline_vX/factors/
â”œâ”€â”€ factor_momentum/
â”‚   â”œâ”€â”€ tearsheet_factor_momentum_ret_1d.html
â”‚   â”œâ”€â”€ tearsheet_factor_momentum_ret_5d.html
â”‚   â”œâ”€â”€ ic_factor_momentum_ret_5d.csv
â”‚   â”œâ”€â”€ quantile_returns_factor_momentum_ret_5d.csv
â”‚   â”œâ”€â”€ ic_series_factor_momentum_ret_5d.png
â”‚   â”œâ”€â”€ ic_dist_factor_momentum_ret_5d.png
â”‚   â”œâ”€â”€ quantile_cumret_factor_momentum_ret_5d.png
â”‚   â”œâ”€â”€ quantile_meanret_factor_momentum_ret_5d.png
â”‚   â”œâ”€â”€ spread_cumret_factor_momentum_ret_5d.png
â”‚   â”œâ”€â”€ ic_heatmap_factor_momentum_ret_5d.png
â”‚   â””â”€â”€ turnover_factor_momentum.png
â”œâ”€â”€ factor_value/
â”‚   â””â”€â”€ ...
â””â”€â”€ factor_quality/
    â””â”€â”€ ...
```

---

## ğŸ”¬ æŠ€æœ¯å®ç°ç»†èŠ‚

### æ¨ªæˆªé¢è®¡ç®—åŸåˆ™

**æ ¸å¿ƒç†å¿µ**ï¼šæ‰€æœ‰è®¡ç®—å‡æŒ‰**æ—¥æ¨ªæˆªé¢**ç‹¬ç«‹è¿›è¡Œï¼Œé¿å…å‰è§†åå·®ï¼ˆLook-ahead Biasï¼‰ã€‚

**å®ç°æ¨¡å¼**ï¼š
```python
# ä¼ªä»£ç ç¤ºä¾‹
for date in unique_dates:
    # æå–å½“æ—¥æˆªé¢æ•°æ®
    factor_cross_section = factors.loc[date]
    
    # åœ¨æˆªé¢å†…è®¡ç®—
    result = some_calculation(factor_cross_section)
    
    # å­˜å‚¨ç»“æœ
    results[date] = result
```

### æ ¸å¿ƒå‡½æ•°å®ç°é€»è¾‘

#### 1. `calculate_forward_returns()` - è¿œæœŸæ”¶ç›Šè®¡ç®—

```python
def calculate_forward_returns(prices, periods, return_type='simple'):
    """
    è®¡ç®—å¤šæœŸè¿œæœŸæ”¶ç›Š
    
    å…³é”®å®ç°:
    - ä½¿ç”¨ groupby(level='ticker').shift(-period) ä¿è¯æŒ‰è‚¡ç¥¨åˆ†ç»„
    - é¿å…è·¨è‚¡ç¥¨çš„é”™è¯¯è®¡ç®—
    """
    forward_returns = {}
    
    for period in periods:
        # æŒ‰è‚¡ç¥¨åˆ†ç»„ï¼Œå‘å‰shift
        future_prices = prices.groupby(level='ticker').shift(-period)
        
        if return_type == 'simple':
            ret = (future_prices / prices - 1)
        else:  # log
            ret = np.log(future_prices / prices)
        
        forward_returns[f'ret_{period}d'] = ret
    
    return pd.DataFrame(forward_returns)
```

#### 2. `calculate_daily_ic()` - æ¯æ—¥ICè®¡ç®—

```python
def calculate_daily_ic(factors, forward_returns, method='spearman'):
    """
    æ¯æ—¥æ¨ªæˆªé¢Rank ICè®¡ç®—
    
    å…³é”®å®ç°:
    - æŒ‰æ—¥æœŸåˆ†ç»„ï¼Œæ¯æ—¥ç‹¬ç«‹è®¡ç®—ç›¸å…³ç³»æ•°
    - ä½¿ç”¨ scipy.stats.spearmanr() è®¡ç®—ç§©ç›¸å…³
    - è‡ªåŠ¨å¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
    """
    ic_series = []
    
    for date in factors.index.get_level_values('date').unique():
        # æå–å½“æ—¥æˆªé¢
        factor_cross = factors.loc[date].values.flatten()
        return_cross = forward_returns.loc[date].values.flatten()
        
        # è¿‡æ»¤NaN
        mask = ~(np.isnan(factor_cross) | np.isnan(return_cross))
        
        if mask.sum() >= 10:  # è‡³å°‘10ä¸ªæœ‰æ•ˆæ ·æœ¬
            ic, p_value = scipy.stats.spearmanr(
                factor_cross[mask],
                return_cross[mask]
            )
            ic_series.append({
                'date': date,
                'ic': ic,
                'p_value': p_value
            })
    
    return pd.DataFrame(ic_series).set_index('date')
```

#### 3. `calculate_quantile_returns()` - åˆ†ä½æ•°æ”¶ç›Šè®¡ç®—

```python
def calculate_quantile_returns(factors, forward_returns, quantiles=5):
    """
    æ¯æ—¥æ¨ªæˆªé¢åˆ†ä½æ•°ç»„åˆæ”¶ç›Š
    
    å…³é”®å®ç°:
    - æŒ‰æ—¥æœŸåˆ†ç»„ï¼Œæ¯æ—¥ç‹¬ç«‹åˆ†æ¡¶
    - ä½¿ç”¨ pd.qcut() ç­‰é¢‘åˆ†ä½
    - è®¡ç®—å„æ¡¶å¹³å‡æ”¶ç›Š
    """
    quantile_returns = []
    
    for date in factors.index.get_level_values('date').unique():
        factor_cross = factors.loc[date]
        return_cross = forward_returns.loc[date]
        
        # ç­‰é¢‘åˆ†ä½
        labels = [f'Q{i+1}' for i in range(quantiles)]
        quantile_labels = pd.qcut(
            factor_cross.rank(method='first'),
            q=quantiles,
            labels=labels,
            duplicates='drop'
        )
        
        # è®¡ç®—å„æ¡¶å¹³å‡æ”¶ç›Š
        for q in labels:
            mask = (quantile_labels == q)
            if mask.sum() > 0:
                quantile_returns.append({
                    'date': date,
                    'quantile': q,
                    'return': return_cross[mask].mean()
                })
    
    df = pd.DataFrame(quantile_returns)
    return df.pivot(index='date', columns='quantile', values='return')
```

#### 4. `neutralize_factor()` - å› å­ä¸­æ€§åŒ–

```python
def neutralize_factor(factors, market_cap, industry):
    """
    æ¨ªæˆªé¢å›å½’ä¸­æ€§åŒ–
    
    å…³é”®å®ç°:
    - æŒ‰æ—¥æœŸåˆ†ç»„ï¼Œæ¯æ—¥ç‹¬ç«‹å›å½’
    - OLS: Î² = (X'X)^-1 X'y
    - å–æ®‹å·®ä½œä¸ºä¸­æ€§åŒ–å› å­
    """
    neutralized = []
    
    for date in factors.index.get_level_values('date').unique():
        # å½“æ—¥æˆªé¢
        y = factors.loc[date].values.flatten()
        
        # æ„å»ºXçŸ©é˜µ
        X_list = []
        
        # å¸‚å€¼ï¼ˆå–å¯¹æ•°ï¼‰
        if market_cap is not None:
            mktcap = np.log(market_cap.loc[date].values.flatten())
            X_list.append(mktcap)
        
        # è¡Œä¸šå“‘å˜é‡
        if industry is not None:
            industry_dummies = pd.get_dummies(
                industry.loc[date],
                drop_first=True
            )
            X_list.append(industry_dummies.values)
        
        # åˆå¹¶X
        X = np.column_stack(X_list)
        
        # OLSå›å½’
        beta, residuals, rank, s = np.linalg.lstsq(X, y, rcond=None)
        
        # å–æ®‹å·®
        neutralized.append({
            'date': date,
            'residuals': y - X @ beta
        })
    
    return pd.DataFrame(neutralized)
```

---

## ğŸ“œ ç ”ç©¶å®ªç« åˆè§„æ€§

æœ¬æ¡†æ¶å®Œå…¨ç¬¦åˆã€Šæœºå™¨å­¦ä¹ é‡åŒ–ç ”ç©¶å®ªç«  v1.0ã€‹ç¬¬2ç« "æ¨ªæˆªé¢è¯„ä¼°æ ‡å‡†"çš„è¦æ±‚ã€‚

### åˆè§„æ£€æŸ¥æ¸…å•

#### âœ… ç¬¬2.1æ¡ï¼šæ¨ªæˆªé¢è®¡ç®—å£å¾„

**å®ªç« è¦æ±‚**ï¼š
> æ‰€æœ‰å› å­è¯„ä¼°å¿…é¡»é‡‡ç”¨æ¨ªæˆªé¢è®¡ç®—æ–¹å¼ï¼Œå³åœ¨æ¯ä¸ªæ—¶é—´æˆªé¢ä¸Šç‹¬ç«‹è®¡ç®—æŒ‡æ ‡ï¼Œä¸¥ç¦ä½¿ç”¨å…¨å±€ç»Ÿè®¡é‡ã€‚

**å®ç°éªŒè¯**ï¼š
```python
# æ‰€æœ‰æ ¸å¿ƒå‡½æ•°å‡ä½¿ç”¨æ¨ªæˆªé¢æ¨¡å¼
grep -r "cross_section.*True" evaluation/*.py
# è¿”å› 20+ å¤„åŒ¹é…

# ç¤ºä¾‹ä»£ç ç‰‡æ®µ
for date in dates:  # æŒ‰æ—¥å¾ªç¯
    factor_cross = factors.loc[date]  # æå–æˆªé¢
    # ... åœ¨æˆªé¢å†…è®¡ç®—
```

#### âœ… ç¬¬2.2æ¡ï¼šICè®¡ç®—æ ‡å‡†

**å®ªç« è¦æ±‚**ï¼š
- Rank ICï¼ˆSpearmanç›¸å…³ï¼‰ä½œä¸ºä¸»è¦æŒ‡æ ‡
- |Rank IC| â‰¥ 0.02 ä¸ºæœ‰æ•ˆå› å­
- p-value < 0.05 ä¸ºç»Ÿè®¡æ˜¾è‘—

**å®ç°éªŒè¯**ï¼š
```python
# calculate_daily_ic() é»˜è®¤ä½¿ç”¨ Spearman
ic, p_value = scipy.stats.spearmanr(factor_cross, return_cross)

# calculate_ic_summary() è‡ªåŠ¨è®¡ç®— t-test
t_stat, p_value = scipy.stats.ttest_1samp(ic_clean, 0)

# tearsheet.py è‡ªåŠ¨è¯„ä¼°æ ‡å‡†
if abs(ic_mean) >= 0.03 and p_value < 0.01:
    quality = "ä¼˜ç§€"
elif abs(ic_mean) >= 0.02 and p_value < 0.05:
    quality = "åˆæ ¼"
```

#### âœ… ç¬¬2.3æ¡ï¼šICIRæ ‡å‡†

**å®ªç« è¦æ±‚**ï¼š
- ICIR = Mean(IC) / Std(IC)
- å¹´åŒ–: ICIR_annual = ICIR Ã— âˆš252
- ICIR â‰¥ 0.5 ä¸ºåˆæ ¼ï¼Œâ‰¥ 1.0 ä¸ºä¼˜ç§€

**å®ç°éªŒè¯**ï¼š
```python
# calculate_ic_summary()
ic_ir = ic_mean / ic_std
ic_ir_annual = ic_ir * np.sqrt(252) if annualize else ic_ir

# tearsheet.py è‡ªåŠ¨è¯„çº§
if ic_ir_annual >= 1.5:
    rating = "â­â­â­â­â­"
elif ic_ir_annual >= 1.0:
    rating = "â­â­â­â­"
```

#### âœ… ç¬¬2.4æ¡ï¼šåˆ†ä½æ•°åˆ†æ

**å®ªç« è¦æ±‚**ï¼š
- ä½¿ç”¨æ¨ªæˆªé¢ç­‰é¢‘åˆ†ä½ï¼ˆpd.qcutï¼‰
- æ¨è5åˆ†ä½æˆ–10åˆ†ä½
- æ£€éªŒå•è°ƒæ€§ï¼ˆKendall Ï„ï¼‰

**å®ç°éªŒè¯**ï¼š
```python
# calculate_quantile_returns()
quantile_labels = pd.qcut(
    factor_cross.rank(method='first'),
    q=quantiles,
    labels=labels,
    duplicates='drop'
)

# calculate_monotonicity()
kendall_tau, kendall_p = scipy.stats.kendalltau(...)
```

#### âœ… ç¬¬2.5æ¡ï¼šSpreadåˆ†æ

**å®ªç« è¦æ±‚**ï¼š
- ä¼˜å…ˆä½¿ç”¨ Top-Mean Spreadï¼ˆå®ç›˜æ›´ç¨³å¥ï¼‰
- è®¡ç®— Spread Sharpe Ratioï¼ˆå¹´åŒ–ï¼‰
- æµ‹è¯•é›† Spread > 0 ä¸ºç¡¬çº¦æŸ

**å®ç°éªŒè¯**ï¼š
```python
# calculate_spread()
if method == 'top_minus_mean':
    spread = quantile_returns.iloc[:, -1] - quantile_returns.mean(axis=1)
elif method == 'top_minus_bottom':
    spread = quantile_returns.iloc[:, -1] - quantile_returns.iloc[:, 0]

# tearsheet.py è‡ªåŠ¨è®¡ç®—å¤æ™®æ¯”
spread_sharpe_annual = spread.mean() / spread.std() * np.sqrt(252)
```

#### âœ… ç¬¬2.6æ¡ï¼šå› å­é¢„å¤„ç†

**å®ªç« è¦æ±‚**ï¼š
- å¿…é¡»å¼€å¯ Winsorizeï¼ˆ1%-99%ï¼‰
- å¿…é¡»æ¨ªæˆªé¢æ ‡å‡†åŒ–ï¼ˆZ-scoreï¼‰
- å¼ºçƒˆæ¨èä¸­æ€§åŒ–ï¼ˆå¸‚å€¼+è¡Œä¸šï¼‰

**å®ç°éªŒè¯**ï¼š
```python
# preprocess_factor_pipeline()
if winsorize:
    factors = winsorize_factor(factors, cross_section=True)

if standardize:
    factors = standardize_factor(factors, method='z_score', cross_section=True)

if neutralize:
    factors = neutralize_factor(factors, market_cap, industry)
```

#### âœ… ç¬¬2.7æ¡ï¼šæŠ¥å‘Šè¾“å‡º

**å®ªç« è¦æ±‚**ï¼š
- HTMLæ ¼å¼Tearsheet
- ICæ—¶é—´åºåˆ—CSV
- åˆ†ä½æ•°æ”¶ç›ŠCSV
- é«˜æ¸…å›¾è¡¨ï¼ˆ300 DPIï¼‰

**å®ç°éªŒè¯**ï¼š
```python
# generate_full_tearsheet()
# ç”Ÿæˆ HTML + CSV + PNG
fig.savefig(plot_path, dpi=300, bbox_inches='tight')
ic_series.to_csv(ic_csv_path)
quantile_returns.to_csv(quantile_csv_path)
```

### å®ªç« åˆè§„è¯„åˆ†

| æ¡æ¬¾ | è¦æ±‚ | å®ç°çŠ¶æ€ | è¯„åˆ† |
|------|------|----------|------|
| 2.1 æ¨ªæˆªé¢å£å¾„ | æ‰€æœ‰è®¡ç®—æ¨ªæˆªé¢ç‹¬ç«‹ | âœ… å®Œå…¨ç¬¦åˆ | â­â­â­â­â­ |
| 2.2 ICæ ‡å‡† | Rank IC + æ˜¾è‘—æ€§æ£€éªŒ | âœ… å®Œå…¨ç¬¦åˆ | â­â­â­â­â­ |
| 2.3 ICIRæ ‡å‡† | å«å¹´åŒ–è®¡ç®— | âœ… å®Œå…¨ç¬¦åˆ | â­â­â­â­â­ |
| 2.4 åˆ†ä½æ•°åˆ†æ | ç­‰é¢‘åˆ†æ¡¶ + å•è°ƒæ€§ | âœ… å®Œå…¨ç¬¦åˆ | â­â­â­â­â­ |
| 2.5 Spreadåˆ†æ | Top-Meanä¼˜å…ˆ | âœ… å®Œå…¨ç¬¦åˆ | â­â­â­â­â­ |
| 2.6 å› å­é¢„å¤„ç† | å®Œæ•´æµæ°´çº¿ | âœ… å®Œå…¨ç¬¦åˆ | â­â­â­â­â­ |
| 2.7 æŠ¥å‘Šè¾“å‡º | HTML+CSV+PNG | âœ… å®Œå…¨ç¬¦åˆ | â­â­â­â­â­ |

**æ€»ä½“åˆè§„æ€§**: â­â­â­â­â­ (5/5æ˜Ÿ)

---

## ğŸ” éªŒæ”¶æ ‡å‡†

### 1. ICè®¡ç®—å‡†ç¡®æ€§

- âœ… ICä¸æ‰‹å·¥è®¡ç®—ä¸€è‡´
- âœ… tæ£€éªŒp-value < 0.05ï¼ˆæ˜¾è‘—æ€§ï¼‰
- âœ… æ­£ICæ¯”ä¾‹ > 50%

### 2. å›¾è¡¨å®Œæ•´æ€§

- âœ… ICèµ°å»Šå›¾ï¼ˆå«å‡å€¼ã€Â±1ÏƒåŒºé—´ï¼‰
- âœ… ICåˆ†å¸ƒç›´æ–¹å›¾ï¼ˆå«æ­£æ€æ‹Ÿåˆï¼‰
- âœ… åˆ†ä½æ•°ç´¯è®¡æ”¶ç›Šå›¾ï¼ˆ5æ¡£ï¼‰
- âœ… Spreadç´¯è®¡æ”¶ç›Šå›¾
- âœ… æ¢æ‰‹ç‡æ—¶é—´åºåˆ—å›¾

### 3. æŠ¥å‘Šå®Œæ•´æ€§

- âœ… HTML Tearsheetç”Ÿæˆ
- âœ… IC CSVå¯¼å‡º
- âœ… åˆ†ä½æ•°æ”¶ç›ŠCSVå¯¼å‡º

---

## ğŸ’¡ å®ç›˜ä½¿ç”¨å»ºè®®

### 1. å› å­é¢„å¤„ç†

```python
analyzer.preprocess(
    winsorize=True,          # å¿…é¡»å¼€å¯
    standardize=True,        # å¿…é¡»å¼€å¯
    neutralize=True,         # å¼ºçƒˆæ¨èå¼€å¯
    winsorize_params={
        'lower_quantile': 0.01,
        'upper_quantile': 0.99
    },
    standardize_params={
        'method': 'z_score',
        'cross_section': True  # æ¨ªæˆªé¢æ ‡å‡†åŒ–
    }
)
```

### 2. å¯äº¤æ˜“æ€§è¿‡æ»¤

```python
# æ„å»ºå¯äº¤æ˜“æ€§mask
tradable_mask = pd.DataFrame({
    'tradable': (
        ~df['is_st'] &           # éST
        ~df['is_suspended'] &    # éåœç‰Œ
        ~df['is_limit_up'] &     # éæ¶¨åœ
        ~df['is_limit_down']     # éè·Œåœ
    )
}, index=df.index)

analyzer = CrossSectionAnalyzer(
    factors=factors,
    prices=prices,
    tradable_mask=tradable_mask  # ä¼ å…¥mask
)
```

### 3. å› å­è¯„ä¼°æ ‡å‡†

**ä¼˜ç§€å› å­**ï¼š
- Mean IC > 0.03
- ICIR(å¹´åŒ–) > 1.5
- p-value < 0.01
- æ­£ICæ¯”ä¾‹ > 60%
- Spread Sharpe(å¹´åŒ–) > 1.0

**åˆæ ¼å› å­**ï¼š
- Mean IC > 0.01
- ICIR(å¹´åŒ–) > 0.5
- p-value < 0.05
- æ­£ICæ¯”ä¾‹ > 55%

**å¼±å› å­**ï¼šä¸æ»¡è¶³åˆæ ¼æ ‡å‡†

### 4. ç»„åˆä½¿ç”¨

```python
# å¤šå› å­ç»„åˆ
factors_combined = pd.DataFrame({
    'factor_composite': (
        factors_processed['factor_momentum'] * 0.4 +
        factors_processed['factor_value'] * 0.3 +
        factors_processed['factor_quality'] * 0.3
    )
})

# é‡æ–°è¯„ä¼°ç»„åˆå› å­
analyzer_combined = CrossSectionAnalyzer(
    factors=factors_combined,
    forward_returns=forward_returns
)
analyzer_combined.analyze()
```

---

## ğŸ”— ä¸ç°æœ‰ç³»ç»Ÿé›†æˆ

### é›†æˆç‚¹1ï¼šæ•°æ®åŠ è½½å™¨ï¼ˆ`data/data_loader.py`ï¼‰

```python
# åœ¨ data_loader.py ä¸­æ·»åŠ æ¨ªæˆªé¢è¯„ä¼°æ¥å£

from evaluation import CrossSectionAnalyzer

def evaluate_feature_quality(features_df, prices_df, feature_cols, **kwargs):
    """
    æ‰¹é‡è¯„ä¼°ç‰¹å¾è´¨é‡
    
    Args:
        features_df: ç‰¹å¾DataFrame (MultiIndex[date, ticker])
        prices_df: ä»·æ ¼DataFrame
        feature_cols: è¦è¯„ä¼°çš„ç‰¹å¾åˆ—è¡¨
        **kwargs: ä¼ é€’ç»™CrossSectionAnalyzerçš„å‚æ•°
    
    Returns:
        pd.DataFrame: ç‰¹å¾è¯„ä¼°æ±‡æ€»è¡¨
    """
    results_summary = []
    
    for col in feature_cols:
        analyzer = CrossSectionAnalyzer(
            factors=features_df[[col]],
            prices=prices_df,
            **kwargs
        )
        
        results = analyzer.analyze()
        ic_summary = results['ic_summary_5']
        
        results_summary.append({
            'feature': col,
            'ic_mean': ic_summary['ic_mean'],
            'icir': ic_summary['ic_ir'],
            'ic_win_rate': ic_summary['ic_win_rate'],
            'p_value': ic_summary['p_value'],
            'qualified': ic_summary['ic_ir'] > 0.5 and ic_summary['p_value'] < 0.05
        })
    
    return pd.DataFrame(results_summary)
```

### é›†æˆç‚¹2ï¼šç‰¹å¾å·¥ç¨‹ï¼ˆ`features/feature_engineering.py`ï¼‰

```python
# åœ¨ feature_engineering.py ä¸­æ·»åŠ ç‰¹å¾ç­›é€‰

from evaluation import CrossSectionAnalyzer

class FeatureSelector:
    """åŸºäºæ¨ªæˆªé¢è¯„ä¼°çš„ç‰¹å¾é€‰æ‹©å™¨"""
    
    def __init__(self, ic_threshold=0.5, p_value_threshold=0.05):
        self.ic_threshold = ic_threshold
        self.p_value_threshold = p_value_threshold
    
    def select_features(self, features_df, prices_df):
        """
        æ ¹æ®ICæ ‡å‡†ç­›é€‰ç‰¹å¾
        
        Returns:
            List[str]: é€šè¿‡ç­›é€‰çš„ç‰¹å¾åˆ—è¡¨
        """
        qualified_features = []
        
        for col in features_df.columns:
            analyzer = CrossSectionAnalyzer(
                factors=features_df[[col]],
                prices=prices_df,
                forward_periods=[5]
            )
            
            results = analyzer.analyze()
            ic_summary = results['ic_summary_5']
            
            if (ic_summary['ic_ir'] >= self.ic_threshold and 
                ic_summary['p_value'] < self.p_value_threshold):
                qualified_features.append(col)
        
        return qualified_features
```

### é›†æˆç‚¹3ï¼šæ¨¡å‹è®­ç»ƒï¼ˆ`pipelines/train_models.py`ï¼‰

```python
# åœ¨ train_models.py ä¸­æ·»åŠ ç‰¹å¾é¢„ç­›é€‰

from evaluation import CrossSectionAnalyzer
from data.data_loader import load_market_data
from features.feature_engineering import FeatureEngineering

def train_with_feature_selection(config):
    """
    è®­ç»ƒå‰å…ˆè¿›è¡Œç‰¹å¾æ¨ªæˆªé¢è¯„ä¼°
    """
    # 1. åŠ è½½åŸå§‹æ•°æ®
    data = load_market_data(config)
    
    # 2. ç”Ÿæˆç‰¹å¾
    fe = FeatureEngineering()
    features_df = fe.create_features(data)
    
    # 3. æ¨ªæˆªé¢è¯„ä¼°ç­›é€‰ç‰¹å¾
    feature_selector = FeatureSelector(ic_threshold=0.5)
    qualified_features = feature_selector.select_features(
        features_df,
        data['prices']
    )
    
    logger.info(f"ç­›é€‰å‰ç‰¹å¾æ•°: {len(features_df.columns)}")
    logger.info(f"ç­›é€‰åç‰¹å¾æ•°: {len(qualified_features)}")
    logger.info(f"å‰”é™¤ç‡: {1 - len(qualified_features)/len(features_df.columns):.2%}")
    
    # 4. ä½¿ç”¨ç­›é€‰åçš„ç‰¹å¾è®­ç»ƒ
    features_filtered = features_df[qualified_features]
    
    # ... ç»§ç»­åç»­è®­ç»ƒæµç¨‹
```

### é›†æˆç‚¹4ï¼šå›æµ‹ç³»ç»Ÿï¼ˆ`backtest/cluster_strategy_backtest.py`ï¼‰

```python
# åœ¨å›æµ‹å‰è¯„ä¼°å› å­æœ‰æ•ˆæ€§

from evaluation import CrossSectionAnalyzer

def backtest_with_factor_validation(factor_df, prices_df, config):
    """
    å›æµ‹å‰å…ˆéªŒè¯å› å­æœ‰æ•ˆæ€§
    """
    # 1. æ¨ªæˆªé¢è¯„ä¼°
    analyzer = CrossSectionAnalyzer(
        factors=factor_df,
        prices=prices_df,
        forward_periods=[5]
    )
    
    results = analyzer.preprocess(
        winsorize=True,
        standardize=True,
        neutralize=True
    ).analyze()
    
    # 2. æ£€æŸ¥å› å­è´¨é‡
    ic_summary = results['ic_summary_5']
    
    if ic_summary['ic_ir'] < 0.5:
        logger.warning(f"å› å­ICIRè¿‡ä½ ({ic_summary['ic_ir']:.2f})ï¼Œå›æµ‹ç»“æœå¯èƒ½ä¸å¯é ")
    
    if ic_summary['p_value'] > 0.05:
        logger.warning(f"å› å­ä¸æ˜¾è‘— (p={ic_summary['p_value']:.4f})ï¼Œå›æµ‹ç»“æœå¯èƒ½ä¸å¯é ")
    
    # 3. ç»§ç»­å›æµ‹
    # ... å›æµ‹é€»è¾‘
```

---

## ğŸ¯ ä½¿ç”¨åœºæ™¯çŸ©é˜µ

| åœºæ™¯ | æ¨èå·¥å…· | ä»£ç ç¤ºä¾‹ |
|------|----------|----------|
| å¿«é€Ÿè¯„ä¼°å•ä¸ªå› å­ | `CrossSectionAnalyzer` + `summary()` | ç¤ºä¾‹1 |
| æ‰¹é‡ç­›é€‰ç‰¹å¾ | `FeatureSelector` | é›†æˆç‚¹2 |
| ç”Ÿæˆå®Œæ•´æŠ¥å‘Š | `generate_full_tearsheet()` | ç¤ºä¾‹1 |
| ä»…è®¡ç®—IC | `calculate_daily_ic()` + `calculate_ic_summary()` | ç¤ºä¾‹2 |
| åˆ†ä½æ•°æ”¶ç›Šåˆ†æ | `calculate_quantile_returns()` | ç¤ºä¾‹3 |
| å› å­é¢„å¤„ç† | `preprocess_factor_pipeline()` | ç¤ºä¾‹4 |
| è‡ªå®šä¹‰å¯è§†åŒ– | `visualization.py`çš„å•ç‹¬å‡½æ•° | ç¤ºä¾‹7 |
| è®­ç»ƒå‰ç‰¹å¾ç­›é€‰ | é›†æˆè‡³`train_models.py` | é›†æˆç‚¹3 |
| å›æµ‹å‰éªŒè¯ | é›†æˆè‡³`backtest` | é›†æˆç‚¹4 |

---

## ğŸš€ ä¸‹ä¸€æ­¥è®¡åˆ’

### çŸ­æœŸä¼˜åŒ–ï¼ˆå»ºè®®1-2å‘¨å†…å®Œæˆï¼‰

1. **æ€§èƒ½ä¼˜åŒ–**
   - [ ] ä½¿ç”¨ `numba` åŠ é€ŸICè®¡ç®—å¾ªç¯
   - [ ] å¹¶è¡ŒåŒ–æ‰¹é‡å› å­è¯„ä¼°ï¼ˆ`multiprocessing`ï¼‰
   - [ ] ç¼“å­˜ä¸­é—´ç»“æœï¼ˆ`joblib`ï¼‰

2. **åŠŸèƒ½å¢å¼º**
   - [ ] æ·»åŠ Pearson ICæ”¯æŒï¼ˆå·²æ”¯æŒSpearmanï¼‰
   - [ ] æ”¯æŒå¤šå‘¨æœŸICè”åˆè¯„ä¼°
   - [ ] æ·»åŠ å› å­è¡°å‡åˆ†æï¼ˆICéšæŒæœ‰æœŸå˜åŒ–ï¼‰
   - [ ] æ”¯æŒåˆ†ç»„å›æµ‹ï¼ˆè¡Œä¸š/å¸‚å€¼ç»„ï¼‰

3. **æŠ¥å‘Šå¢å¼º**
   - [ ] æ·»åŠ PDFå¯¼å‡ºåŠŸèƒ½
   - [ ] äº¤äº’å¼HTMLæŠ¥å‘Šï¼ˆPlotlyï¼‰
   - [ ] å› å­å¯¹æ¯”æŠ¥å‘Šï¼ˆå¤šå› å­å¹¶æ’ï¼‰

### ä¸­æœŸæ‰©å±•ï¼ˆå»ºè®®1-2ä¸ªæœˆå†…å®Œæˆï¼‰

4. **å¤šå› å­åˆ†æ**
   - [ ] å› å­ç›¸å…³æ€§çŸ©é˜µ
   - [ ] å› å­æ­£äº¤åŒ–å·¥å…·
   - [ ] å› å­åˆæˆä¼˜åŒ–ï¼ˆæœ€ä¼˜æƒé‡ï¼‰

5. **é«˜çº§è¯„ä¼°**
   - [ ] äº‹ä»¶ç ”ç©¶ï¼ˆEvent Studyï¼‰
   - [ ] å› å­æ—¶å˜æ€§åˆ†æ
   - [ ] å› å­æ‹¥æŒ¤åº¦æŒ‡æ ‡

6. **å›æµ‹é›†æˆ**
   - [ ] ä¸ `backtest` æ¨¡å—æ·±åº¦é›†æˆ
   - [ ] åŸºäºICçš„åŠ¨æ€æƒé‡å›æµ‹
   - [ ] è€ƒè™‘äº¤æ˜“æˆæœ¬çš„å‡€å€¼æ›²çº¿

### é•¿æœŸè§„åˆ’ï¼ˆå»ºè®®3-6ä¸ªæœˆå†…å®Œæˆï¼‰

7. **æœºå™¨å­¦ä¹ å¢å¼º**
   - [ ] å› å­è‡ªåŠ¨æŒ–æ˜ï¼ˆAutoMLï¼‰
   - [ ] å› å­èšç±»åˆ†æ
   - [ ] å› å­éçº¿æ€§ç»„åˆï¼ˆæ ‘æ¨¡å‹ï¼‰

8. **å®ç›˜æ”¯æŒ**
   - [ ] å®æ—¶å› å­ç›‘æ§
   - [ ] å› å­è¡°å‡é¢„è­¦
   - [ ] å› å­å¤±æ•ˆæ£€æµ‹

9. **æ–‡æ¡£ä¸æµ‹è¯•**
   - [ ] å•å…ƒæµ‹è¯•è¦†ç›–ç‡ > 80%
   - [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•
   - [ ] ç”¨æˆ·ä½¿ç”¨æ¡ˆä¾‹åº“

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

æœ¬æ¡†æ¶è®¾è®¡å‚è€ƒäº†ä»¥ä¸‹å·¥å…·å’Œæ–‡çŒ®ï¼š

1. **Alphalens** - Quantopianå¼€æºå› å­åˆ†æå·¥å…·
   - GitHub: https://github.com/quantopian/alphalens
   - è®ºæ–‡: Alphalens Documentation

2. **WorldQuant 101 Alphas** - ç»å…¸å› å­åº“
   - è®ºæ–‡: "101 Formulaic Alphas" (2015)

3. **Barra Risk Models** - å¤šå› å­é£é™©æ¨¡å‹
   - CNE5 ä¸­å›½Aè‚¡é£é™©æ¨¡å‹
   - USE4 ç¾è‚¡é£é™©æ¨¡å‹

4. **Fama-Frenchå› å­æ¨¡å‹** - å­¦æœ¯åŸºå‡†
   - è®ºæ–‡: "Common risk factors in the returns on stocks and bonds" (1993)
   - è®ºæ–‡: "A five-factor asset pricing model" (2015)

5. **å…¶ä»–å‚è€ƒ**
   - ã€Šå› å­æŠ•èµ„ï¼šæ–¹æ³•ä¸å®è·µã€‹ï¼ˆçŸ³å·ç­‰ï¼Œ2020ï¼‰
   - ã€Šé‡åŒ–æŠ•èµ„ï¼šç­–ç•¥ä¸æŠ€æœ¯ã€‹ï¼ˆä¸é¹ï¼Œ2022ï¼‰

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆICå¾ˆä½ï¼Ÿ

A: å¯èƒ½åŸå› ï¼š
- å› å­é¢„æµ‹èƒ½åŠ›å¼±
- æœªè¿›è¡Œä¸­æ€§åŒ–å¤„ç†
- æ•°æ®è´¨é‡é—®é¢˜ï¼ˆåœç‰Œã€æ¶¨è·Œåœæœªè¿‡æ»¤ï¼‰
- å‰ç»æœŸé€‰æ‹©ä¸å½“

### Q2: ä¸ºä»€ä¹ˆåˆ†ä½æ•°æ”¶ç›Šä¸å•è°ƒï¼Ÿ

A: å¯èƒ½åŸå› ï¼š
- å› å­å™ªéŸ³è¾ƒå¤§
- æ ·æœ¬æ•°ä¸è¶³
- å­˜åœ¨æç«¯å€¼ï¼ˆå»ºè®®å¼€å¯winsorizeï¼‰
- æœªè¿›è¡Œæ¨ªæˆªé¢æ ‡å‡†åŒ–

### Q3: å¦‚ä½•æå‡å› å­è¡¨ç°ï¼Ÿ

A: å»ºè®®ï¼š
1. å¼€å¯å®Œæ•´é¢„å¤„ç†æµç¨‹
2. æ·»åŠ å¯äº¤æ˜“æ€§è¿‡æ»¤
3. è€ƒè™‘å¸‚å€¼å’Œè¡Œä¸šä¸­æ€§åŒ–
4. ç»„åˆå¤šä¸ªå¼±ç›¸å…³å› å­
5. ä¼˜åŒ–å‰ç»æœŸé€‰æ‹©

### Q4: æ¢æ‰‹ç‡è¿‡é«˜æ€ä¹ˆåŠï¼Ÿ

A: ç­–ç•¥ï¼š
- å»¶é•¿æŒä»“å‘¨æœŸ
- è®¾ç½®æ¢æ‰‹ç‡ä¸Šé™
- è€ƒè™‘äº¤æ˜“æˆæœ¬
- ä½¿ç”¨å› å­å¹³æ»‘æŠ€æœ¯

---

## ğŸ”§ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰åº¦é‡

```python
from evaluation.cross_section_metrics import calculate_daily_ic

# ä½¿ç”¨Pearsonç›¸å…³ï¼ˆè€ŒéSpearmanï¼‰
ic_pearson = calculate_daily_ic(
    factors,
    forward_returns,
    method='pearson'
)
```

### æ‰¹é‡å› å­è¯„ä¼°

```python
factor_cols = ['factor_1', 'factor_2', 'factor_3']

for factor_col in factor_cols:
    factor_single = factors[[factor_col]]
    
    analyzer = CrossSectionAnalyzer(
        factors=factor_single,
        forward_returns=forward_returns
    )
    
    analyzer.analyze()
    results = analyzer.get_results()
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_full_tearsheet(...)
```

---

## ğŸ“ æ”¯æŒä¸åé¦ˆ

### å¸¸è§é—®é¢˜æ’æŸ¥

**é—®é¢˜1ï¼šICè®¡ç®—ç»“æœå…¨ä¸ºNaN**

å¯èƒ½åŸå› ï¼š
- å› å­æˆ–æ”¶ç›Šæ•°æ®å­˜åœ¨å¤§é‡ç¼ºå¤±å€¼
- MultiIndexæ ¼å¼ä¸æ­£ç¡®
- æ—¥æœŸå¯¹é½é—®é¢˜

è§£å†³æ–¹æ³•ï¼š
```python
# æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
print(f"å› å­ç¼ºå¤±ç‡: {factors.isna().sum().sum() / len(factors):.2%}")
print(f"æ”¶ç›Šç¼ºå¤±ç‡: {forward_returns.isna().sum().sum() / len(forward_returns):.2%}")

# æ£€æŸ¥MultiIndex
assert isinstance(factors.index, pd.MultiIndex)
assert factors.index.names == ['date', 'ticker']
```

**é—®é¢˜2ï¼šä¸­æ€§åŒ–å¤±è´¥**

å¯èƒ½åŸå› ï¼š
- è¡Œä¸šæ•°æ®æ ¼å¼é”™è¯¯ï¼ˆåº”ä¸ºå­—ç¬¦ä¸²ï¼‰
- å¸‚å€¼æ•°æ®åŒ…å«è´Ÿå€¼æˆ–é›¶å€¼
- æŸæ—¥æˆªé¢æ ·æœ¬æ•°è¿‡å°‘

è§£å†³æ–¹æ³•ï¼š
```python
# æ£€æŸ¥è¡Œä¸šæ•°æ®
assert industry.dtype == 'object' or industry.dtype.name == 'category'

# æ£€æŸ¥å¸‚å€¼æ•°æ®
assert (market_cap > 0).all().all()

# è¿‡æ»¤å°æ ·æœ¬æˆªé¢
min_samples = 30
valid_dates = factors.groupby(level='date').size() >= min_samples
factors = factors[factors.index.get_level_values('date').isin(valid_dates[valid_dates].index)]
```

**é—®é¢˜3ï¼šå›¾è¡¨ä¸­æ–‡ä¹±ç **

è§£å†³æ–¹æ³•ï¼š
```python
import matplotlib.pyplot as plt

# æ–¹å¼1ï¼šä½¿ç”¨å†…ç½®å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# æ–¹å¼2ï¼šä½¿ç”¨ç³»ç»Ÿå­—ä½“
from matplotlib.font_manager import FontProperties
font = FontProperties(fname='/System/Library/Fonts/STHeiti Medium.ttc')
plt.xlabel('æ—¥æœŸ', fontproperties=font)
```

### ç‰ˆæœ¬å†å²

**v1.0.0** (2024)
- âœ… å®Œæ•´çš„æ¨ªæˆªé¢è¯„ä¼°æ¡†æ¶
- âœ… 6ä¸ªæ ¸å¿ƒæ¨¡å— + 1ä¸ªç¤ºä¾‹è„šæœ¬
- âœ… ç¬¦åˆç ”ç©¶å®ªç«  v1.0 æ‰€æœ‰è¦æ±‚
- âœ… å®Œæ•´çš„æ–‡æ¡£å’Œä½¿ç”¨ç¤ºä¾‹

### è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

**å¼€å‘ç¯å¢ƒè®¾ç½®**ï¼š
```bash
# å…‹éš†é¡¹ç›®
cd "d:\vscode projects\stock\machine learning"

# å®‰è£…ä¾èµ–
pip install pandas numpy scipy matplotlib seaborn

# è¿è¡Œæµ‹è¯•
cd pipelines
python run_cross_section_analysis.py
```

**ä»£ç è§„èŒƒ**ï¼š
- éµå¾ªPEP 8
- å‡½æ•°å¿…é¡»åŒ…å«å®Œæ•´docstring
- æ‰€æœ‰æ¨ªæˆªé¢è®¡ç®—å¿…é¡»æ˜¾å¼å¾ªç¯æ—¥æœŸ
- æäº¤å‰è¿è¡Œç¤ºä¾‹è„šæœ¬éªŒè¯

### è”ç³»æ–¹å¼

- é¡¹ç›®ä½ç½®: `d:\vscode projects\stock\machine learning\evaluation\`
- æ–‡æ¡£: `README_CROSS_SECTION.md`ï¼ˆæœ¬æ–‡ä»¶ï¼‰
- ç¤ºä¾‹è„šæœ¬: `pipelines/run_cross_section_analysis.py`

---

## ğŸ“ æœ¯è¯­è¡¨

| æœ¯è¯­ | è‹±æ–‡ | è¯´æ˜ |
|------|------|------|
| æ¨ªæˆªé¢ | Cross-Section | åœ¨åŒä¸€æ—¶é—´ç‚¹ä¸Šï¼Œå¯¹å¤šä¸ªè‚¡ç¥¨è¿›è¡Œçš„åˆ†æ |
| Rank IC | Rank Information Coefficient | å› å­å€¼ä¸æœªæ¥æ”¶ç›Šçš„Spearmanç§©ç›¸å…³ç³»æ•° |
| ICIR | IC Information Ratio | ICçš„å‡å€¼/æ ‡å‡†å·®ï¼Œè¡¡é‡ICçš„ç¨³å®šæ€§ |
| Winsorize | Winsorization | æå€¼å¤„ç†ï¼Œå°†è¶…å‡ºåˆ†ä½æ•°çš„å€¼è£å‰ªåˆ°åˆ†ä½æ•° |
| ä¸­æ€§åŒ– | Neutralization | é€šè¿‡å›å½’æ®‹å·®æ³•ï¼Œå»é™¤å› å­ä¸­çš„å¸‚å€¼/è¡Œä¸šæ•ˆåº” |
| Spread | Spread | é¡¶éƒ¨åˆ†ä½æ•°æ”¶ç›Šä¸åº•éƒ¨/å‡å€¼çš„å·®å€¼ |
| Tearsheet | Tearsheet | ç»¼åˆæ€§è¯„ä¼°æŠ¥å‘Šï¼ŒåŒ…å«å¤šä¸ªç»´åº¦çš„åˆ†æç»“æœ |
| Monotonicity | Monotonicity | å•è°ƒæ€§ï¼Œæ£€éªŒåˆ†ä½æ•°æ”¶ç›Šæ˜¯å¦éšå› å­å€¼å•è°ƒé€’å¢ |
| Turnover | Turnover | æ¢æ‰‹ç‡ï¼Œè¡¡é‡æŒä»“å˜åŒ–é¢‘ç‡ |
| Quantile | Quantile | åˆ†ä½æ•°ï¼ŒæŒ‰å› å­å€¼ç­‰é¢‘åˆ†ç»„ |

---

## ğŸ“Š è¾“å‡ºç¤ºä¾‹

### HTML Tearsheet é¢„è§ˆ

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š å› å­è¯„ä¼°æŠ¥å‘Šï¼šmomentum_factor
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“ˆ ICç»Ÿè®¡æ‘˜è¦

æŒ‡æ ‡                    å€¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ICå‡å€¼                 0.0342
ICæ ‡å‡†å·®               0.0876
ICIR                   0.39
ICIR(å¹´åŒ–)             6.19
tç»Ÿè®¡é‡                15.67
p-value                < 0.001
ICèƒœç‡                 58.3%
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âœ… å› å­è´¨é‡è¯„ä¼°ï¼šä¼˜ç§€å› å­

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š åˆ†ä½æ•°æ”¶ç›Šåˆ†æ

åˆ†ä½æ•°    å¹³å‡æ”¶ç›Š    ç´¯è®¡æ”¶ç›Š    å¤æ™®æ¯”
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Q5(Top)   0.0012      142.3%      1.89
Q4        0.0008      98.7%       1.45
Q3        0.0005      67.2%       1.12
Q2        0.0003      42.1%       0.89
Q1(Bottom) 0.0001     18.5%       0.34
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Spread(Top-Mean): 0.0007 (å¤æ™®æ¯”: 1.56)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

[å›¾è¡¨å±•ç¤ºåŒºåŸŸ]
- ICæ—¶é—´åºåˆ—èµ°å»Šå›¾
- ICåˆ†å¸ƒç›´æ–¹å›¾
- åˆ†ä½æ•°ç´¯è®¡æ”¶ç›Šæ›²çº¿
- åˆ†ä½æ•°å¹³å‡æ”¶ç›ŠæŸ±çŠ¶å›¾
- Spreadç´¯è®¡æ”¶ç›Šæ›²çº¿
- æœˆåº¦ICçƒ­åŠ›å›¾
- æ¢æ‰‹ç‡æ—¶é—´åºåˆ—

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ç”Ÿæˆæ—¶é—´ï¼š2024-01-15 14:23:45
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## ğŸ† è‡´è°¢

æ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®å’Œç¤¾åŒºçš„è´¡çŒ®ï¼š

- **Quantopian/Alphalens** - æä¾›äº†å› å­åˆ†æçš„æœ€ä½³å®è·µ
- **Pandas/NumPy/SciPy** - æä¾›äº†å¼ºå¤§çš„æ•°æ®å¤„ç†å’Œç§‘å­¦è®¡ç®—èƒ½åŠ›
- **Matplotlib/Seaborn** - æä¾›äº†ä¼˜ç§€çš„å¯è§†åŒ–å·¥å…·
- **é‡åŒ–æŠ•èµ„ç¤¾åŒº** - æä¾›äº†å®è´µçš„ç»éªŒå’Œåé¦ˆ

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªé¡¹ç›®æ ¹ç›®å½•çš„è®¸å¯è¯ã€‚

---

**Happy Factor Mining! ğŸš€ğŸ“ˆ**

*"In God we trust, all others must bring data."* - W. Edwards Deming

---

*æœ€åæ›´æ–°: 2024*  
*æ–‡æ¡£ç‰ˆæœ¬: v1.0.0*  
*æ¡†æ¶ç‰ˆæœ¬: v1.0.0*
