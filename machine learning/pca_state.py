#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PCAçŠ¶æ€è¡¨ç¤ºæ¨¡å— - é™ç»´å’ŒçŠ¶æ€ç”Ÿæˆ

åŠŸèƒ½ï¼š
1. åŸºäºæ—¶é—´åˆ‡åˆ†çš„PCAè®­ç»ƒï¼ˆé˜²æ­¢æ•°æ®æ³„æ¼ï¼‰
2. ç”Ÿæˆä½ç»´çŠ¶æ€è¡¨ç¤º
3. ä¿å­˜PCAæ¨¡å‹å’ŒçŠ¶æ€æ•°æ®
4. ç´¯è®¡è§£é‡Šæ–¹å·®éªŒè¯

ä½œè€…: Assistant
æ—¥æœŸ: 2025å¹´9æœˆ25æ—¥
"""

import os
import sys
import pandas as pd
import numpy as np
from typing import Dict, Tuple, Optional
from datetime import datetime
import pickle
import warnings
warnings.filterwarnings('ignore')

# æœºå™¨å­¦ä¹ åº“
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# å¯¼å…¥ç‰¹å¾å·¥ç¨‹å’Œç›®æ ‡å·¥ç¨‹
from feature_engineering import FeatureEngineer
from target_engineering import TargetEngineer


class PCAStateGenerator:
    """
    PCAçŠ¶æ€ç”Ÿæˆå™¨ - åŸºäºæ—¶é—´åºåˆ—çš„é™ç»´è¡¨ç¤º
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. æ—¶é—´åˆ‡åˆ†é˜²æ­¢æ•°æ®æ³„æ¼çš„PCAè®­ç»ƒ
    2. ç”Ÿæˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„PCAçŠ¶æ€
    3. æ¨¡å‹æŒä¹…åŒ–å’ŒçŠ¶æ€ä¿å­˜
    4. è§£é‡Šæ–¹å·®éªŒè¯
    """
    
    def __init__(self, models_dir: str = "machine learning/ML output/models",
                 states_dir: str = "machine learning/ML output/states"):
        """
        åˆå§‹åŒ–PCAçŠ¶æ€ç”Ÿæˆå™¨
        
        Parameters:
        -----------
        models_dir : str
            æ¨¡å‹ä¿å­˜ç›®å½•
        states_dir : str
            çŠ¶æ€æ•°æ®ä¿å­˜ç›®å½•
        """
        # è®¾ç½®ä¿å­˜ç›®å½•
        self.project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        
        if os.path.isabs(models_dir):
            self.models_dir = models_dir
        else:
            self.models_dir = os.path.join(self.project_root, models_dir)
            
        if os.path.isabs(states_dir):
            self.states_dir = states_dir
        else:
            self.states_dir = os.path.join(self.project_root, states_dir)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.models_dir, exist_ok=True)
        os.makedirs(self.states_dir, exist_ok=True)
        
        print("ğŸ¯ PCAçŠ¶æ€ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ“ æ¨¡å‹ç›®å½•: {self.models_dir}")
        print(f"   ğŸ“ çŠ¶æ€ç›®å½•: {self.states_dir}")

    def load_scaled_features(self, csv_path: str) -> pd.DataFrame:
        """
        åŠ è½½å·²æ ‡å‡†åŒ–çš„ç‰¹å¾æ•°æ®
        
        Parameters:
        -----------
        csv_path : str
            æ ‡å‡†åŒ–ç‰¹å¾CSVæ–‡ä»¶è·¯å¾„
            
        Returns:
        --------
        pd.DataFrame
            æ ‡å‡†åŒ–åçš„ç‰¹å¾æ•°æ®
        """
        print("ğŸ“Š åŠ è½½æ ‡å‡†åŒ–ç‰¹å¾æ•°æ®...")
        
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"æ ‡å‡†åŒ–ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        
        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(csv_path, index_col=0, parse_dates=True)
        
        # ç§»é™¤ç›®æ ‡åˆ—å’Œæ ‡ç­¾åˆ—ï¼Œåªä¿ç•™ç‰¹å¾åˆ—
        feature_cols = [col for col in df.columns 
                       if not col.startswith(('future_return_', 'label_', 'close'))]
        
        features_df = df[feature_cols].copy()
        
        print(f"   âœ… æˆåŠŸåŠ è½½ç‰¹å¾æ•°æ®: {features_df.shape}")
        print(f"   ğŸ“… æ—¶é—´èŒƒå›´: {features_df.index.min().date()} ~ {features_df.index.max().date()}")
        print(f"   ğŸ”¢ ç‰¹å¾æ•°é‡: {len(feature_cols)}")
        
        return features_df

    def fit_pca_with_time_split(self, features_df: pd.DataFrame,
                               n_components: float = 0.9,
                               train_ratio: float = 0.8) -> Dict:
        """
        åŸºäºæ—¶é—´åˆ‡åˆ†æ‹ŸåˆPCAï¼ˆé˜²æ­¢æ•°æ®æ³„æ¼ï¼‰
        
        Parameters:
        -----------
        features_df : pd.DataFrame
            æ ‡å‡†åŒ–åçš„ç‰¹å¾æ•°æ®
        n_components : float, default=0.9
            ç›®æ ‡è§£é‡Šæ–¹å·®æ¯”ä¾‹
        train_ratio : float, default=0.8
            è®­ç»ƒé›†æ¯”ä¾‹
            
        Returns:
        --------
        dict
            åŒ…å«PCAæ¨¡å‹å’Œç›¸å…³ä¿¡æ¯çš„å­—å…¸
        """
        print("ğŸ”§ å¼€å§‹PCAè®­ç»ƒ...")
        print(f"   ğŸ¯ ç›®æ ‡è§£é‡Šæ–¹å·®: {n_components:.1%}")
        print(f"   ğŸ“Š æ—¶é—´åˆ‡åˆ†æ¯”ä¾‹: {train_ratio:.1%}")
        
        # æ—¶é—´åˆ‡åˆ†
        n_samples = len(features_df)
        split_idx = int(n_samples * train_ratio)
        
        if split_idx < 50:
            raise ValueError(f"è®­ç»ƒæ ·æœ¬è¿‡å°‘({split_idx})ï¼Œæ— æ³•è¿›è¡ŒPCAè®­ç»ƒ")
        
        train_index = features_df.index[:split_idx]
        test_index = features_df.index[split_idx:]
        
        print(f"   ğŸ“ˆ è®­ç»ƒé›†: {split_idx} æ ·æœ¬ ({train_index.min().date()} ~ {train_index.max().date()})")
        print(f"   ğŸ“‰ æµ‹è¯•é›†: {len(test_index)} æ ·æœ¬ ({test_index.min().date()} ~ {test_index.max().date()})")
        
        # æå–è®­ç»ƒå’Œæµ‹è¯•ç‰¹å¾
        X_train = features_df.iloc[:split_idx].fillna(0)  # å¡«å……å¯èƒ½çš„ç¼ºå¤±å€¼
        X_test = features_df.iloc[split_idx:].fillna(0)
        
        original_features = X_train.shape[1]
        print(f"   ğŸ”¢ åŸå§‹ç‰¹å¾ç»´åº¦: {original_features}")
        
        # åˆå§‹åŒ–PCAï¼ˆå…ˆç”¨è¾ƒå¤§çš„æˆåˆ†æ•°é‡ï¼‰
        pca_init = PCA(n_components=min(original_features, split_idx-1))
        pca_init.fit(X_train)
        
        # è®¡ç®—ç´¯è®¡è§£é‡Šæ–¹å·®
        cumsum_variance = np.cumsum(pca_init.explained_variance_ratio_)
        
        # æ‰¾åˆ°æ»¡è¶³ç›®æ ‡è§£é‡Šæ–¹å·®çš„æˆåˆ†æ•°é‡
        n_components_needed = np.argmax(cumsum_variance >= n_components) + 1
        final_variance = cumsum_variance[n_components_needed - 1]
        
        # éªŒè¯æˆåˆ†æ•°é‡èŒƒå›´ï¼ˆåº”ä¸ºåŸå§‹ç‰¹å¾æ•°çš„1/6åˆ°1/3ï¼‰
        min_components = max(2, original_features // 6)
        max_components = original_features // 3
        
        if n_components_needed < min_components:
            print(f"   âš ï¸ æˆåˆ†æ•°é‡è¿‡å°‘({n_components_needed})ï¼Œè°ƒæ•´ä¸ºæœ€å°å€¼ {min_components}")
            n_components_needed = min_components
        elif n_components_needed > max_components:
            print(f"   âš ï¸ æˆåˆ†æ•°é‡è¿‡å¤š({n_components_needed})ï¼Œè°ƒæ•´ä¸ºæœ€å¤§å€¼ {max_components}")
            n_components_needed = max_components
        
        final_variance = cumsum_variance[n_components_needed - 1]
        
        print(f"   ğŸ¯ æœ€ç»ˆæˆåˆ†æ•°é‡: {n_components_needed}")
        print(f"   ğŸ“Š ç´¯è®¡è§£é‡Šæ–¹å·®: {final_variance:.3f} ({final_variance:.1%})")
        print(f"   ğŸ“‰ ç‰¹å¾å‹ç¼©ç‡: {original_features}/{n_components_needed} = {original_features/n_components_needed:.1f}x")
        
        # è®­ç»ƒæœ€ç»ˆPCAæ¨¡å‹
        pca_final = PCA(n_components=n_components_needed)
        pca_final.fit(X_train)
        
        # ç”Ÿæˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„PCAçŠ¶æ€
        states_train = pca_final.transform(X_train)
        states_test = pca_final.transform(X_test)
        
        print(f"   âœ… PCAè®­ç»ƒå®Œæˆ")
        print(f"   ğŸ“Š è®­ç»ƒçŠ¶æ€å½¢çŠ¶: {states_train.shape}")
        print(f"   ğŸ“Š æµ‹è¯•çŠ¶æ€å½¢çŠ¶: {states_test.shape}")
        
        # éªŒæ”¶æ£€æŸ¥
        if final_variance >= 0.9:
            print(f"   âœ… éªŒæ”¶é€šè¿‡: ç´¯è®¡è§£é‡Šæ–¹å·® {final_variance:.3f} â‰¥ 0.9")
        else:
            print(f"   âš ï¸ éªŒæ”¶è­¦å‘Š: ç´¯è®¡è§£é‡Šæ–¹å·® {final_variance:.3f} < 0.9")
        
        component_ratio = n_components_needed / original_features
        if 1/6 <= component_ratio <= 1/3:
            print(f"   âœ… éªŒæ”¶é€šè¿‡: æˆåˆ†æ¯”ä¾‹ {component_ratio:.3f} åœ¨åˆç†èŒƒå›´å†…")
        else:
            print(f"   âš ï¸ éªŒæ”¶è­¦å‘Š: æˆåˆ†æ¯”ä¾‹ {component_ratio:.3f} è¶…å‡ºå»ºè®®èŒƒå›´ [1/6, 1/3]")
        
        return {
            'pca_model': pca_final,
            'states_train': states_train,
            'states_test': states_test,
            'train_index': train_index,
            'test_index': test_index,
            'n_components': n_components_needed,
            'explained_variance_ratio': pca_final.explained_variance_ratio_,
            'cumulative_variance': final_variance,
            'original_features': original_features,
            'compression_ratio': original_features / n_components_needed,
            'feature_names': list(X_train.columns)
        }

    def save_pca_results(self, pca_results: Dict, symbol: str = "stock") -> Dict[str, str]:
        """
        ä¿å­˜PCAæ¨¡å‹å’ŒçŠ¶æ€æ•°æ®
        
        Parameters:
        -----------
        pca_results : dict
            PCAè®­ç»ƒç»“æœ
        symbol : str
            è‚¡ç¥¨ä»£ç ï¼Œç”¨äºæ–‡ä»¶å‘½å
            
        Returns:
        --------
        dict
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        print("ğŸ’¾ ä¿å­˜PCAç»“æœ...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜PCAæ¨¡å‹
        pca_filename = f"pca_{symbol}_{timestamp}.pkl"
        pca_path = os.path.join(self.models_dir, pca_filename)
        
        pca_data = {
            'pca_model': pca_results['pca_model'],
            'n_components': pca_results['n_components'],
            'explained_variance_ratio': pca_results['explained_variance_ratio'],
            'cumulative_variance': pca_results['cumulative_variance'],
            'original_features': pca_results['original_features'],
            'compression_ratio': pca_results['compression_ratio'],
            'feature_names': pca_results['feature_names'],
            'train_samples': len(pca_results['states_train']),
            'test_samples': len(pca_results['states_test']),
            'created_time': datetime.now().isoformat()
        }
        
        with open(pca_path, 'wb') as f:
            pickle.dump(pca_data, f)
        
        pca_size = os.path.getsize(pca_path) / 1024  # KB
        print(f"   âœ… PCAæ¨¡å‹å·²ä¿å­˜: {pca_filename} ({pca_size:.1f} KB)")
        
        # ä¿å­˜è®­ç»ƒçŠ¶æ€
        states_train_filename = f"states_pca_train_{symbol}_{timestamp}.npy"
        states_train_path = os.path.join(self.states_dir, states_train_filename)
        
        np.save(states_train_path, pca_results['states_train'])
        train_size = os.path.getsize(states_train_path) / 1024  # KB
        print(f"   âœ… è®­ç»ƒçŠ¶æ€å·²ä¿å­˜: {states_train_filename} ({train_size:.1f} KB)")
        
        # ä¿å­˜æµ‹è¯•çŠ¶æ€
        states_test_filename = f"states_pca_test_{symbol}_{timestamp}.npy"
        states_test_path = os.path.join(self.states_dir, states_test_filename)
        
        np.save(states_test_path, pca_results['states_test'])
        test_size = os.path.getsize(states_test_path) / 1024  # KB
        print(f"   âœ… æµ‹è¯•çŠ¶æ€å·²ä¿å­˜: {states_test_filename} ({test_size:.1f} KB)")
        
        # ä¿å­˜å…ƒæ•°æ®æ–‡ä»¶
        metadata = {
            'symbol': symbol,
            'created_time': datetime.now().isoformat(),
            'original_features': pca_results['original_features'],
            'n_components': pca_results['n_components'],
            'cumulative_variance': float(pca_results['cumulative_variance']),
            'compression_ratio': float(pca_results['compression_ratio']),
            'train_samples': len(pca_results['states_train']),
            'test_samples': len(pca_results['states_test']),
            'train_period': f"{pca_results['train_index'].min().date()} ~ {pca_results['train_index'].max().date()}",
            'test_period': f"{pca_results['test_index'].min().date()} ~ {pca_results['test_index'].max().date()}",
            'files': {
                'pca_model': pca_filename,
                'states_train': states_train_filename,
                'states_test': states_test_filename
            }
        }
        
        metadata_filename = f"pca_metadata_{symbol}_{timestamp}.json"
        metadata_path = os.path.join(self.models_dir, metadata_filename)
        
        import json
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"   ğŸ“‹ å…ƒæ•°æ®å·²ä¿å­˜: {metadata_filename}")
        
        return {
            'pca_model_path': pca_path,
            'states_train_path': states_train_path,
            'states_test_path': states_test_path,
            'metadata_path': metadata_path
        }

    def generate_pca_states(self, csv_path: str, symbol: str = "stock",
                           n_components: float = 0.9, train_ratio: float = 0.8) -> Dict:
        """
        å®Œæ•´çš„PCAçŠ¶æ€ç”Ÿæˆæµç¨‹
        
        Parameters:
        -----------
        csv_path : str
            æ ‡å‡†åŒ–ç‰¹å¾CSVæ–‡ä»¶è·¯å¾„
        symbol : str
            è‚¡ç¥¨ä»£ç 
        n_components : float
            ç›®æ ‡è§£é‡Šæ–¹å·®æ¯”ä¾‹
        train_ratio : float
            è®­ç»ƒé›†æ¯”ä¾‹
            
        Returns:
        --------
        dict
            åŒ…å«PCAç»“æœå’Œä¿å­˜è·¯å¾„çš„å®Œæ•´ä¿¡æ¯
        """
        print("ğŸš€ å¼€å§‹å®Œæ•´PCAçŠ¶æ€ç”Ÿæˆæµç¨‹...")
        print("=" * 60)
        
        # 1. åŠ è½½ç‰¹å¾æ•°æ®
        features_df = self.load_scaled_features(csv_path)
        
        # 2. æ‹ŸåˆPCA
        pca_results = self.fit_pca_with_time_split(
            features_df, 
            n_components=n_components,
            train_ratio=train_ratio
        )
        
        # 3. ä¿å­˜ç»“æœ
        save_paths = self.save_pca_results(pca_results, symbol)
        
        # 4. æœ€ç»ˆæŠ¥å‘Š
        print("\n" + "=" * 60)
        print("ğŸ‰ PCAçŠ¶æ€ç”Ÿæˆå®Œæˆ!")
        print(f"   ğŸ“Š åŸå§‹ç‰¹å¾: {pca_results['original_features']} ç»´")
        print(f"   ğŸ¯ PCAæˆåˆ†: {pca_results['n_components']} ç»´")
        print(f"   ğŸ“ˆ è§£é‡Šæ–¹å·®: {pca_results['cumulative_variance']:.3f} ({pca_results['cumulative_variance']:.1%})")
        print(f"   ğŸ“‰ å‹ç¼©æ¯”ç‡: {pca_results['compression_ratio']:.1f}x")
        print(f"   ğŸ‹ï¸ è®­ç»ƒæ ·æœ¬: {len(pca_results['states_train'])}")
        print(f"   ğŸ§ª æµ‹è¯•æ ·æœ¬: {len(pca_results['states_test'])}")
        
        # åˆå¹¶ç»“æœ
        final_results = {**pca_results, **save_paths}
        
        return final_results

    def load_pca_model(self, pca_path: str) -> Dict:
        """
        åŠ è½½å·²ä¿å­˜çš„PCAæ¨¡å‹
        
        Parameters:
        -----------
        pca_path : str
            PCAæ¨¡å‹æ–‡ä»¶è·¯å¾„
            
        Returns:
        --------
        dict
            PCAæ¨¡å‹æ•°æ®
        """
        print(f"ğŸ“– åŠ è½½PCAæ¨¡å‹: {os.path.basename(pca_path)}")
        
        with open(pca_path, 'rb') as f:
            pca_data = pickle.load(f)
        
        print(f"   âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"   ğŸ¯ æˆåˆ†æ•°é‡: {pca_data['n_components']}")
        print(f"   ğŸ“Š è§£é‡Šæ–¹å·®: {pca_data['cumulative_variance']:.3f}")
        
        return pca_data


def run_complete_feature_pipeline(symbol: str = '000001',
                                  start_date: str = '2023-01-01', 
                                  end_date: str = '2024-12-31',
                                  use_auto_features: bool = False,
                                  final_k_features: int = 15) -> Dict:
    """
    è¿è¡Œå®Œæ•´çš„ç‰¹å¾å·¥ç¨‹ç®¡é“
    
    Parameters:
    -----------
    symbol : str
        è‚¡ç¥¨ä»£ç 
    start_date : str
        å¼€å§‹æ—¥æœŸ
    end_date : str
        ç»“æŸæ—¥æœŸ
    use_auto_features : bool
        æ˜¯å¦ä½¿ç”¨è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆ
    final_k_features : int
        æœ€ç»ˆä¿ç•™çš„ç‰¹å¾æ•°é‡
        
    Returns:
    --------
    dict
        ç‰¹å¾å·¥ç¨‹ç»“æœ
    """
    print("ğŸ”§ æ­¥éª¤1: å®Œæ•´ç‰¹å¾å·¥ç¨‹æµç¨‹")
    print("-" * 50)
    
    try:
        # åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹å™¨
        feature_engineer = FeatureEngineer(use_talib=True, use_tsfresh=use_auto_features)
        
        # åŠ è½½æ•°æ®
        print(f"ğŸ“ˆ åŠ è½½è‚¡ç¥¨æ•°æ®: {symbol} ({start_date} ~ {end_date})")
        raw_data = feature_engineer.load_stock_data(symbol, start_date, end_date)
        
        if len(raw_data) < 100:
            raise ValueError(f"æ•°æ®é‡å¤ªå°‘({len(raw_data)}è¡Œ)ï¼Œå»ºè®®è‡³å°‘100è¡Œæ•°æ®")
        
        # ç”Ÿæˆç‰¹å¾
        print("ğŸ­ ç”ŸæˆæŠ€æœ¯ç‰¹å¾...")
        features_df = feature_engineer.prepare_features(
            raw_data,
            use_auto_features=use_auto_features,
            window_size=20,
            max_auto_features=30
        )
        
        # ç‰¹å¾é€‰æ‹©
        print("ğŸ¯ æ‰§è¡Œç‰¹å¾é€‰æ‹©...")
        selection_results = feature_engineer.select_features(
            features_df,
            final_k=final_k_features,
            variance_threshold=0.01,
            correlation_threshold=0.9,
            train_ratio=0.8
        )
        
        final_features_df = selection_results['final_features_df']
        
        # ç‰¹å¾æ ‡å‡†åŒ–
        print("ğŸ“ æ‰§è¡Œç‰¹å¾æ ‡å‡†åŒ–...")
        scale_results = feature_engineer.scale_features(
            final_features_df,
            scaler_type='robust',
            train_ratio=0.8,
            save_path=f'machine learning/ML output/scaler_{symbol}.pkl'
        )
        
        scaled_features_df = scale_results['scaled_df']
        print(f"   âœ… ç¼©æ”¾å™¨å·²ä¿å­˜: {scale_results['scaler_path']}")
        if scale_results.get('csv_path'):
            print(f"   ğŸ“Š æ ‡å‡†åŒ–ç‰¹å¾å·²ä¿å­˜: {scale_results['csv_path']}")
        
        # ç‰¹å¾åˆ†æ
        print("ğŸ“Š åˆ†æç‰¹å¾è´¨é‡...")
        analysis_results = feature_engineer.analyze_features(scaled_features_df)
        
        return {
            'success': True,
            'scaled_features_df': scaled_features_df,
            'csv_path': scale_results.get('csv_path'),
            'scaler_path': scale_results['scaler_path'],
            'final_feature_count': len(selection_results['final_features']),
            'sample_count': len(scaled_features_df),
            'selection_results': selection_results,
            'scale_results': scale_results,
            'analysis_results': analysis_results
        }
        
    except Exception as e:
        print(f"âŒ ç‰¹å¾å·¥ç¨‹å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return {'success': False, 'error': str(e)}

def run_complete_target_pipeline(scaled_features_df: pd.DataFrame, 
                                symbol: str = 'stock',
                                target_periods: list = [1, 5, 10]) -> Dict:
    """
    è¿è¡Œå®Œæ•´çš„ç›®æ ‡å˜é‡å·¥ç¨‹ç®¡é“
    
    Parameters:
    -----------
    scaled_features_df : pd.DataFrame
        æ ‡å‡†åŒ–åçš„ç‰¹å¾æ•°æ®
    symbol : str
        è‚¡ç¥¨ä»£ç 
    target_periods : list
        ç›®æ ‡æ—¶é—´çª—å£
        
    Returns:
    --------
    dict
        ç›®æ ‡å·¥ç¨‹ç»“æœ
    """
    print("\nğŸ¯ æ­¥éª¤2: å®Œæ•´ç›®æ ‡å˜é‡å·¥ç¨‹æµç¨‹")
    print("-" * 50)
    
    try:
        # åˆå§‹åŒ–ç›®æ ‡å·¥ç¨‹å™¨
        target_engineer = TargetEngineer()
        
        # åˆ›å»ºå®Œæ•´æ•°æ®é›†ï¼ˆç‰¹å¾ + ç›®æ ‡ï¼‰
        print("ğŸ”¨ åˆ›å»ºå®Œæ•´æ•°æ®é›†...")
        complete_dataset = target_engineer.create_complete_dataset(
            scaled_features_df,
            periods=target_periods,
            price_col='close',
            include_labels=True,
            label_types=['binary', 'quantile']
        )
        
        # ä¿å­˜æ•°æ®é›†
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        suffix = f"complete_{timestamp}"
        save_path = target_engineer.save_dataset(complete_dataset, symbol, suffix)
        
        # è®¡ç®—ç›®æ ‡ç»Ÿè®¡
        target_cols = [col for col in complete_dataset.columns if col.startswith('future_return_')]
        max_period = max(target_periods) if target_periods else 0
        trainable_samples = len(complete_dataset) - max_period
        
        return {
            'success': True,
            'complete_dataset': complete_dataset,
            'save_path': save_path,
            'target_sample_count': len(complete_dataset),
            'trainable_samples': trainable_samples,
            'target_cols': target_cols,
            'max_period': max_period
        }
        
    except Exception as e:
        print(f"âŒ ç›®æ ‡å˜é‡å·¥ç¨‹å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return {'success': False, 'error': str(e)}

def generate_final_summary(feature_results: Dict, target_results: Dict, pca_results: Dict = None) -> bool:
    """
    ç”Ÿæˆæœ€ç»ˆæµç¨‹æ‘˜è¦æŠ¥å‘Š
    
    Parameters:
    -----------
    feature_results : dict
        ç‰¹å¾å·¥ç¨‹ç»“æœ
    target_results : dict
        ç›®æ ‡å·¥ç¨‹ç»“æœ
    pca_results : dict, optional
        PCAç»“æœ
        
    Returns:
    --------
    bool
        æ˜¯å¦æˆåŠŸç”Ÿæˆæ‘˜è¦
    """
    print("\nğŸ“‹ ç”Ÿæˆæœ€ç»ˆæ‘˜è¦æŠ¥å‘Š")
    print("-" * 50)
    
    try:
        # åˆ›å»ºæ‘˜è¦å†…å®¹
        summary_lines = []
        summary_lines.append("=" * 70)
        summary_lines.append("è‚¡ç¥¨æœºå™¨å­¦ä¹ å®Œæ•´æµç¨‹æ‘˜è¦æŠ¥å‘Š")
        summary_lines.append("=" * 70)
        summary_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        summary_lines.append("")
        
        # ç‰¹å¾å·¥ç¨‹æ‘˜è¦
        summary_lines.append("ğŸ”§ ç‰¹å¾å·¥ç¨‹ç»“æœ:")
        if feature_results.get('success'):
            summary_lines.append(f"   âœ… çŠ¶æ€: æˆåŠŸå®Œæˆ")
            summary_lines.append(f"   ğŸ“Š æœ€ç»ˆç‰¹å¾æ•°: {feature_results['final_feature_count']}")
            summary_lines.append(f"   ğŸ”¢ æ ·æœ¬æ•°é‡: {feature_results['sample_count']}")
            summary_lines.append(f"   ğŸ’¾ ç¼©æ”¾å™¨: {os.path.basename(feature_results['scaler_path'])}")
            if feature_results.get('csv_path'):
                summary_lines.append(f"   ğŸ“Š ç‰¹å¾æ–‡ä»¶: {os.path.basename(feature_results['csv_path'])}")
        else:
            summary_lines.append(f"   âŒ çŠ¶æ€: å¤±è´¥ - {feature_results.get('error', 'æœªçŸ¥é”™è¯¯')}")
        summary_lines.append("")
        
        # ç›®æ ‡å·¥ç¨‹æ‘˜è¦
        summary_lines.append("ğŸ¯ ç›®æ ‡å˜é‡å·¥ç¨‹ç»“æœ:")
        if target_results.get('success'):
            summary_lines.append(f"   âœ… çŠ¶æ€: æˆåŠŸå®Œæˆ")
            summary_lines.append(f"   ğŸ“Š ç›®æ ‡æ ·æœ¬æ•°: {target_results['target_sample_count']}")
            summary_lines.append(f"   ğŸ“ å¯è®­ç»ƒæ ·æœ¬: {target_results['trainable_samples']}")
            summary_lines.append(f"   ğŸ¯ ç›®æ ‡å˜é‡æ•°: {len(target_results['target_cols'])}")
            summary_lines.append(f"   ğŸ’¾ æ•°æ®é›†æ–‡ä»¶: {os.path.basename(target_results['save_path'])}")
        else:
            summary_lines.append(f"   âŒ çŠ¶æ€: å¤±è´¥ - {target_results.get('error', 'æœªçŸ¥é”™è¯¯')}")
        summary_lines.append("")
        
        # PCAæ‘˜è¦ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if pca_results:
            summary_lines.append("ğŸ” PCAçŠ¶æ€ç”Ÿæˆç»“æœ:")
            if 'n_components' in pca_results:
                summary_lines.append(f"   âœ… çŠ¶æ€: æˆåŠŸå®Œæˆ")
                summary_lines.append(f"   ğŸ“Š åŸå§‹ç‰¹å¾æ•°: {pca_results['original_features']}")
                summary_lines.append(f"   ğŸ¯ PCAæˆåˆ†æ•°: {pca_results['n_components']}")
                summary_lines.append(f"   ğŸ“ˆ è§£é‡Šæ–¹å·®: {pca_results['cumulative_variance']:.3f} ({pca_results['cumulative_variance']:.1%})")
                summary_lines.append(f"   ğŸ“‰ å‹ç¼©æ¯”ç‡: {pca_results['compression_ratio']:.1f}x")
                summary_lines.append(f"   ğŸ‹ï¸ è®­ç»ƒæ ·æœ¬: {len(pca_results['states_train'])}")
                summary_lines.append(f"   ğŸ§ª æµ‹è¯•æ ·æœ¬: {len(pca_results['states_test'])}")
                if 'pca_model_path' in pca_results:
                    summary_lines.append(f"   ğŸ’¾ PCAæ¨¡å‹: {os.path.basename(pca_results['pca_model_path'])}")
            else:
                summary_lines.append(f"   âŒ çŠ¶æ€: å¤±è´¥")
            summary_lines.append("")
        
        # æ–‡ä»¶ç»Ÿè®¡
        ml_output_dir = os.path.join("machine learning", "ML output")
        summary_lines.append("ğŸ“ ç”Ÿæˆæ–‡ä»¶ç»Ÿè®¡:")
        
        try:
            file_count = 0
            total_size = 0
            
            # æ‰«æML outputç›®å½•åŠå­ç›®å½•
            for root, dirs, files in os.walk(ml_output_dir):
                for file in files:
                    if file.endswith(('.csv', '.pkl', '.npy', '.json', '.txt')):
                        file_path = os.path.join(root, file)
                        if os.path.exists(file_path):
                            size = os.path.getsize(file_path) / 1024  # KB
                            rel_path = os.path.relpath(file_path, ml_output_dir)
                            summary_lines.append(f"   ğŸ“„ {rel_path} ({size:.1f} KB)")
                            file_count += 1
                            total_size += size
            
            summary_lines.append("")
            summary_lines.append(f"   ğŸ“Š æ€»æ–‡ä»¶æ•°: {file_count}")
            summary_lines.append(f"   ğŸ’¾ æ€»å¤§å°: {total_size:.1f} KB ({total_size/1024:.2f} MB)")
            
        except Exception as e:
            summary_lines.append(f"   âš ï¸ æ–‡ä»¶æ‰«æå¤±è´¥: {str(e)}")
        
        summary_lines.append("")
        summary_lines.append("=" * 70)
        
        # æ ¹æ®ç»“æœåˆ¤æ–­æœ€ç»ˆçŠ¶æ€
        all_success = (feature_results.get('success', False) and 
                      target_results.get('success', False))
        
        if all_success:
            summary_lines.append("ğŸŠ å®Œæ•´æµç¨‹æˆåŠŸå®Œæˆï¼")
            summary_lines.append("âœ¨ ç°åœ¨å¯ä»¥å¼€å§‹æœºå™¨å­¦ä¹ å»ºæ¨¡äº†")
        else:
            summary_lines.append("âš ï¸ æµç¨‹éƒ¨åˆ†å®Œæˆï¼Œè¯·æ£€æŸ¥å¤±è´¥çš„æ­¥éª¤")
        
        summary_lines.append("=" * 70)
        
        # æ˜¾ç¤ºæ‘˜è¦
        summary_text = "\n".join(summary_lines)
        print(summary_text)
        
        # ä¿å­˜æ‘˜è¦åˆ°æ–‡ä»¶
        os.makedirs(ml_output_dir, exist_ok=True)
        summary_path = os.path.join(ml_output_dir, f'pipeline_summary_{datetime.now().strftime("%Y%m%d_%H%M%S")}.txt')
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(summary_text)
        
        print(f"\nğŸ’¾ æ‘˜è¦å·²ä¿å­˜: {os.path.basename(summary_path)}")
        return True
        
    except Exception as e:
        print(f"âŒ æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}")
        return False

def main():
    """
    ä¸»å‡½æ•° - å®Œæ•´çš„è‚¡ç¥¨æœºå™¨å­¦ä¹ é¢„å¤„ç†æµç¨‹
    åŒ…å«ï¼šç‰¹å¾å·¥ç¨‹ â†’ ç›®æ ‡å·¥ç¨‹ â†’ PCAçŠ¶æ€ç”Ÿæˆ
    """
    print("ğŸš€ è‚¡ç¥¨æœºå™¨å­¦ä¹ å®Œæ•´é¢„å¤„ç†æµç¨‹")
    print("=" * 70)
    print("åŒ…å«: ç‰¹å¾å·¥ç¨‹ â†’ ç›®æ ‡å·¥ç¨‹ â†’ PCAçŠ¶æ€ç”Ÿæˆ")
    print("=" * 70)
    
    start_time = datetime.now()
    
    try:
        # é…ç½®å‚æ•°
        config = {
            'symbol': '000001',  # å¹³å®‰é“¶è¡Œ
            'start_date': '2023-01-01',
            'end_date': '2024-12-31',
            'use_auto_features': True,  # æ˜¯å¦ä½¿ç”¨è‡ªåŠ¨ç‰¹å¾ç”Ÿæˆ
            'final_k_features': 15,      # æœ€ç»ˆç‰¹å¾æ•°é‡
            'target_periods': [1, 5, 10], # ç›®æ ‡æ—¶é—´çª—å£
            'pca_components': 0.9,       # PCAè§£é‡Šæ–¹å·®æ¯”ä¾‹
            'train_ratio': 0.8           # è®­ç»ƒé›†æ¯”ä¾‹
        }
        
        print("ğŸ“‹ æ‰§è¡Œé…ç½®:")
        for key, value in config.items():
            print(f"   {key}: {value}")
        print()
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs("machine learning/ML output/models", exist_ok=True)
        os.makedirs("machine learning/ML output/states", exist_ok=True)
        
        success_steps = 0
        total_steps = 3
        
        # === æ­¥éª¤1: ç‰¹å¾å·¥ç¨‹ ===
        feature_results = run_complete_feature_pipeline(
            symbol=config['symbol'],
            start_date=config['start_date'],
            end_date=config['end_date'],
            use_auto_features=config['use_auto_features'],
            final_k_features=config['final_k_features']
        )
        
        if feature_results.get('success'):
            success_steps += 1
            print("âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ")
        else:
            print("âŒ ç‰¹å¾å·¥ç¨‹å¤±è´¥ï¼Œæµç¨‹ç»ˆæ­¢")
            return False
        
        # === æ­¥éª¤2: ç›®æ ‡å˜é‡å·¥ç¨‹ ===
        target_results = run_complete_target_pipeline(
            scaled_features_df=feature_results['scaled_features_df'],
            symbol=config['symbol'],
            target_periods=config['target_periods']
        )
        
        if target_results.get('success'):
            success_steps += 1
            print("âœ… ç›®æ ‡å˜é‡å·¥ç¨‹å®Œæˆ")
        else:
            print("âŒ ç›®æ ‡å˜é‡å·¥ç¨‹å¤±è´¥ï¼Œæµç¨‹ç»ˆæ­¢")
            return False
        
        # === æ­¥éª¤3: PCAçŠ¶æ€ç”Ÿæˆ ===
        print(f"\nğŸ” æ­¥éª¤3: PCAçŠ¶æ€ç”Ÿæˆ")
        print("-" * 50)
        
        pca_results = None
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰CSVæ–‡ä»¶å¯ç”¨
            csv_path = feature_results.get('csv_path')
            if not csv_path or not os.path.exists(csv_path):
                print("âš ï¸ æ ‡å‡†åŒ–ç‰¹å¾CSVæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡PCAæ­¥éª¤")
            else:
                # åˆå§‹åŒ–PCAçŠ¶æ€ç”Ÿæˆå™¨
                pca_generator = PCAStateGenerator()
                
                # ç”ŸæˆPCAçŠ¶æ€
                pca_results = pca_generator.generate_pca_states(
                    csv_path=csv_path,
                    symbol=config['symbol'],
                    n_components=config['pca_components'],
                    train_ratio=config['train_ratio']
                )
                
                if pca_results and 'n_components' in pca_results:
                    success_steps += 1
                    print("âœ… PCAçŠ¶æ€ç”Ÿæˆå®Œæˆ")
                else:
                    print("âš ï¸ PCAçŠ¶æ€ç”Ÿæˆå¤±è´¥ï¼Œä½†å‰ç»­æ­¥éª¤å·²å®Œæˆ")
        
        except Exception as e:
            print(f"âš ï¸ PCAçŠ¶æ€ç”Ÿæˆå¼‚å¸¸: {str(e)}")
            print("   å‰ç»­æ­¥éª¤å·²å®Œæˆï¼Œå¯ä»¥ç»§ç»­åç»­åˆ†æ")
        
        # === ç”Ÿæˆæœ€ç»ˆæ‘˜è¦ ===
        generate_final_summary(feature_results, target_results, pca_results)
        
        # è®¡ç®—æ€»è€—æ—¶
        end_time = datetime.now()
        duration = end_time - start_time
        
        print(f"\nâ±ï¸ æµç¨‹æ€»è€—æ—¶: {duration.total_seconds():.1f} ç§’")
        print(f"ğŸ“Š æˆåŠŸæ­¥éª¤: {success_steps}/{total_steps}")
        
        if success_steps >= 2:  # è‡³å°‘ç‰¹å¾å·¥ç¨‹å’Œç›®æ ‡å·¥ç¨‹æˆåŠŸ
            print("\nğŸŠ æ ¸å¿ƒæµç¨‹æˆåŠŸå®Œæˆï¼")
            print("ğŸ“ æ‰€æœ‰ç»“æœæ–‡ä»¶å·²ä¿å­˜åˆ°: machine learning/ML output/")
            print("âœ¨ ç°åœ¨å¯ä»¥å¼€å§‹æœºå™¨å­¦ä¹ å»ºæ¨¡äº†")
            return True
        else:
            print(f"\nâš ï¸ æµç¨‹æœªèƒ½æˆåŠŸå®Œæˆ")
            return False
        
    except Exception as e:
        print(f"\nğŸ’¥ æµç¨‹å¼‚å¸¸ç»ˆæ­¢: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    # æ‰§è¡ŒPCAçŠ¶æ€ç”Ÿæˆ
    main()
