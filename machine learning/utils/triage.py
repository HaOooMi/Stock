#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
60åˆ†é’Ÿå¿«é€Ÿä½“æ£€ï¼ˆTriageï¼‰

åœ¨è¿›å…¥æœºå™¨å­¦ä¹ åŸºçº¿ä¹‹å‰,å…ˆå¿«é€ŸéªŒè¯ä¿¡å·æœ¬èº«æ˜¯å¦æœ‰åŒºåˆ†åŠ›ã€‚
å¦‚æœä½“æ£€æ˜¾ç¤º"ä¿¡å·æœ¬èº«æœ‰å¼±ä½†å­˜åœ¨çš„åŒºåˆ†åŠ›(IC/Spread>0)ä½†è¢«è½¬æ¢/æˆæœ¬/å¯¹é½é—®é¢˜åƒæ‰",
åº”å…ˆä¿®æ­£ç­–ç•¥è½¬æ¢ä¸å›æµ‹å‡è®¾ï¼›
å¦‚æœä½“æ£€æ˜¾ç¤º"æ ·æœ¬å¤–ICâ‰ˆ0ã€ç°‡æ”¶ç›Šåˆ†å±‚æ¶ˆå¤±",å†è¿›å…¥é˜¶æ®µ12(æœºå™¨å­¦ä¹ åŸºçº¿)ã€‚

6ä¸ªå¿«é€Ÿä½“æ£€æ­¥éª¤:
1. ä¿¡å·å¯¹é½ä¸æ³„éœ²æ£€æŸ¥
2. æˆæœ¬ä¸æ¢æ‰‹æ‹†è§£
3. æ’åºåŠ›ä½“æ£€(ä¸ä¾èµ–ç­–ç•¥)
4. çŠ¶æ€è¿‡æ»¤ä½“æ£€(PCA+KMeans)
5. é—¨æ§›/æŒæœ‰å‘¨æœŸå°ç½‘æ ¼
6. éšæœºåŸºå‡†ä¸å¹´åº¦åˆ‡ç‰‡

ç®€åŒ–åˆ¤æ–­:
- æ ·æœ¬å¤–ICâ‰¥0.02 æˆ– 5æ¡¶Spread>0 ä¸”ç¨³å®š â†’ å…ˆä¼˜åŒ–ä¿¡å·è½¬æ¢
- ICâ‰ˆ0ã€Spreadâ‰ˆ0 â†’ è¿›å…¥é˜¶æ®µ12,æˆ–å›åˆ°ç‰¹å¾/çŠ¶æ€å±‚

ä½œè€…: Assistant
æ—¥æœŸ: 2025-10-14
"""

import os
import sys
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from scipy import stats
from sklearn.cluster import KMeans


class QuickTriage:
    """
    60åˆ†é’Ÿå¿«é€Ÿä½“æ£€ç³»ç»Ÿ
    
    åœ¨è¿›å…¥å¤æ‚çš„æœºå™¨å­¦ä¹ æ¨¡å‹ä¹‹å‰,å¿«é€Ÿè¯Šæ–­ä¿¡å·è´¨é‡å’Œç­–ç•¥è½¬æ¢é—®é¢˜
    """
    
    def __init__(self, reports_dir: str = "ML output/reports"):
        """åˆå§‹åŒ–ä½“æ£€ç³»ç»Ÿ"""
        self.project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        
        if os.path.isabs(reports_dir):
            self.reports_dir = reports_dir
        else:
            self.reports_dir = os.path.join(self.project_root, reports_dir)
        
        os.makedirs(self.reports_dir, exist_ok=True)
        
        # ä½“æ£€æŠ¥å‘Š
        self.triage_report = []
        self.issues_found = []
        self.recommendations = []
        
        print("ğŸ¥ å¿«é€Ÿä½“æ£€ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ æŠ¥å‘Šç›®å½•: {self.reports_dir}")
    
    def log(self, message: str, level: str = "INFO"):
        """è®°å½•æ—¥å¿—"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_message = f"[{timestamp}] [{level}] {message}"
        print(log_message)
        self.triage_report.append(log_message)
    
    def add_issue(self, issue: str):
        """è®°å½•å‘ç°çš„é—®é¢˜"""
        self.issues_found.append(issue)
        self.log(f"âš ï¸  é—®é¢˜: {issue}", "WARN")
    
    def add_recommendation(self, recommendation: str):
        """æ·»åŠ å»ºè®®"""
        self.recommendations.append(recommendation)
        self.log(f"ğŸ’¡ å»ºè®®: {recommendation}", "SUGGEST")
    
    # ========== ä½“æ£€1: ä¿¡å·å¯¹é½ä¸æ³„éœ²æ£€æŸ¥ ==========
    
    def check_signal_alignment_and_leakage(self, signal_data: pd.DataFrame) -> Dict:
        """
        ä½“æ£€1: ä¿¡å·å¯¹é½ä¸æ³„éœ²æ£€æŸ¥
        
        æ£€æŸ¥é¡¹:
        1. ä¿¡å·ç”Ÿæˆæ—¶æ˜¯å¦ä½¿ç”¨äº†æœªæ¥æ•°æ®(look-ahead bias)
        2. ä¿¡å·ä¸æ”¶ç›Šçš„æ—¶é—´å¯¹é½æ˜¯å¦æ­£ç¡®
        3. æ˜¯å¦æœ‰æ•°æ®æ³„éœ²(target leakage)
        
        Parameters:
        -----------
        signal_data : pd.DataFrame
            åŒ…å«ä¿¡å·å’Œæœªæ¥æ”¶ç›Šçš„æ•°æ®
            å¿…é¡»åŒ…å«: signal_combined, future_return_5d, close
        
        Returns:
        --------
        dict: æ£€æŸ¥ç»“æœ
        """
        self.log("=" * 70)
        self.log("ä½“æ£€1: ä¿¡å·å¯¹é½ä¸æ³„éœ²æ£€æŸ¥")
        self.log("=" * 70)
        
        results = {
            'alignment_correct': True,
            'no_leakage': True,
            'issues': []
        }
        
        # æ£€æŸ¥1: ä¿¡å·æ˜¯å¦åœ¨æ­£ç¡®çš„æ—¶é—´ç‚¹ç”Ÿæˆ
        # ä¿¡å·åº”è¯¥åŸºäºå½“å‰åŠä¹‹å‰çš„æ•°æ®,ä¸åº”ä½¿ç”¨æœªæ¥æ•°æ®
        self.log("æ£€æŸ¥ä¿¡å·ç”Ÿæˆæ—¶ç‚¹...")
        
        # æ£€æŸ¥ä¿¡å·ä¸æœªæ¥æ”¶ç›Šçš„ç›¸å…³æ€§
        # ã€å…³é”®ä¿®å¤ã€‘ä½¿ç”¨T+1å¯¹é½é¿å…look-ahead bias
        signal = signal_data['signal_combined'].values
        returns = signal_data['future_return_5d'].values
        
        # ã€å…³é”®ä¿®å¤ã€‘T+1å¯¹é½: ä»Šå¤©çš„ä¿¡å·å†³å®šæ˜å¤©çš„ä»“ä½
        signal_t_plus_1 = np.roll(signal, 1)  # ä¿¡å·åç§»1å¤©
        signal_t_plus_1[0] = 0  # ç¬¬ä¸€å¤©æ— ä¿¡å·
        
        # ç§»é™¤NaN
        valid_mask = ~np.isnan(returns)
        signal_clean = signal_t_plus_1[valid_mask]  # ä½¿ç”¨å¯¹é½åçš„ä¿¡å·
        returns_clean = returns[valid_mask]
        
        if len(signal_clean) > 0:
            correlation = np.corrcoef(signal_clean, returns_clean)[0, 1]
            self.log(f"   ä¿¡å·ä¸æœªæ¥æ”¶ç›Šç›¸å…³æ€§(T+1å¯¹é½): {correlation:.4f}")
            
            # ã€ä¿®æ”¹é˜ˆå€¼ã€‘T+1å¯¹é½å,åˆç†ICåº”åœ¨0.02~0.15ä¹‹é—´
            if abs(correlation) > 0.15:  # é™ä½é˜ˆå€¼(åŸæ¥0.3,T+1ååº”æ˜¾è‘—é™ä½)
                issue = f"ä¿¡å·ä¸æœªæ¥æ”¶ç›Šç›¸å…³æ€§è¿‡é«˜({correlation:.4f}),å¯èƒ½å­˜åœ¨æ•°æ®æ³„éœ²"
                self.add_issue(issue)
                results['no_leakage'] = False
                results['issues'].append(issue)
            elif abs(correlation) < 0.02:
                issue = f"ä¿¡å·ä¸æœªæ¥æ”¶ç›Šç›¸å…³æ€§è¿‡ä½({correlation:.4f}),ä¿¡å·æ— é¢„æµ‹åŠ›"
                self.add_issue(issue)
                results['no_predictive_power'] = True
                results['issues'].append(issue)
            else:
                self.log(f"   âœ… ç›¸å…³æ€§æ­£å¸¸(IC={correlation:.4f}åœ¨åˆç†èŒƒå›´0.02~0.15)")
        
        # æ£€æŸ¥2: éªŒè¯ä¿¡å·ç”Ÿæˆä¸ä½¿ç”¨å½“æœŸæ”¶ç›Š
        self.log("\næ£€æŸ¥ä¿¡å·æ˜¯å¦ä½¿ç”¨å½“æœŸæ”¶ç›Š...")
        
        # å¦‚æœä¿¡å·å®Œå…¨ä¾èµ–äºå½“æœŸæ”¶ç›Š,è¯´æ˜å¯¹é½æœ‰é—®é¢˜
        if 'close' in signal_data.columns:
            current_return = signal_data['close'].pct_change().values
            current_return_clean = current_return[valid_mask]
            
            # è®¡ç®—ä¿¡å·åˆ‡æ¢ç‚¹ä¸å½“æœŸæ”¶ç›Šçš„å…³ç³»
            signal_changes = np.diff(signal_clean, prepend=signal_clean[0])
            signal_change_mask = signal_changes != 0
            
            if signal_change_mask.sum() > 0:
                # åœ¨ä¿¡å·å˜åŒ–æ—¶,æ£€æŸ¥æ˜¯å¦æ€»æ˜¯è·Ÿéšå½“æœŸæ”¶ç›Šæ–¹å‘
                changes_with_positive_return = np.sum(
                    (signal_changes[signal_change_mask] > 0) & 
                    (current_return_clean[signal_change_mask] > 0)
                )
                changes_with_negative_return = np.sum(
                    (signal_changes[signal_change_mask] < 0) & 
                    (current_return_clean[signal_change_mask] < 0)
                )
                
                alignment_ratio = (changes_with_positive_return + changes_with_negative_return) / signal_change_mask.sum()
                self.log(f"   ä¿¡å·å˜åŒ–ä¸å½“æœŸæ”¶ç›ŠåŒå‘æ¯”ä¾‹: {alignment_ratio:.2%}")
                
                if alignment_ratio > 0.7:
                    issue = f"ä¿¡å·å˜åŒ–è¿‡åº¦è·Ÿéšå½“æœŸæ”¶ç›Š({alignment_ratio:.2%}),å¯èƒ½å­˜åœ¨å¯¹é½é—®é¢˜"
                    self.add_issue(issue)
                    results['alignment_correct'] = False
                    results['issues'].append(issue)
                else:
                    self.log("   âœ… ä¿¡å·ç‹¬ç«‹äºå½“æœŸæ”¶ç›Š")
        
        # æ£€æŸ¥3: éªŒè¯æµ‹è¯•é›†ä¸å‚ä¸æ¨¡å‹è®­ç»ƒ
        self.log("\næ£€æŸ¥è®­ç»ƒ/æµ‹è¯•é›†åˆ†ç¦»...")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„è®­ç»ƒé›†è¿‡æ‹Ÿåˆè¿¹è±¡
        # å°†æ•°æ®åˆ†ä¸ºä¸¤åŠ,æ¯”è¾ƒæ€§èƒ½å·®å¼‚
        mid_point = len(signal_clean) // 2
        
        first_half_signal = signal_clean[:mid_point]
        first_half_return = returns_clean[:mid_point]
        second_half_signal = signal_clean[mid_point:]
        second_half_return = returns_clean[mid_point:]
        
        if len(first_half_signal) > 0 and first_half_signal.sum() > 0:
            first_half_perf = np.mean(first_half_return[first_half_signal == 1])
        else:
            first_half_perf = 0
            
        if len(second_half_signal) > 0 and second_half_signal.sum() > 0:
            second_half_perf = np.mean(second_half_return[second_half_signal == 1])
        else:
            second_half_perf = 0
        
        self.log(f"   å‰åŠæ®µä¿¡å·å¹³å‡æ”¶ç›Š: {first_half_perf:+.4f}")
        self.log(f"   ååŠæ®µä¿¡å·å¹³å‡æ”¶ç›Š: {second_half_perf:+.4f}")
        
        if first_half_perf > 0 and second_half_perf < -abs(first_half_perf) * 0.5:
            issue = "å‰ååŠæ®µæ€§èƒ½ä¸¥é‡èƒŒç¦»,å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆæˆ–æµ‹è¯•é›†æ³„éœ²"
            self.add_issue(issue)
            results['issues'].append(issue)
        else:
            self.log("   âœ… å‰ååŠæ®µæ€§èƒ½ä¸€è‡´æ€§è‰¯å¥½")
        
        # æ€»ç»“
        if results['alignment_correct'] and results['no_leakage']:
            self.log("\nâœ… ä½“æ£€1é€šè¿‡: æœªå‘ç°ä¿¡å·å¯¹é½æˆ–æ³„éœ²é—®é¢˜")
        else:
            self.log("\nâŒ ä½“æ£€1æœªé€šè¿‡: å‘ç°ä¿¡å·å¯¹é½æˆ–æ³„éœ²é—®é¢˜")
            self.add_recommendation("ä¿®æ­£ä¿¡å·ç”Ÿæˆé€»è¾‘,ç¡®ä¿ä¸ä½¿ç”¨æœªæ¥æ•°æ®")
        
        return results
    
    # ========== ä½“æ£€1A: ç ´åæ€§å¯¹ç…§å®éªŒ ==========
    
    def check_leakage_with_wrong_labels(self, signal_data: pd.DataFrame) -> Dict:
        """
        ä½“æ£€1A: ç ´åæ€§å¯¹ç…§å®éªŒ
        
        ä½¿ç”¨æ•…æ„é”™è¯¯çš„æ ‡ç­¾æ¥éªŒè¯æ˜¯å¦å­˜åœ¨æ•°æ®æ³„æ¼ã€‚
        å¦‚æœé”™è¯¯æ ‡ç­¾çš„æ€§èƒ½"æ›´å¥½",è¯´æ˜ç®¡çº¿é‡Œæœ‰ç©¿è¶Šã€‚
        
        Parameters:
        -----------
        signal_data : pd.DataFrame
            åŒ…å«closeåˆ—çš„æ•°æ®
        
        Returns:
        --------
        dict: å¯¹ç…§å®éªŒç»“æœ
        """
        self.log("=" * 70)
        self.log("ä½“æ£€1A: ç ´åæ€§å¯¹ç…§å®éªŒ (ç”¨é”™è¯¯æ ‡ç­¾éªŒè¯æ³„æ¼)")
        self.log("=" * 70)
        
        if 'close' not in signal_data.columns:
            self.log("âš ï¸  ç¼ºå°‘closeåˆ—,è·³è¿‡ç ´åæ€§å¯¹ç…§")
            return {'skipped': True}
        
        # è·å–PCAç‰¹å¾å’Œæ­£ç¡®æ ‡ç­¾
        pca_columns = [col for col in signal_data.columns if col.startswith('PC')]
        if len(pca_columns) == 0:
            self.log("âš ï¸  æœªæ‰¾åˆ°PCAç‰¹å¾,è·³è¿‡ç ´åæ€§å¯¹ç…§")
            return {'skipped': True}
        
        correct_label = signal_data['future_return_5d'].fillna(0).values
        feature = signal_data[pca_columns[0]].fillna(0).values
        
        # ã€å…³é”®ä¿®å¤ã€‘T+1å¯¹é½: ä»Šå¤©çš„ç‰¹å¾é¢„æµ‹æ˜å¤©çš„æ”¶ç›Š
        feature_t_plus_1 = np.roll(feature, 1)
        feature_t_plus_1[0] = 0  # ç¬¬ä¸€å¤©æ— ç‰¹å¾å€¼
        
        # è®¡ç®—æ­£ç¡®æ ‡ç­¾çš„IC
        valid_mask = ~np.isnan(signal_data['future_return_5d'].values)
        if valid_mask.sum() < 10:
            self.log("âš ï¸  æœ‰æ•ˆæ ·æœ¬è¿‡å°‘,è·³è¿‡ç ´åæ€§å¯¹ç…§")
            return {'skipped': True}
        
        feature_valid = feature_t_plus_1[valid_mask]  # ä½¿ç”¨å¯¹é½åçš„ç‰¹å¾
        correct_label_valid = correct_label[valid_mask]
        
        correct_ic, correct_p = stats.spearmanr(feature_valid, correct_label_valid)
        self.log(f"âœ… æ­£ç¡®æ ‡ç­¾IC(T+1å¯¹é½): {correct_ic:+.4f} (p={correct_p:.4f})")
        
        results = {
            'correct_ic': correct_ic,
            'wrong_ics': {},
            'leakage_detected': False
        }
        
        # é”™è¯¯æ ‡ç­¾1: è¿‡å»5å¤©æ”¶ç›Š(ç”¨äºæ£€æµ‹åŠ¨é‡ç‰¹å¾å¼ºåº¦,éæ³„æ¼æŒ‡æ ‡)
        self.log("\næµ‹è¯•é”™è¯¯æ ‡ç­¾1: è¿‡å»5å¤©æ”¶ç›Š...")
        wrong_label_1 = signal_data['close'].pct_change(5).fillna(0).values[valid_mask]
        wrong_ic_1, wrong_p_1 = stats.spearmanr(feature_valid, wrong_label_1)
        self.log(f"   è¿‡å»æ”¶ç›Š IC: {wrong_ic_1:+.4f} (p={wrong_p_1:.4f})")
        results['wrong_ics']['past_5d_return'] = wrong_ic_1
        
        # ã€é‡è¦ã€‘è¿‡å»æ”¶ç›ŠICé«˜ä¸æ˜¯æ³„æ¼,è€Œæ˜¯PCAæ•æ‰åŠ¨é‡ç‰¹å¾çš„æ­£å¸¸è¡¨ç°
        # å› ä¸ºç‰¹å¾å±‚åŒ…å«return_5d/momentum_5dç­‰,PCAè‡ªç„¶ä¼šä¸è¿‡å»æ”¶ç›Šç›¸å…³
        if abs(wrong_ic_1) > abs(correct_ic):
            momentum_strength = abs(wrong_ic_1) / (abs(correct_ic) + 1e-6)
            self.log(f"   â„¹ï¸  åŠ¨é‡å¼ºåº¦: {momentum_strength:.1f}x (è¿‡å»IC={wrong_ic_1:+.4f}, æœªæ¥IC={correct_ic:+.4f})")
            
            # åˆ¤æ–­åŠ¨é‡æ–¹å‘
            if correct_ic * wrong_ic_1 > 0:
                self.log("   ğŸ“ˆ åŠ¨é‡å»¶ç»­: è¿‡å»è¡¨ç°å¥½çš„æœªæ¥ç»§ç»­å¥½")
            else:
                self.log("   ğŸ”„ åŠ¨é‡åè½¬: è¿‡å»è¡¨ç°å¥½çš„æœªæ¥è¡¨ç°å·®ï¼ˆå½“å‰çŠ¶æ€ï¼‰")
            
            # åªæœ‰åœ¨è¿‡å»æ”¶ç›ŠICæç«¯é«˜æ—¶æ‰è­¦å‘Šï¼ˆå¯èƒ½æ˜¯shifté”™è¯¯ï¼‰
            if abs(wrong_ic_1) > abs(correct_ic) * 3.0:
                self.add_issue(f"è¿‡å»æ”¶ç›ŠIC({wrong_ic_1:+.4f})è¿œè¶…æœªæ¥æ”¶ç›ŠIC({correct_ic:+.4f})çš„3å€,è¯·æ£€æŸ¥ç‰¹å¾shiftæ–¹å‘")
                results['leakage_detected'] = True
        else:
            self.log("   âœ… æœªæ¥é¢„æµ‹æ€§ä¼˜äºå†å²ç›¸å…³æ€§")
        
        # é”™è¯¯æ ‡ç­¾2: éšæœºæ ‡ç­¾(çº¯å™ªå£°)
        self.log("\næµ‹è¯•é”™è¯¯æ ‡ç­¾2: éšæœºæ ‡ç­¾...")
        np.random.seed(42)
        wrong_label_2 = np.random.randn(len(feature_valid))
        wrong_ic_2, wrong_p_2 = stats.spearmanr(feature_valid, wrong_label_2)
        self.log(f"   é”™è¯¯æ ‡ç­¾2 IC: {wrong_ic_2:+.4f} (p={wrong_p_2:.4f})")
        results['wrong_ics']['random'] = wrong_ic_2
        
        if abs(wrong_ic_2) > abs(correct_ic):
            self.add_issue(f"éšæœºæ ‡ç­¾çš„IC({wrong_ic_2:+.4f})è¶…è¿‡æ­£ç¡®æ ‡ç­¾({correct_ic:+.4f}),ç–‘ä¼¼æ³„æ¼!")
            results['leakage_detected'] = True
        
        # é”™è¯¯æ ‡ç­¾3: å½“æœŸæ”¶ç›Š(Tè€ŒéT+h,å¯¹é½é”™è¯¯)
        self.log("\næµ‹è¯•é”™è¯¯æ ‡ç­¾3: å½“æœŸæ”¶ç›Š(å¯¹é½é”™è¯¯)...")
        wrong_label_3 = signal_data['close'].pct_change().fillna(0).values[valid_mask]
        wrong_ic_3, wrong_p_3 = stats.spearmanr(feature_valid, wrong_label_3)
        self.log(f"   é”™è¯¯æ ‡ç­¾3 IC: {wrong_ic_3:+.4f} (p={wrong_p_3:.4f})")
        results['wrong_ics']['current_return'] = wrong_ic_3
        
        if abs(wrong_ic_3) > abs(correct_ic) * 1.5:
            self.add_issue(f"å½“æœŸæ”¶ç›Šçš„IC({wrong_ic_3:+.4f})è¿œè¶…æ­£ç¡®æ ‡ç­¾({correct_ic:+.4f}),ç–‘ä¼¼å¯¹é½é”™è¯¯!")
            results['leakage_detected'] = True
        
        # æ€»ç»“
        if results['leakage_detected']:
            self.log("\nâŒ ç ´åæ€§å¯¹ç…§æœªé€šè¿‡: æ£€æµ‹åˆ°æ•°æ®æ³„æ¼è¿¹è±¡")
            self.add_recommendation("ç´§æ€¥æ£€æŸ¥æ ‡ç­¾ç”Ÿæˆã€æ•°æ®å¯¹é½å’Œç‰¹å¾å·¥ç¨‹ç®¡çº¿")
        else:
            self.log("\nâœ… ç ´åæ€§å¯¹ç…§é€šè¿‡: æœªæ£€æµ‹åˆ°æ˜æ˜¾æ³„æ¼")
        
        return results
    
    # ========== ä½“æ£€2: æˆæœ¬ä¸æ¢æ‰‹æ‹†è§£ ==========
    
    def analyze_cost_and_turnover(self, signal_data: pd.DataFrame, 
                                  transaction_cost: float = 0.002,
                                  slippage: float = 0.001) -> Dict:
        """
        ä½“æ£€2: æˆæœ¬ä¸æ¢æ‰‹æ‹†è§£
        
        åˆ†æäº¤æ˜“æˆæœ¬å’Œæ¢æ‰‹ç‡å¯¹ç­–ç•¥æ”¶ç›Šçš„å½±å“
        
        Parameters:
        -----------
        signal_data : pd.DataFrame
            åŒ…å«ä¿¡å·çš„æ•°æ®
        transaction_cost : float
            å•è¾¹äº¤æ˜“æˆæœ¬(åŒ…æ‹¬ä½£é‡‘ã€å°èŠ±ç¨ç­‰),é»˜è®¤0.2%
        slippage : float
            æ»‘ç‚¹æˆæœ¬,é»˜è®¤0.1%
        
        Returns:
        --------
        dict: æˆæœ¬åˆ†æç»“æœ
        """
        self.log("=" * 70)
        self.log("ä½“æ£€2: æˆæœ¬ä¸æ¢æ‰‹æ‹†è§£")
        self.log("=" * 70)
        
        signal = signal_data['signal_combined'].values
        returns = signal_data['future_return_5d'].fillna(0).values
        
        # ã€å…³é”®ä¿®å¤ã€‘ä¸¥æ ¼T+1æ‰§è¡Œï¼šä»Šå¤©çš„ä¿¡å·å†³å®šæ˜å¤©çš„ä»“ä½
        signal_t1 = np.roll(signal, 1)
        signal_t1[0] = 0  # ç¬¬ä¸€å¤©æ— ä¿¡å·
        
        # è®¡ç®—æ¢æ‰‹ç‡ï¼ˆæŒ‰å›åˆè®¡è´¹ï¼‰
        signal_changes = np.abs(np.diff(signal, prepend=signal[0]))
        flips = signal_changes.sum()
        roundtrips = flips / 2.0  # æ¯ä¸¤ä¸ªç¿»è½¬æ„æˆä¸€æ¬¡å®Œæ•´å›åˆï¼ˆå¼€+å¹³ï¼‰
        turnover_rate = roundtrips / len(signal)
        
        self.log(f"æ¢æ‰‹ç»Ÿè®¡:")
        self.log(f"   ä¿¡å·ç¿»è½¬æ¬¡æ•°: {flips:.0f}")
        self.log(f"   äº¤æ˜“å›åˆæ•°: {roundtrips:.1f}")
        self.log(f"   æ¢æ‰‹ç‡: {turnover_rate:.2%}")
        self.log(f"   å¹³å‡æŒæœ‰æœŸ: {1/turnover_rate:.1f} æœŸ" if turnover_rate > 0 else "   å¹³å‡æŒæœ‰æœŸ: N/A")
        
        # è®¡ç®—ä¸åŒæˆæœ¬å‡è®¾ä¸‹çš„æ”¶ç›Šï¼ˆä½¿ç”¨T+1å¯¹é½çš„ä¿¡å·ï¼‰
        # ç­–ç•¥æ”¶ç›Š(ä¸è€ƒè™‘æˆæœ¬)
        strategy_returns_gross = signal_t1 * returns
        gross_total_return = float(np.sum(strategy_returns_gross))
        
        # è®¡ç®—äº¤æ˜“æˆæœ¬ï¼ˆæŒ‰å›åˆè®¡è´¹ï¼ŒåŒè¾¹æˆæœ¬ï¼‰
        per_roundtrip_cost = (transaction_cost + slippage) * 2  # åŒè¾¹æˆæœ¬
        total_transaction_cost = roundtrips * per_roundtrip_cost
        
        # å‡€æ”¶ç›Š
        net_total_return = gross_total_return - total_transaction_cost
        
        self.log(f"\næ”¶ç›Šæ‹†è§£:")
        self.log(f"   æ¯›æ”¶ç›Š: {gross_total_return:+.4f}")
        self.log(f"   äº¤æ˜“æˆæœ¬: {total_transaction_cost:-.4f} ({roundtrips:.1f}å›åˆ Ã— {per_roundtrip_cost:.4f})")
        self.log(f"   å‡€æ”¶ç›Š: {net_total_return:+.4f}")
        self.log(f"   æˆæœ¬ä¾µèš€æ¯”ä¾‹: {(total_transaction_cost/abs(gross_total_return)*100):.1f}%" if gross_total_return != 0 else "   æˆæœ¬ä¾µèš€æ¯”ä¾‹: N/A")
        
        # åˆ¤æ–­æˆæœ¬å½±å“
        results = {
            'turnover_count': flips,
            'roundtrips': roundtrips,
            'turnover_rate': turnover_rate,
            'gross_return': gross_total_return,
            'transaction_cost': total_transaction_cost,
            'net_return': net_total_return,
            'cost_erosion_ratio': total_transaction_cost / abs(gross_total_return) if gross_total_return != 0 else 0
        }
        
        if results['cost_erosion_ratio'] > 0.5:
            issue = f"äº¤æ˜“æˆæœ¬åƒæ‰{results['cost_erosion_ratio']*100:.0f}%çš„æ¯›æ”¶ç›Š"
            self.add_issue(issue)
            self.add_recommendation("é™ä½æ¢æ‰‹ç‡:å»¶é•¿æŒæœ‰å‘¨æœŸã€æé«˜ä¿¡å·é—¨æ§›ã€åˆå¹¶ç›¸é‚»ä¿¡å·")
        elif results['cost_erosion_ratio'] > 0.3:
            self.log("\nâš ï¸  äº¤æ˜“æˆæœ¬è¾ƒé«˜,å»ºè®®ä¼˜åŒ–æ¢æ‰‹")
            self.add_recommendation("é€‚åº¦é™ä½æ¢æ‰‹ç‡å¯æå‡å‡€æ”¶ç›Š")
        else:
            self.log("\nâœ… æˆæœ¬æ§åˆ¶è‰¯å¥½")
        
        # ä¸åŒæˆæœ¬å‡è®¾çš„æ•æ„Ÿæ€§åˆ†æ
        self.log("\næˆæœ¬æ•æ„Ÿæ€§åˆ†æ:")
        cost_scenarios = [0.001, 0.002, 0.003, 0.005]
        
        for cost in cost_scenarios:
            scenario_cost = roundtrips * cost * 2  # æŒ‰å›åˆè®¡è´¹
            scenario_net = gross_total_return - scenario_cost
            self.log(f"   æˆæœ¬{cost*100:.2f}%: å‡€æ”¶ç›Š {scenario_net:+.4f}")
        
        return results
    
    # ========== ä½“æ£€3: æ’åºåŠ›ä½“æ£€ ==========
    
    def check_ranking_power(self, signal_data: pd.DataFrame, n_quantiles: int = 5) -> Dict:
        """
        ä½“æ£€3: æ’åºåŠ›ä½“æ£€(ä¸ä¾èµ–ç­–ç•¥)
        
        ä½¿ç”¨IC(ä¿¡æ¯ç³»æ•°)å’Œåˆ†å±‚æ”¶ç›Šæ£€éªŒä¿¡å·çš„æ’åºèƒ½åŠ›
        
        Parameters:
        -----------
        signal_data : pd.DataFrame
            å¿…é¡»åŒ…å«ä¿¡å·ç›¸å…³ç‰¹å¾å’Œfuture_return_5d
        n_quantiles : int
            åˆ†å±‚æ•°é‡,é»˜è®¤5
        
        Returns:
        --------
        dict: æ’åºåŠ›åˆ†æç»“æœ
        """
        self.log("=" * 70)
        self.log("ä½“æ£€3: æ’åºåŠ›ä½“æ£€(ä¸ä¾èµ–ç­–ç•¥)")
        self.log("=" * 70)
        
        # è·å–PCAç‰¹å¾ä½œä¸ºä¿¡å·å¼ºåº¦ä»£ç†
        pca_columns = [col for col in signal_data.columns if col.startswith('PC')]
        
        if len(pca_columns) == 0:
            self.log("âš ï¸  æœªæ‰¾åˆ°PCAç‰¹å¾,è·³è¿‡æ’åºåŠ›æ£€éªŒ")
            return {'skipped': True}
        
        returns = signal_data['future_return_5d'].fillna(0).values
        
        results = {
            'ic_values': {},
            'quantile_returns': {},
            'spread': None,
            'ranking_power': 'unknown'
        }
        
        # è®¡ç®—å„PCAæˆåˆ†çš„ICå€¼
        self.log("è®¡ç®—ä¿¡æ¯ç³»æ•°(IC,T+1å¯¹é½)...")
        
        ic_values = []
        for pc in pca_columns[:5]:  # åªçœ‹å‰5ä¸ªä¸»æˆåˆ†
            feature = signal_data[pc].fillna(0).values
            
            # ã€å…³é”®ä¿®å¤ã€‘T+1å¯¹é½: ä»Šå¤©çš„ç‰¹å¾é¢„æµ‹æ˜å¤©çš„æ”¶ç›Š
            feature_t_plus_1 = np.roll(feature, 1)
            feature_t_plus_1[0] = 0  # ç¬¬ä¸€å¤©æ— ç‰¹å¾å€¼
            
            # è®¡ç®—IC (Spearmanç›¸å…³ç³»æ•°)
            valid_mask = ~np.isnan(returns)
            if valid_mask.sum() > 10:
                ic, p_value = stats.spearmanr(feature_t_plus_1[valid_mask], returns[valid_mask])
                ic_values.append((pc, ic, p_value))
                self.log(f"   {pc}: IC={ic:+.4f} (p={p_value:.4f})")
        
        # é€‰æ‹©ICç»å¯¹å€¼æœ€å¤§çš„ä¸»æˆåˆ†ä½œä¸ºä¿¡å·
        if len(ic_values) > 0:
            ic_values_sorted = sorted(ic_values, key=lambda x: abs(x[1]), reverse=True)
            best_pc, best_ic, best_p = ic_values_sorted[0]
            
            self.log(f"\næœ€ä½³ä¿¡å·: {best_pc} (IC={best_ic:+.4f})")
            results['ic_values'] = {pc: ic for pc, ic, p in ic_values}
            results['best_ic'] = best_ic
            results['best_pc'] = best_pc
            
            # åˆ†å±‚æµ‹è¯•
            self.log(f"\nåˆ†{n_quantiles}å±‚æ”¶ç›Šåˆ†æ...")
            
            feature = signal_data[best_pc].fillna(0).values
            valid_mask = ~np.isnan(returns)
            feature_clean = feature[valid_mask]
            returns_clean = returns[valid_mask]
            
            # æŒ‰ä¿¡å·å¼ºåº¦åˆ†å±‚
            quantile_labels = pd.qcut(feature_clean, q=n_quantiles, labels=False, duplicates='drop')
            
            quantile_returns = []
            for q in range(n_quantiles):
                q_mask = quantile_labels == q
                if q_mask.sum() > 0:
                    q_return = returns_clean[q_mask].mean()
                    quantile_returns.append(q_return)
                    self.log(f"   Q{q+1} (n={q_mask.sum():4d}): {q_return:+.6f}")
                else:
                    quantile_returns.append(0)
            
            # è®¡ç®—Spread (Q5 - Q1)
            spread = quantile_returns[-1] - quantile_returns[0]
            results['quantile_returns'] = quantile_returns
            results['spread'] = spread
            
            self.log(f"\nå¤šç©ºä»·å·®(Spread): {spread:+.6f}")
            
            # åˆ¤æ–­æ’åºåŠ›
            if abs(best_ic) >= 0.02 and spread * np.sign(best_ic) > 0:
                self.log("âœ… ä¿¡å·æœ‰æ’åºåŠ› (ICâ‰¥0.02 ä¸” Spreadæ–¹å‘ä¸€è‡´)")
                results['ranking_power'] = 'strong'
                self.add_recommendation("ä¿¡å·æœ‰åŒºåˆ†åŠ›,åº”ä¼˜å…ˆä¼˜åŒ–ä¿¡å·è½¬æ¢å’Œæˆæœ¬  æ§åˆ¶")
            elif abs(best_ic) >= 0.01:
                self.log("âš ï¸  ä¿¡å·æœ‰å¼±æ’åºåŠ› (0.01â‰¤IC<0.02)")
                results['ranking_power'] = 'weak'
                self.add_recommendation("ä¿¡å·æœ‰å¼±åŒºåˆ†åŠ›,å¯å°è¯•ä¼˜åŒ–ç‰¹å¾å·¥ç¨‹æˆ–ä¿¡å·ç»„åˆ")
            else:
                self.log("âŒ ä¿¡å·æ— æ’åºåŠ› (IC<0.01)")
                results['ranking_power'] = 'none'
                self.add_issue("ä¿¡å·æœ¬èº«æ— åŒºåˆ†åŠ›")
                self.add_recommendation("åº”å›åˆ°ç‰¹å¾å·¥ç¨‹æˆ–è¿›å…¥é˜¶æ®µ12(æœºå™¨å­¦ä¹ åŸºçº¿)")
        
        return results
    
    # ========== ä½“æ£€4: çŠ¶æ€è¿‡æ»¤ä½“æ£€ ==========
    
    def check_state_filtering(self, signal_data: pd.DataFrame, 
                              k_values: List[int] = [3, 4, 5]) -> Dict:
        """
        ä½“æ£€4: çŠ¶æ€è¿‡æ»¤ä½“æ£€(PCA+KMeans)
        
        æ£€æŸ¥èšç±»çŠ¶æ€è¿‡æ»¤æ˜¯å¦æœ‰æ•ˆ
        
        Parameters:
        -----------
        signal_data : pd.DataFrame
            åŒ…å«PCAç‰¹å¾çš„æ•°æ®
        k_values : List[int]
            è¦æµ‹è¯•çš„kå€¼åˆ—è¡¨
        
        Returns:
        --------
        dict: çŠ¶æ€è¿‡æ»¤åˆ†æç»“æœ
        """
        self.log("=" * 70)
        self.log("ä½“æ£€4: çŠ¶æ€è¿‡æ»¤ä½“æ£€(PCA+KMeans)")
        self.log("=" * 70)
        
        pca_columns = [col for col in signal_data.columns if col.startswith('PC')]
        
        if len(pca_columns) == 0:
            self.log("âš ï¸  æœªæ‰¾åˆ°PCAç‰¹å¾,è·³è¿‡çŠ¶æ€è¿‡æ»¤æ£€éªŒ")
            return {'skipped': True}
        
        X_pca = signal_data[pca_columns].fillna(0).values
        returns = signal_data['future_return_5d'].fillna(0).values
        
        # åˆ†è®­ç»ƒ/æµ‹è¯•é›†(8:2)
        split_point = int(len(X_pca) * 0.8)
        X_train = X_pca[:split_point]
        X_test = X_pca[split_point:]
        returns_train = returns[:split_point]
        returns_test = returns[split_point:]
        
        self.log(f"æ•°æ®åˆ‡åˆ†: è®­ç»ƒé›† {len(X_train)}, æµ‹è¯•é›† {len(X_test)}")
        
        results = {
            'k_results': {},
            'best_k': None,
            'consistency_check': False
        }
        
        # æµ‹è¯•ä¸åŒkå€¼
        for k in k_values:
            self.log(f"\næµ‹è¯• k={k}...")
            
            # ä»…åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒ
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            train_labels = kmeans.fit_predict(X_train)
            test_labels = kmeans.predict(X_test)
            
            # è®¡ç®—æ¯ä¸ªç°‡çš„æ”¶ç›Š
            train_cluster_returns = []
            test_cluster_returns = []
            train_cluster_pcts = []
            test_cluster_pcts = []
            
            MIN_CLUSTER_PCT = 0.05  # æœ€å°ç°‡å æ¯”5%
            
            for c in range(k):
                train_mask = train_labels == c
                test_mask = test_labels == c
                
                train_pct = train_mask.sum() / len(train_labels)
                test_pct = test_mask.sum() / len(test_labels)
                
                train_cluster_pcts.append(train_pct)
                test_cluster_pcts.append(test_pct)
                
                # æ ‡è®°å æ¯”è¿‡å°çš„ç°‡
                is_valid = train_pct >= MIN_CLUSTER_PCT
                
                train_return = returns_train[train_mask].mean() if train_mask.sum() > 0 else 0
                test_return = returns_test[test_mask].mean() if test_mask.sum() > 0 else 0
                
                train_cluster_returns.append(train_return if is_valid else -999)  # æ— æ•ˆç°‡æ ‡è®°ä¸ºæä½æ”¶ç›Š
                test_cluster_returns.append(test_return)
                
                status = "âœ…" if is_valid else f"âŒ (å æ¯”<{MIN_CLUSTER_PCT:.0%})"
                self.log(f"   ç°‡{c}: {status} è®­ç»ƒ={train_return:+.6f} ({train_pct:5.1%}), "
                        f"æµ‹è¯•={test_return:+.6f} ({test_pct:5.1%})")
            
            # æ‰¾åˆ°è®­ç»ƒé›†æœ€ä½³ç°‡(æ’é™¤å æ¯”è¿‡å°çš„ç°‡)
            valid_clusters = [i for i in range(k) if train_cluster_pcts[i] >= MIN_CLUSTER_PCT]
            
            if len(valid_clusters) == 0:
                self.log(f"   âš ï¸ k={k} æ²¡æœ‰ç¬¦åˆå æ¯”è¦æ±‚çš„ç°‡!")
                best_cluster_idx = np.argmax(train_cluster_returns)  # é€€åŒ–åˆ°å…¨éƒ¨ç°‡
            else:
                valid_returns = [train_cluster_returns[i] for i in valid_clusters]
                best_idx_in_valid = np.argmax(valid_returns)
                best_cluster_idx = valid_clusters[best_idx_in_valid]
            
            best_train_return = train_cluster_returns[best_cluster_idx]
            if best_train_return == -999:
                best_train_return = 0  # æ¢å¤çœŸå®å€¼
                best_train_return = returns_train[train_labels == best_cluster_idx].mean()
            best_test_return = test_cluster_returns[best_cluster_idx]
            
            # æ£€æŸ¥å æ¯”
            train_best_pct = train_cluster_pcts[best_cluster_idx]
            test_best_pct = test_cluster_pcts[best_cluster_idx]
            
            self.log(f"   ğŸ† æœ€ä½³ç°‡{best_cluster_idx}: å æ¯” è®­ç»ƒ={train_best_pct:.1%}, æµ‹è¯•={test_best_pct:.1%}")
            
            results['k_results'][k] = {
                'best_cluster': best_cluster_idx,
                'train_return': best_train_return,
                'test_return': best_test_return,
                'train_pct': train_best_pct,
                'test_pct': test_best_pct,
                'direction_consistent': (best_train_return * best_test_return > 0)
            }
            
            # æ£€æŸ¥å æ¯”æ˜¯å¦åˆç†(10%-60%)
            if train_best_pct < 0.1 or train_best_pct > 0.6:
                self.add_issue(f"k={k} æœ€ä½³ç°‡å æ¯”å¼‚å¸¸: {train_best_pct:.1%}")
        
        # æ£€æŸ¥ä¸€è‡´æ€§
        self.log("\nä¸€è‡´æ€§æ£€æŸ¥:")
        
        consistent_ks = []
        for k, k_result in results['k_results'].items():
            if k_result['direction_consistent']:
                consistent_ks.append(k)
                self.log(f"   k={k}: âœ… æ–¹å‘ä¸€è‡´")
            else:
                self.log(f"   k={k}: âŒ æ–¹å‘ä¸ä¸€è‡´")
        
        if len(consistent_ks) >= 2:
            results['consistency_check'] = True
            self.log("\nâœ… å¤šä¸ªkå€¼æ–¹å‘ä¸€è‡´,çŠ¶æ€è¿‡æ»¤æœ‰æ•ˆ")
            self.add_recommendation("çŠ¶æ€è¿‡æ»¤æœ‰æ•ˆ,å¯ç»§ç»­ä½¿ç”¨èšç±»ç­–ç•¥")
        else:
            self.log("\nâŒ çŠ¶æ€è¿‡æ»¤æ–¹å‘ä¸ä¸€è‡´,å¯èƒ½è¿‡æ‹Ÿåˆ")
            self.add_issue("èšç±»çŠ¶æ€è¿‡æ»¤ä¸ç¨³å®š")
            self.add_recommendation("è€ƒè™‘ç®€åŒ–çŠ¶æ€å®šä¹‰æˆ–ä½¿ç”¨å…¶ä»–è¿‡æ»¤æ–¹å¼")
        
        return results
    
    # ========== ä½“æ£€5: é—¨æ§›/æŒæœ‰å‘¨æœŸå°ç½‘æ ¼ ==========
    
    def grid_search_threshold_holding(self, signal_data: pd.DataFrame,
                                     quantiles: List[float] = [0.6, 0.7, 0.8, 0.9],
                                     hold_periods: List[int] = [1, 3, 5]) -> Dict:
        """
        ä½“æ£€5: é—¨æ§›/æŒæœ‰å‘¨æœŸå°ç½‘æ ¼æœç´¢
        
        é€šè¿‡å°ç½‘æ ¼æœç´¢æ‰¾åˆ°æœ€ä¼˜çš„ä¿¡å·é—¨æ§›å’ŒæŒæœ‰å‘¨æœŸ
        
        Parameters:
        -----------
        signal_data : pd.DataFrame
            åŒ…å«PCAç‰¹å¾çš„æ•°æ®
        quantiles : List[float]
            ä¿¡å·é—¨æ§›åˆ†ä½æ•°åˆ—è¡¨
        hold_periods : List[int]
            æŒæœ‰å‘¨æœŸåˆ—è¡¨
        
        Returns:
        --------
        dict: ç½‘æ ¼æœç´¢ç»“æœ
        """
        self.log("=" * 70)
        self.log("ä½“æ£€5: é—¨æ§›/æŒæœ‰å‘¨æœŸå°ç½‘æ ¼æœç´¢")
        self.log("=" * 70)
        
        pca_columns = [col for col in signal_data.columns if col.startswith('PC')]
        
        if len(pca_columns) == 0 or 'future_return_5d' not in signal_data.columns:
            self.log("âš ï¸  æ•°æ®ä¸è¶³,è·³è¿‡ç½‘æ ¼æœç´¢")
            return {'skipped': True}
        
        # ä½¿ç”¨PC1ä½œä¸ºä¿¡å·å¼ºåº¦
        signal_strength = signal_data[pca_columns[0]].fillna(0).values
        
        # åˆ†è®­ç»ƒ/æµ‹è¯•é›†
        split_point = int(len(signal_strength) * 0.8)
        
        results = {
            'grid_results': [],
            'best_combo': None
        }
        
        self.log(f"ç½‘æ ¼: quantiles={quantiles}, hold_periods={hold_periods}")
        self.log(f"æ€»å…±æµ‹è¯• {len(quantiles) * len(hold_periods)} ç§ç»„åˆ\n")
        
        # ç½‘æ ¼æœç´¢
        for q in quantiles:
            for hold_n in hold_periods:
                # è®­ç»ƒé›†
                train_strength = signal_strength[:split_point]
                train_returns = signal_data['future_return_5d'].iloc[:split_point].fillna(0).values
                
                # ç”Ÿæˆä¿¡å·: ä¿¡å·å¼ºåº¦ > qåˆ†ä½æ•°
                train_threshold = np.quantile(train_strength, q)
                train_signal = (train_strength >= train_threshold).astype(int)
                
                # åº”ç”¨æŒæœ‰å‘¨æœŸ: ä¸€æ—¦ä¹°å…¥,æŒæœ‰hold_næœŸ
                train_signal_hold = self._apply_holding_period(train_signal, hold_n)
                
                # è®¡ç®—è®­ç»ƒé›†æ”¶ç›Š
                if hold_n == 1:
                    train_return = (train_signal_hold * train_returns).sum()
                else:
                    # æŒæœ‰æœŸå†…ç´¯è®¡æ”¶ç›Š
                    train_return = self._calculate_holding_return(train_signal_hold, train_returns, hold_n)
                
                # æµ‹è¯•é›†
                test_strength = signal_strength[split_point:]
                test_returns = signal_data['future_return_5d'].iloc[split_point:].fillna(0).values
                
                # ä½¿ç”¨è®­ç»ƒé›†é˜ˆå€¼
                test_signal = (test_strength >= train_threshold).astype(int)
                test_signal_hold = self._apply_holding_period(test_signal, hold_n)
                
                if hold_n == 1:
                    test_return = (test_signal_hold * test_returns).sum()
                else:
                    test_return = self._calculate_holding_return(test_signal_hold, test_returns, hold_n)
                
                # è®°å½•ç»“æœ
                combo_result = {
                    'quantile': q,
                    'hold_period': hold_n,
                    'train_return': train_return,
                    'test_return': test_return,
                    'train_signal_ratio': train_signal_hold.mean(),
                    'test_signal_ratio': test_signal_hold.mean()
                }
                results['grid_results'].append(combo_result)
                
                self.log(f"q={q:.1f}, hold={hold_n}: "
                        f"è®­ç»ƒ={train_return:+.4f} ({train_signal_hold.mean():.1%}), "
                        f"æµ‹è¯•={test_return:+.4f} ({test_signal_hold.mean():.1%})")
        
        # é€‰æ‹©æµ‹è¯•é›†è¡¨ç°æœ€å¥½çš„ç»„åˆ
        results['grid_results'].sort(key=lambda x: x['test_return'], reverse=True)
        best_combo = results['grid_results'][0]
        results['best_combo'] = best_combo
        
        self.log(f"\næœ€ä½³ç»„åˆ (æ ·æœ¬å¤–):")
        self.log(f"   é—¨æ§›åˆ†ä½æ•°: {best_combo['quantile']:.1f}")
        self.log(f"   æŒæœ‰å‘¨æœŸ: {best_combo['hold_period']}")
        self.log(f"   æµ‹è¯•æ”¶ç›Š: {best_combo['test_return']:+.4f}")
        self.log(f"   ä¿¡å·æ¯”ä¾‹: {best_combo['test_signal_ratio']:.1%}")
        
        # ä¸é»˜è®¤å‚æ•°å¯¹æ¯”
        default_combo = [c for c in results['grid_results'] 
                        if c['quantile'] == 0.7 and c['hold_period'] == 1]
        
        if len(default_combo) > 0:
            improvement = best_combo['test_return'] - default_combo[0]['test_return']
            if improvement > 0.01:
                self.log(f"\nâœ… ç½‘æ ¼æœç´¢æå‡æ”¶ç›Š: {improvement:+.4f}")
                self.add_recommendation(f"ä½¿ç”¨æœ€ä½³å‚æ•°ç»„åˆ: q={best_combo['quantile']:.1f}, hold={best_combo['hold_period']}")
            else:
                self.log(f"\nâš ï¸  ç½‘æ ¼æœç´¢æå‡æœ‰é™: {improvement:+.4f}")
        
        return results
    
    def _apply_holding_period(self, signal: np.ndarray, hold_n: int) -> np.ndarray:
        """åº”ç”¨æŒæœ‰å‘¨æœŸ: ä¹°å…¥åæŒæœ‰hold_næœŸ"""
        signal_hold = signal.copy()
        
        for i in range(1, len(signal_hold)):
            if signal[i-1] == 1:
                # å‰ä¸€æœŸæœ‰ä¿¡å·,ç»§ç»­æŒæœ‰
                for j in range(1, hold_n):
                    if i + j < len(signal_hold):
                        signal_hold[i + j] = 1
        
        return signal_hold
    
    def _calculate_holding_return(self, signal: np.ndarray, returns: np.ndarray, hold_n: int) -> float:
        """è®¡ç®—æŒæœ‰æœŸæ”¶ç›Š"""
        total_return = 0
        position = 0  # å½“å‰æŒä»“
        
        for i in range(len(signal)):
            if signal[i] == 1 and position == 0:
                # ä¹°å…¥
                position = 1
                holding_days = 0
            
            if position == 1:
                # æŒä»“ä¸­,ç´¯è®¡æ”¶ç›Š
                total_return += returns[i]
                holding_days += 1
                
                if holding_days >= hold_n:
                    # æŒæœ‰æœŸæ»¡,å–å‡º
                    position = 0
        
        return total_return
    
    # ========== ä½“æ£€6: éšæœºåŸºå‡†ä¸å¹´åº¦åˆ‡ç‰‡ ==========
    
    def check_random_baseline_and_yearly(self, signal_data: pd.DataFrame,
                                        n_random: int = 100) -> Dict:
        """
        ä½“æ£€6: éšæœºåŸºå‡†ä¸å¹´åº¦åˆ‡ç‰‡
        
        Parameters:
        -----------
        signal_data : pd.DataFrame
            åŒ…å«ä¿¡å·çš„æ•°æ®
        n_random : int
            éšæœºæ¨¡æ‹Ÿæ¬¡æ•°
        
        Returns:
        --------
        dict: éšæœºåŸºå‡†å’Œå¹´åº¦åˆ†æç»“æœ
        """
        self.log("=" * 70)
        self.log("ä½“æ£€6: éšæœºåŸºå‡†ä¸å¹´åº¦åˆ‡ç‰‡")
        self.log("=" * 70)
        
        signal = signal_data['signal_combined'].values
        returns = signal_data['future_return_5d'].fillna(0).values
        
        # ç­–ç•¥æ”¶ç›Š
        strategy_returns = signal * returns
        strategy_total = strategy_returns.sum()
        
        # éšæœºåŸºå‡†
        self.log(f"è¿è¡Œ{n_random}æ¬¡éšæœºæ¨¡æ‹Ÿ...")
        
        signal_ratio = signal.mean()
        random_results = []
        
        np.random.seed(42)
        for i in range(n_random):
            # ç”Ÿæˆç›¸åŒå æ¯”çš„éšæœºä¿¡å·
            n_signals = int(len(signal) * signal_ratio)
            random_signal = np.zeros(len(signal))
            if n_signals > 0:
                random_indices = np.random.choice(len(signal), n_signals, replace=False)
                random_signal[random_indices] = 1
            
            random_return = (random_signal * returns).sum()
            random_results.append(random_return)
        
        random_results = np.array(random_results)
        random_mean = random_results.mean()
        random_std = random_results.std()
        
        # è®¡ç®—ç­–ç•¥æ’å
        percentile = (random_results < strategy_total).mean()
        z_score = (strategy_total - random_mean) / random_std if random_std > 0 else 0
        
        self.log(f"\néšæœºåŸºå‡†å¯¹æ¯”:")
        self.log(f"   ç­–ç•¥æ”¶ç›Š: {strategy_total:+.4f}")
        self.log(f"   éšæœºå¹³å‡: {random_mean:+.4f} Â± {random_std:.4f}")
        self.log(f"   ç­–ç•¥åˆ†ä½æ•°: {percentile:.1%}")
        self.log(f"   Z-score: {z_score:+.2f}")
        
        results = {
            'strategy_return': strategy_total,
            'random_mean': random_mean,
            'random_std': random_std,
            'percentile': percentile,
            'z_score': z_score
        }
        
        if percentile < 0.6:
            self.add_issue(f"ç­–ç•¥æœªæ˜¾è‘—ä¼˜äºéšæœºåŸºå‡†(åˆ†ä½æ•°={percentile:.1%})")
            self.add_recommendation("ç­–ç•¥å¯èƒ½ç¼ºä¹çœŸå®ä¿¡å·,å»ºè®®é‡æ–°å®¡è§†ç‰¹å¾å’ŒçŠ¶æ€å®šä¹‰")
        elif z_score >= 2:
            self.log("âœ… ç­–ç•¥æ˜¾è‘—ä¼˜äºéšæœºåŸºå‡†(2Ïƒä»¥ä¸Š)")
        elif z_score >= 1:
            self.log("âš ï¸  ç­–ç•¥é€‚åº¦ä¼˜äºéšæœºåŸºå‡†(1-2Ïƒ)")
        else:
            self.log("âŒ ç­–ç•¥æœªæ˜¾è‘—ä¼˜äºéšæœºåŸºå‡†(<1Ïƒ)")
        
        # å¹´åº¦åˆ‡ç‰‡
        self.log("\nå¹´åº¦åˆ‡ç‰‡åˆ†æ:")
        
        if 'datetime' in signal_data.index.names or isinstance(signal_data.index, pd.DatetimeIndex):
            dates = signal_data.index
            
            # æŒ‰å¹´ä»½åˆ†ç»„
            years = dates.year.unique()
            
            yearly_results = []
            for year in sorted(years):
                year_mask = dates.year == year
                year_signal = signal[year_mask]
                year_returns = returns[year_mask]
                
                year_strategy_return = (year_signal * year_returns).sum()
                year_signal_ratio = year_signal.mean()
                
                yearly_results.append({
                    'year': year,
                    'return': year_strategy_return,
                    'signal_ratio': year_signal_ratio,
                    'n_samples': year_mask.sum()
                })
                
                self.log(f"   {year}: æ”¶ç›Š={year_strategy_return:+.4f}, "
                        f"ä¿¡å·ç‡={year_signal_ratio:.1%}, n={year_mask.sum()}")
            
            # æ£€æŸ¥å¹´åº¦ä¸€è‡´æ€§
            yearly_returns = [y['return'] for y in yearly_results]
            
            if len(yearly_returns) > 1:
                positive_years = sum(1 for r in yearly_returns if r > 0)
                consistency = positive_years / len(yearly_returns)
                
                self.log(f"\nå¹´åº¦ä¸€è‡´æ€§: {positive_years}/{len(yearly_returns)} ({consistency:.0%})")
                
                if consistency >= 0.7:
                    self.log("âœ… å¹´åº¦è¡¨ç°ä¸€è‡´")
                elif consistency >= 0.5:
                    self.log("âš ï¸  å¹´åº¦è¡¨ç°ä¸€èˆ¬")
                else:
                    self.log("âŒ å¹´åº¦è¡¨ç°ä¸ä¸€è‡´")
                    self.add_issue("ç­–ç•¥å¹´åº¦è¡¨ç°ä¸ç¨³å®š")
                
                results['yearly_consistency'] = consistency
        else:
            self.log("âš ï¸  æ•°æ®æ— æ—¶é—´ç´¢å¼•,è·³è¿‡å¹´åº¦åˆ‡ç‰‡")
        
        return results
    
    # ========== ä¸»ä½“æ£€æµç¨‹ ==========
    
    def run_full_triage(self, signal_data: pd.DataFrame) -> Dict:
        """
        è¿è¡Œå®Œæ•´çš„6æ­¥ä½“æ£€æµç¨‹
        
        Parameters:
        -----------
        signal_data : pd.DataFrame
            åŒ…å«ä¿¡å·ã€PCAç‰¹å¾å’Œæœªæ¥æ”¶ç›Šçš„å®Œæ•´æ•°æ®
        
        Returns:
        --------
        dict: å®Œæ•´ä½“æ£€æŠ¥å‘Š
        """
        self.log("=" * 70)
        self.log("ğŸ¥ 60åˆ†é’Ÿå¿«é€Ÿä½“æ£€ (Full Triage)")
        self.log("=" * 70)
        self.log(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.log(f"æ•°æ®é‡: {len(signal_data)} æ¡")
        self.log("")
        
        start_time = datetime.now()
        
        # ä½“æ£€1: ä¿¡å·å¯¹é½ä¸æ³„éœ²æ£€æŸ¥
        check1 = self.check_signal_alignment_and_leakage(signal_data)
        
        # ä½“æ£€1A: ç ´åæ€§å¯¹ç…§å®éªŒ(éªŒè¯æ³„æ¼)
        check1a = self.check_leakage_with_wrong_labels(signal_data)
        
        # ä½“æ£€2: æˆæœ¬ä¸æ¢æ‰‹æ‹†è§£
        check2 = self.analyze_cost_and_turnover(signal_data)
        
        # ä½“æ£€3: æ’åºåŠ›ä½“æ£€
        check3 = self.check_ranking_power(signal_data)
        
        # ä½“æ£€4: çŠ¶æ€è¿‡æ»¤ä½“æ£€
        check4 = self.check_state_filtering(signal_data)
        
        # ä½“æ£€5: é—¨æ§›/æŒæœ‰å‘¨æœŸç½‘æ ¼
        check5 = self.grid_search_threshold_holding(signal_data)
        
        # ä½“æ£€6: éšæœºåŸºå‡†ä¸å¹´åº¦åˆ‡ç‰‡
        check6 = self.check_random_baseline_and_yearly(signal_data)
        
        # æ•´åˆç»“æœ
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds() / 60
        
        triage_summary = {
            'check1_alignment': check1,
            'check1a_leakage_test': check1a,
            'check2_cost': check2,
            'check3_ranking': check3,
            'check4_state': check4,
            'check5_grid': check5,
            'check6_baseline': check6,
            'issues_found': self.issues_found,
            'recommendations': self.recommendations,
            'duration_minutes': duration
        }
        
        # ç”Ÿæˆæœ€ç»ˆè¯Šæ–­
        self.generate_final_diagnosis(triage_summary)
        
        # ä¿å­˜æŠ¥å‘Š
        self.save_triage_report(triage_summary)
        
        self.log(f"\nå®Œæˆæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        self.log(f"è€—æ—¶: {duration:.1f} åˆ†é’Ÿ")
        
        return triage_summary
    
    def generate_final_diagnosis(self, summary: Dict):
        """ç”Ÿæˆæœ€ç»ˆè¯Šæ–­å»ºè®®"""
        self.log("\n" + "=" * 70)
        self.log("ğŸ¯ æœ€ç»ˆè¯Šæ–­ä¸å»ºè®®")
        self.log("=" * 70)
        
        # æå–å…³é”®æŒ‡æ ‡
        ranking = summary.get('check3_ranking', {})
        cost = summary.get('check2_cost', {})
        baseline = summary.get('check6_baseline', {})
        
        ic_value = ranking.get('best_ic', 0)
        spread = ranking.get('spread', 0)
        cost_erosion = cost.get('cost_erosion_ratio', 0)
        z_score = baseline.get('z_score', 0)
        
        self.log("\nå…³é”®æŒ‡æ ‡:")
        self.log(f"   ICå€¼: {ic_value:+.4f}")
        self.log(f"   Spread: {spread:+.6f}")
        self.log(f"   æˆæœ¬ä¾µèš€: {cost_erosion:.1%}")
        self.log(f"   éšæœºåŸºå‡†Z-score: {z_score:+.2f}")
        
        # å†³ç­–æ ‘
        self.log("\nè¯Šæ–­ç»“è®º:")
        
        if abs(ic_value) >= 0.02 or (spread > 0 and abs(spread) > 0.001):
            # ä¿¡å·æœ‰åŒºåˆ†åŠ›
            self.log("âœ… ä¿¡å·æœ‰åŒºåˆ†åŠ› (ICâ‰¥0.02 æˆ– Spread>0)")
            
            if cost_erosion > 0.5:
                self.log("âš ï¸  ä½†è¢«æˆæœ¬å¤§å¹…ä¾µèš€")
                self.log("\nğŸ”§ å»ºè®®è·¯å¾„: å…ˆä¼˜åŒ–ä¿¡å·è½¬æ¢")
                self.log("   1. é™ä½æ¢æ‰‹ç‡(å»¶é•¿æŒæœ‰æœŸã€æé«˜é—¨æ§›)")
                self.log("   2. ä¼˜åŒ–å…¥åœº/å‡ºåœºé€»è¾‘")
                self.log("   3. ä½¿ç”¨ä½“æ£€5çš„æœ€ä½³å‚æ•°ç»„åˆ")
            elif z_score < 1:
                self.log("âš ï¸  ä½†æœªæ˜¾è‘—ä¼˜äºéšæœº")
                self.log("\nğŸ”§ å»ºè®®è·¯å¾„: å…ˆä¼˜åŒ–ç­–ç•¥è½¬æ¢é€»è¾‘")
                self.log("   1. æ£€æŸ¥ä¿¡å·å¯¹é½(ä½“æ£€1)")
                self.log("   2. è°ƒæ•´é—¨æ§›å’ŒæŒæœ‰æœŸ(ä½“æ£€5)")
                self.log("   3. è€ƒè™‘çŠ¶æ€è¿‡æ»¤çš„æœ‰æ•ˆæ€§(ä½“æ£€4)")
            else:
                self.log("âœ… ä¸”è½¬æ¢æ•ˆç‡è‰¯å¥½")
                self.log("\nğŸ”§ å»ºè®®è·¯å¾„: ç»§ç»­ä¼˜åŒ–å’Œå®ç›˜éªŒè¯")
                self.log("   1. åŠ å…¥æ­¢æŸ/æ­¢ç›ˆé€»è¾‘")
                self.log("   2. åšå¥½ä»“ä½ç®¡ç†")
                self.log("   3. å‡†å¤‡å®ç›˜æµ‹è¯•")
        else:
            # ä¿¡å·æ— åŒºåˆ†åŠ›
            self.log("âŒ ä¿¡å·æ— åŒºåˆ†åŠ› (ICâ‰ˆ0, Spreadâ‰ˆ0)")
            self.log("\nğŸ”§ å»ºè®®è·¯å¾„: å›åˆ°ç‰¹å¾å±‚æˆ–è¿›å…¥MLåŸºçº¿")
            self.log("   1. é‡æ–°å®¡è§†ç‰¹å¾å·¥ç¨‹(æ˜¯å¦æœ‰é¢„æµ‹æ€§)")
            self.log("   2. æ£€æŸ¥PCAé™ç»´æ˜¯å¦ä¸¢å¤±ä¿¡æ¯")
            self.log("   3. è¿›å…¥é˜¶æ®µ12: å°è¯•ç›‘ç£å­¦ä¹ æ¨¡å‹")
            self.log("   4. è€ƒè™‘æ¢ç”¨å…¶ä»–å› å­æˆ–æ•°æ®æº")
        
        # åˆ—å‡ºæ‰€æœ‰é—®é¢˜
        if len(self.issues_found) > 0:
            self.log("\nâš ï¸  å‘ç°çš„é—®é¢˜:")
            for i, issue in enumerate(self.issues_found, 1):
                self.log(f"   {i}. {issue}")
        
        # åˆ—å‡ºæ‰€æœ‰å»ºè®®
        if len(self.recommendations) > 0:
            self.log("\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            for i, rec in enumerate(self.recommendations, 1):
                self.log(f"   {i}. {rec}")
    
    def save_triage_report(self, summary: Dict):
        """ä¿å­˜ä½“æ£€æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = os.path.join(self.reports_dir, f"triage_report_{timestamp}.txt")
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("\n".join(self.triage_report))
        
        self.log(f"\nğŸ“‹ ä½“æ£€æŠ¥å‘Šå·²ä¿å­˜: {os.path.basename(report_file)}")


def main():
    """ä¸»å‡½æ•°: è¿è¡Œå¿«é€Ÿä½“æ£€"""
    try:
        # åˆå§‹åŒ–ä½“æ£€ç³»ç»Ÿ
        triage = QuickTriage()
        
        # åŠ è½½æ•°æ®(éœ€è¦å…ˆè¿è¡Œstrategy_backtestç”Ÿæˆæ•°æ®)
        print("\nğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®...")
        
        # è¿™é‡Œéœ€è¦å…ˆè¿è¡Œstrategy_backtestç”Ÿæˆsignal_data
        # ç„¶åä¿å­˜ä¸ºCSVä¾›ä½“æ£€ä½¿ç”¨
        # æˆ–è€…ç›´æ¥ä»strategy_backtest.pyå¯¼å…¥æ•°æ®
        
        from strategy_backtest import StrategyBacktest
        
        backtest = StrategyBacktest()
        
        # 1. åŠ è½½èšç±»ç»“æœ
        cluster_results = backtest.load_cluster_evaluation_results()
        
        # 2. é€‰æ‹©èšç±»
        selection_results = backtest.select_best_clusters(cluster_results['comparison_df'], top_n=3)
        
        # 3. å‡†å¤‡æµ‹è¯•æ•°æ®
        test_data = backtest.prepare_test_data(symbol="000001")
        
        # 4. ç”Ÿæˆä¿¡å·
        signal_data = backtest.generate_trading_signals(test_data, selection_results['selected_clusters'])
        
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(signal_data)} æ¡è®°å½•")
        print(f"   ç‰¹å¾: {[col for col in signal_data.columns if col.startswith('PC')][:5]}")
        print(f"   ä¿¡å·: signal_combined")
        print(f"   ç›®æ ‡: future_return_5d")
        
        # è¿è¡Œå®Œæ•´ä½“æ£€
        print("\n" + "=" * 70)
        print("å¼€å§‹60åˆ†é’Ÿå¿«é€Ÿä½“æ£€...")
        print("=" * 70)
        
        triage_results = triage.run_full_triage(signal_data)
        
        print("\n" + "=" * 70)
        print("ğŸ‰ å¿«é€Ÿä½“æ£€å®Œæˆ!")
        print("=" * 70)
        
        return triage_results
        
    except Exception as e:
        print(f"\nğŸ’¥ ä½“æ£€å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    main()
