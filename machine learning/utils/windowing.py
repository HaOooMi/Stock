"""
è‚¡ç¥¨æ»‘çª—æ•°æ®ç”Ÿæˆå™¨ - é‡æ„ç‰ˆ
ç”¨äºæ—¶åºé¢„æµ‹çš„æ ·æœ¬ç”Ÿæˆï¼Œä½¿ç”¨ç‹¬ç«‹çš„ç‰¹å¾å·¥ç¨‹æ¨¡å—

æ ¸å¿ƒæ¦‚å¿µï¼š
1. çª—å£é•¿åº¦(window_size)ï¼šç”¨å¤šå°‘å†å²æ•°æ®ä½œä¸ºè¾“å…¥ç‰¹å¾
2. é¢„æµ‹ç›®æ ‡(target_type)ï¼šé¢„æµ‹ä»€ä¹ˆï¼ˆä»·æ ¼ã€æ”¶ç›Šç‡ã€æ¶¨è·Œç­‰ï¼‰
3. é¢„æµ‹æ­¥é•¿(prediction_steps)ï¼šé¢„æµ‹æœªæ¥ç¬¬å‡ å¤©
4. æ»‘åŠ¨æ­¥é•¿(stride)ï¼šçª—å£æ¯æ¬¡ç§»åŠ¨å¤šå°‘æ­¥
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from typing import Tuple, List, Optional, Dict, Any
import warnings
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
ml_root = os.path.dirname(current_dir)
if ml_root not in sys.path:
    sys.path.insert(0, ml_root)

# å¯¼å…¥ç‰¹å¾å·¥ç¨‹æ¨¡å—
from features.feature_engineering import FeatureEngineer

# æ·»åŠ ä¸Šå±‚ç›®å½•ä»¥å¯¼å…¥stock_infoæ¨¡å—
project_root = os.path.dirname(os.path.dirname(ml_root))  # stock/
get_stock_info_path = os.path.join(project_root, "get_stock_info")
if get_stock_info_path not in sys.path:
    sys.path.insert(0, get_stock_info_path)

try:
    import utils
    from stock_market_data_akshare import get_history_data
    INFLUXDB_AVAILABLE = True
except ImportError:
    INFLUXDB_AVAILABLE = False
    print("âš ï¸ InfluxDBç›¸å…³æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œå°†å›é€€åˆ°CSVæ•°æ®")

warnings.filterwarnings('ignore')


def load_real_stock_data(symbol: str = "000001", start_date: str = "2022-01-01", end_date: str = "2024-12-31") -> pd.DataFrame:
    """
    ä»InfluxDBåŠ è½½çœŸå®è‚¡ç¥¨æ•°æ®
    
    Parameters:
    -----------
    symbol : str, default="000001"
        è‚¡ç¥¨ä»£ç 
    start_date : str, default="2022-01-01"
        å¼€å§‹æ—¥æœŸ
    end_date : str, default="2024-12-31"
        ç»“æŸæ—¥æœŸ
        
    Returns:
    --------
    pd.DataFrame
        åŒ…å«OHLCVæ•°æ®çš„DataFrameï¼Œå¦‚æœåŠ è½½å¤±è´¥åˆ™è¿”å›None
    """
    if not INFLUXDB_AVAILABLE:
        print("ERROR: InfluxDB modules not available, cannot load real data")
        return None
    
    try:
        print(f"Loading {symbol} data from InfluxDB...")
        
        # è·å–InfluxDBå®¢æˆ·ç«¯
        client = utils.get_influxdb_client()
        if client is None:
            print("ERROR: Cannot connect to InfluxDB")
            return None
        
        query_api = client.query_api()
        
        # è½¬æ¢æ—¥æœŸæ ¼å¼
        start_str_rfc = f"{start_date}T00:00:00Z"
        end_str_rfc = f"{end_date}T23:59:59Z"
        
        # è·å–å†å²æ•°æ®
        df = get_history_data(query_api, symbol, start_str_rfc, end_str_rfc)
        
        if df.empty:
            print(f"ERROR: No data found for {symbol} in InfluxDB")
            client.close()
            return None
        
        # æ ‡å‡†åŒ–åˆ—åï¼ˆå®Œæ•´æ˜ å°„InfluxDBå­—æ®µï¼‰
        column_mapping = {
            'æ—¥æœŸ': 'datetime',
            'å¼€ç›˜': 'open',
            'æœ€é«˜': 'high', 
            'æœ€ä½': 'low',
            'æ”¶ç›˜': 'close',
            'æˆäº¤é‡': 'volume',
            'æˆäº¤é¢': 'amount',      # æˆäº¤é¢æ˜¯amount
            'æŒ¯å¹…': 'amplitude',
            'æ¶¨è·Œå¹…': 'pct_change',
            'æ¶¨è·Œé¢': 'change',
            'æ¢æ‰‹ç‡': 'turnover',    # æ¢æ‰‹ç‡æ‰æ˜¯turnover
            'æ˜¯å¦åœç‰Œ': 'is_suspended'
        }
        
        df = df.rename(columns=column_mapping)
        
        # ç¡®ä¿datetimeåˆ—æ˜¯æ­£ç¡®çš„æ—¶é—´æ ¼å¼
        df['datetime'] = pd.to_datetime(df['datetime'])
        df = df.sort_values('datetime').reset_index(drop=True)
        
        # æ·»åŠ ç¼ºå¤±çš„åˆ—ï¼ˆå¦‚æœåŸå§‹æ•°æ®æ²¡æœ‰ï¼‰
        if 'amount' not in df.columns:
            df['amount'] = 0.0
        if 'turnover' not in df.columns:
            df['turnover'] = 0.0
        
        print(f"SUCCESS: Loaded {len(df)} records for {symbol} from InfluxDB")
        print(f"Date range: {df['datetime'].min().date()} to {df['datetime'].max().date()}")
        
        client.close()
        return df[['datetime', 'open', 'high', 'low', 'close', 'volume', 'amount', 'turnover']]
        
    except Exception as e:
        print(f"ERROR: Failed to load data from InfluxDB: {str(e)}")
        return None


class SlidingWindowGenerator:
    """
    æ»‘çª—æ•°æ®ç”Ÿæˆå™¨ - é‡æ„ç‰ˆ
    ä½¿ç”¨ç‹¬ç«‹çš„ç‰¹å¾å·¥ç¨‹æ¨¡å—å¤„ç†ç‰¹å¾ç”Ÿæˆ
    """
    
    def __init__(self, 
                 window_size: int = 30,
                 prediction_steps: int = 1,
                 stride: int = 1,
                 target_type: str = 'return',
                 scaler_type: str = 'standard',
                 feature_type: str = 'manual',  # 'manual', 'auto', 'combined'
                 max_auto_features: int = 50):
        """
        åˆå§‹åŒ–æ»‘çª—ç”Ÿæˆå™¨
        
        Parameters:
        -----------
        window_size : int, default=30
            æ»‘åŠ¨çª—å£å¤§å°ï¼ˆå†å²æ•°æ®é•¿åº¦ï¼‰
        prediction_steps : int, default=1
            é¢„æµ‹æ­¥é•¿ï¼ˆé¢„æµ‹æœªæ¥ç¬¬å‡ æ­¥ï¼‰
        stride : int, default=1
            æ»‘åŠ¨æ­¥é•¿ï¼ˆçª—å£æ¯æ¬¡ç§»åŠ¨çš„æ­¥æ•°ï¼‰
        target_type : str, default='return'
            ç›®æ ‡ç±»å‹ï¼š'price', 'return', 'return_multi', 'direction', 'high_low'
        scaler_type : str, default='standard'
            ç‰¹å¾ç¼©æ”¾ç±»å‹ï¼š'standard', 'minmax', None
        feature_type : str, default='manual'
            ç‰¹å¾ç±»å‹ï¼š'manual'(æ‰‹å·¥), 'auto'(è‡ªåŠ¨), 'combined'(ç»„åˆ)
        max_auto_features : int, default=50
            æœ€å¤§è‡ªåŠ¨ç‰¹å¾æ•°é‡
        """
        self.window_size = window_size
        self.prediction_steps = prediction_steps
        self.stride = stride
        self.target_type = target_type
        self.scaler_type = scaler_type
        self.feature_type = feature_type
        self.max_auto_features = max_auto_features
        self.scaler = None
        
        # åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹å™¨
        self.feature_engineer = FeatureEngineer()
        
        print(f"ğŸ”§ æ»‘çª—ç”Ÿæˆå™¨é…ç½®:")
        print(f"   ğŸ“Š çª—å£å¤§å°: {self.window_size}")
        print(f"   ğŸ¯ é¢„æµ‹æ­¥é•¿: {self.prediction_steps} æ­¥å")
        print(f"   âš¡ æ»‘åŠ¨æ­¥é•¿: {self.stride}")
        print(f"   ğŸ“ˆ ç›®æ ‡ç±»å‹: {self.target_type}")
        print(f"   ğŸ“ ç¼©æ”¾æ–¹å¼: {self.scaler_type}")
        print(f"   ğŸ”§ ç‰¹å¾ç±»å‹: {self.feature_type}")
        if self.feature_type in ['auto', 'combined']:
            print(f"   ğŸ¤– æœ€å¤§è‡ªåŠ¨ç‰¹å¾: {self.max_auto_features}")

    def prepare_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ä½¿ç”¨ç‰¹å¾å·¥ç¨‹æ¨¡å—ç”Ÿæˆç‰¹å¾
        
        Parameters:
        -----------
        df : pd.DataFrame
            åŸå§‹OHLCVæ•°æ®
            
        Returns:
        --------
        pd.DataFrame
            åŒ…å«ç‰¹å¾çš„DataFrame
        """
        if self.feature_type == 'manual':
            return self.feature_engineer.prepare_manual_features(df)
        elif self.feature_type == 'auto':
            return self.feature_engineer.prepare_auto_features(
                df, window_size=self.window_size, max_features=self.max_auto_features
            )
        elif self.feature_type == 'combined':
            return self.feature_engineer.prepare_combined_features(
                df, window_size=self.window_size, 
                auto_features=True, max_auto_features=self.max_auto_features
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç‰¹å¾ç±»å‹: {self.feature_type}")

    def create_target(self, df: pd.DataFrame) -> np.ndarray:
        """
        æ ¹æ®target_typeåˆ›å»ºé¢„æµ‹ç›®æ ‡
        
        æ³¨æ„ï¼šå·²ä¿®å¤np.rollå¯¼è‡´çš„æœ«å°¾å¾ªç¯é—®é¢˜
        è¶…å‡ºèŒƒå›´çš„ä½ç½®å¡«å……np.nanï¼Œé¿å…ä½¿ç”¨é”™è¯¯çš„å¾ªç¯æ•°æ®
        """
        close_prices = df['close'].values
        
        if self.target_type == 'price':
            # é¢„æµ‹æœªæ¥ä»·æ ¼
            target = np.full_like(close_prices, np.nan, dtype=float)
            if self.prediction_steps < len(close_prices):
                target[:-self.prediction_steps] = close_prices[self.prediction_steps:]
            
        elif self.target_type == 'return':
            # é¢„æµ‹æœªæ¥æ”¶ç›Šç‡
            target = np.full_like(close_prices, np.nan, dtype=float)
            if self.prediction_steps < len(close_prices):
                future_prices = close_prices[self.prediction_steps:]
                current_prices = close_prices[:-self.prediction_steps]
                target[:-self.prediction_steps] = (future_prices - current_prices) / current_prices
            
        elif self.target_type == 'return_multi':
            # é¢„æµ‹æœªæ¥Nå¤©ç´¯è®¡æ”¶ç›Šç‡
            target = []
            for i in range(len(close_prices)):
                if i + self.prediction_steps < len(close_prices):
                    future_returns = []
                    for j in range(1, self.prediction_steps + 1):
                        if i + j < len(close_prices):
                            daily_return = (close_prices[i + j] - close_prices[i + j - 1]) / close_prices[i + j - 1]
                            future_returns.append(daily_return)
                    
                    if future_returns:
                        # ç´¯è®¡æ”¶ç›Šç‡
                        cumulative_return = np.prod(1 + np.array(future_returns)) - 1
                        target.append(cumulative_return)
                    else:
                        target.append(np.nan)
                else:
                    target.append(np.nan)
            target = np.array(target)
            
        elif self.target_type == 'direction':
            # é¢„æµ‹æ¶¨è·Œæ–¹å‘ï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰
            target = np.full(len(close_prices), np.nan, dtype=float)
            if self.prediction_steps < len(close_prices):
                future_prices = close_prices[self.prediction_steps:]
                current_prices = close_prices[:-self.prediction_steps]
                target[:-self.prediction_steps] = (future_prices > current_prices).astype(int)
            
        elif self.target_type == 'high_low':
            # é¢„æµ‹æœªæ¥Nå¤©çš„æœ€é«˜ä»·å’Œæœ€ä½ä»·
            high_prices = df['high'].values
            low_prices = df['low'].values
            target = []
            
            for i in range(len(close_prices)):
                if i + self.prediction_steps < len(close_prices):
                    future_high = np.max(high_prices[i+1:i+1+self.prediction_steps])
                    future_low = np.min(low_prices[i+1:i+1+self.prediction_steps])
                    # è½¬æ¢ä¸ºç›¸å¯¹å½“å‰ä»·æ ¼çš„æ¯”ä¾‹
                    high_ratio = future_high / close_prices[i] - 1
                    low_ratio = future_low / close_prices[i] - 1
                    target.append([high_ratio, low_ratio])
                else:
                    target.append([np.nan, np.nan])
            target = np.array(target)
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„target_type: {self.target_type}")
        
        return target

    def generate_samples(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, pd.DataFrame]:
        """
        ç”Ÿæˆæ»‘çª—æ ·æœ¬
        
        Returns:
        --------
        X : np.ndarray, shape (n_samples, window_size, n_features)
            è¾“å…¥ç‰¹å¾æ•°ç»„
        y : np.ndarray, shape (n_samples, target_dim)
            ç›®æ ‡æ•°ç»„
        metadata : pd.DataFrame
            æ ·æœ¬å…ƒæ•°æ®ï¼ˆæ—¶é—´æˆ³ç­‰ï¼‰
        """
        print(f"ğŸ”„ å¼€å§‹ç”Ÿæˆæ»‘çª—æ ·æœ¬...")
        
        # ä½¿ç”¨ç‰¹å¾å·¥ç¨‹æ¨¡å—ç”Ÿæˆç‰¹å¾
        data = self.prepare_features(df)
        print(f"ğŸ“Š ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œç‰¹å¾æ•°é‡: {len(data.columns) - 2}")  # å‡å»datetimeå’Œclose
        
        # åˆ›å»ºç›®æ ‡å€¼
        target = self.create_target(data)
        
        # å‡†å¤‡ç‰¹å¾æ•°ç»„
        feature_columns = [col for col in data.columns if col not in ['datetime', 'close']]
        features = data[feature_columns].values
        
        # ç‰¹å¾ç¼©æ”¾
        if self.scaler_type == 'standard':
            self.scaler = StandardScaler()
            features = self.scaler.fit_transform(features)
        elif self.scaler_type == 'minmax':
            self.scaler = MinMaxScaler()
            features = self.scaler.fit_transform(features)
        
        # åˆå§‹åŒ–æ ·æœ¬å­˜å‚¨
        X, y, metadata = [], [], []
        
        # è®¡ç®—æœ€å¤§èµ·å§‹ç´¢å¼•
        max_start_idx = len(features) - self.window_size - self.prediction_steps
        
        # æ»‘çª—ç”Ÿæˆæ ·æœ¬
        for i in range(0, max_start_idx, self.stride):
            # æå–çª—å£ç‰¹å¾
            x_window = features[i:i+self.window_size]
            
            # ç›®æ ‡ç´¢å¼•
            target_idx = i + self.window_size - 1  # çª—å£æœ€åä¸€å¤©çš„ç´¢å¼•
            
            # æ£€æŸ¥ç›®æ ‡å€¼æœ‰æ•ˆæ€§
            if target_idx < len(target) and not np.any(np.isnan(target[target_idx])):
                X.append(x_window)
                y.append(target[target_idx])
                
                # ä¿å­˜å…ƒæ•°æ®
                window_start = data.iloc[i]['datetime']
                window_end = data.iloc[i+self.window_size-1]['datetime']
                prediction_date = data.iloc[min(target_idx + self.prediction_steps, len(data)-1)]['datetime']
                
                metadata.append({
                    'window_start': window_start,
                    'window_end': window_end,
                    'prediction_date': prediction_date,
                    'current_price': data.iloc[target_idx]['close']
                })
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        X = np.array(X)
        y = np.array(y)
        metadata = pd.DataFrame(metadata)
        
        print(f"âœ… æ ·æœ¬ç”Ÿæˆå®Œæˆ:")
        print(f"   ğŸ“¦ æ ·æœ¬æ•°é‡: {len(X)}")
        print(f"   ğŸ“ è¾“å…¥å½¢çŠ¶: {X.shape}")
        print(f"   ğŸ¯ è¾“å‡ºå½¢çŠ¶: {y.shape}")
        
        return X, y, metadata

    def analyze_samples(self, X: np.ndarray, y: np.ndarray, metadata: pd.DataFrame):
        """
        åˆ†æç”Ÿæˆçš„æ ·æœ¬
        """
        print(f"\nğŸ“Š æ ·æœ¬åˆ†ææŠ¥å‘Š")
        print("=" * 50)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"æ•°æ®æ¦‚è§ˆ:")
        print(f"  - æ€»æ ·æœ¬æ•°: {len(X)}")
        print(f"  - è¾“å…¥ç»´åº¦: {X.shape}")
        print(f"  - è¾“å‡ºç»´åº¦: {y.shape}")
        print(f"  - æ—¶é—´è·¨åº¦: {metadata['window_start'].min()} åˆ° {metadata['prediction_date'].max()}")
        
        # ç›®æ ‡å€¼åˆ†æ
        print(f"\nç›®æ ‡å€¼åˆ†æ:")
        if self.target_type == 'direction':
            # åˆ†ç±»ä»»åŠ¡åˆ†æ
            unique, counts = np.unique(y, return_counts=True)
            for val, count in zip(unique, counts):
                label = "ä¸Šæ¶¨" if val == 1 else "ä¸‹è·Œ"
                print(f"  - {label}: {count} ä¸ªæ ·æœ¬ ({count/len(y)*100:.1f}%)")
        else:
            # å›å½’ä»»åŠ¡åˆ†æ
            if len(y.shape) > 1 and y.shape[1] > 1:
                # å¤šç»´ç›®æ ‡
                for i in range(y.shape[1]):
                    print(f"  - ç»´åº¦{i} - å‡å€¼: {np.mean(y[:, i]):.4f}, æ ‡å‡†å·®: {np.std(y[:, i]):.4f}")
            else:
                # å•ç»´ç›®æ ‡
                print(f"  - å‡å€¼: {np.mean(y):.4f}")
                print(f"  - æ ‡å‡†å·®: {np.std(y):.4f}")
                print(f"  - æœ€å°å€¼: {np.min(y):.4f}")
                print(f"  - æœ€å¤§å€¼: {np.max(y):.4f}")
        
        # ç¼ºå¤±å€¼æ£€æŸ¥
        if np.any(np.isnan(X)) or np.any(np.isnan(y)):
            print(f"\nâš ï¸  å‘ç°ç¼ºå¤±å€¼:")
            print(f"  - Xä¸­ç¼ºå¤±å€¼: {np.sum(np.isnan(X))}")
            print(f"  - yä¸­ç¼ºå¤±å€¼: {np.sum(np.isnan(y))}")
        else:
            print(f"\nâœ… æ— ç¼ºå¤±å€¼")

    def visualize_samples(self, X: np.ndarray, y: np.ndarray, metadata: pd.DataFrame, n_samples: int = 3):
        """
        å¯è§†åŒ–æ ·æœ¬åˆ†æ
        """
        plt.rcParams['font.sans-serif'] = ['SimHei']
        plt.rcParams['axes.unicode_minus'] = False
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # å­å›¾1: æ ·æœ¬æ—¶é—´åˆ†å¸ƒ
        axes[0,0].hist(metadata['window_end'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0,0].set_title('æ ·æœ¬æ—¶é—´åˆ†å¸ƒ')
        axes[0,0].tick_params(axis='x', rotation=45)
        
        # å­å›¾2: ç›®æ ‡å€¼åˆ†å¸ƒ
        if self.target_type == 'direction':
            unique, counts = np.unique(y, return_counts=True)
            labels = ['ä¸‹è·Œ' if x == 0 else 'ä¸Šæ¶¨' for x in unique]
            axes[0,1].bar(labels, counts, color=['red', 'green'], alpha=0.7)
            axes[0,1].set_title('æ¶¨è·Œåˆ†å¸ƒ')
        else:
            axes[0,1].hist(y.flatten() if len(y.shape) > 1 else y, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')
            axes[0,1].set_title('ç›®æ ‡å€¼åˆ†å¸ƒ')
        
        # å­å›¾3: ç‰¹å¾è¶‹åŠ¿ï¼ˆå±•ç¤ºç¬¬0ä¸ªç‰¹å¾çš„å¹³å‡æ›²çº¿ï¼‰
        if len(X) > 0:
            mean_feature_0 = np.mean(X[:, :, 0], axis=0)
            axes[1,0].plot(range(self.window_size), mean_feature_0, marker='o', linewidth=2)
            axes[1,0].set_title('ç‰¹å¾0çš„å¹³å‡è¶‹åŠ¿')
            axes[1,0].set_xlabel('æ—¶é—´æ­¥')
            axes[1,0].grid(True, alpha=0.3)
        
        # å­å›¾4: éšæœºæ ·æœ¬å±•ç¤º
        if len(X) >= n_samples:
            sample_indices = np.random.choice(len(X), n_samples, replace=False)
            for idx in sample_indices:
                axes[1,1].plot(range(self.window_size), X[idx, :, 0], alpha=0.7, 
                             label=f'æ ·æœ¬{idx} (ç›®æ ‡: {y[idx]:.3f})')
            axes[1,1].set_title(f'éšæœº{n_samples}ä¸ªæ ·æœ¬çš„ç‰¹å¾0')
            axes[1,1].set_xlabel('æ—¶é—´æ­¥')
            axes[1,1].legend()
            axes[1,1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()


def demo_new_sliding_window():
    """
    æ¼”ç¤ºæ–°çš„æ»‘çª—ç”Ÿæˆå™¨åŠŸèƒ½
    """
    print("ğŸš€ æ–°ç‰ˆæ»‘çª—ç”Ÿæˆå™¨æ¼”ç¤º")
    print("=" * 60)
    
    # ä¼˜å…ˆå°è¯•ä»InfluxDBåŠ è½½çœŸå®æ•°æ®
    print("Loading data...")
    df = load_real_stock_data("000001", "2022-01-01", "2024-12-31")
    
    data_source = "Real stock data (InfluxDB)"
    
    # å¦‚æœçœŸå®æ•°æ®å¤ªå¤šï¼Œå–æœ€è¿‘çš„æ•°æ®ä»¥æé«˜æ¼”ç¤ºé€Ÿåº¦
    if len(df) > 500:
        df = df.tail(500).reset_index(drop=True)
        print(f"Using latest 500 records for demo performance")
    
    print(f"SUCCESS: Data loaded ({data_source}): {len(df)} records")
    print(f"Date range: {df['datetime'].min().date()} to {df['datetime'].max().date()}")
    
    # æµ‹è¯•é…ç½®
    test_configs = [
        {
            'name': 'æ‰‹å·¥ç‰¹å¾ - çŸ­æœŸé¢„æµ‹',
            'window_size': 30,
            'prediction_steps': 1,
            'target_type': 'return',
            'feature_type': 'manual',
            'stride': 1
        },
        {
            'name': 'æ‰‹å·¥ç‰¹å¾ - ä¸­æœŸé¢„æµ‹', 
            'window_size': 60,
            'prediction_steps': 5,
            'target_type': 'return',
            'feature_type': 'manual',
            'stride': 5
        },
        {
            'name': 'ç»„åˆç‰¹å¾ - åˆ†ç±»é¢„æµ‹',
            'window_size': 20,
            'prediction_steps': 3,
            'target_type': 'direction',
            'feature_type': 'manual',  # æš‚æ—¶åªç”¨æ‰‹å·¥ç‰¹å¾ï¼Œé¿å…tsfreshé—®é¢˜
            'stride': 1,
            'max_auto_features': 30
        }
    ]
    
    results = {}
    
    for i, config in enumerate(test_configs):
        print(f"\n{'='*60}")
        print(f"ğŸ”¬ é…ç½® {i+1}: {config['name']}")
        print(f"{'='*60}")
        
        # åˆ›å»ºç”Ÿæˆå™¨
        generator = SlidingWindowGenerator(
            window_size=config['window_size'],
            prediction_steps=config['prediction_steps'],
            target_type=config['target_type'],
            feature_type=config['feature_type'],
            stride=config['stride'],
            max_auto_features=config.get('max_auto_features', 50)
        )
        
        # ç”Ÿæˆæ ·æœ¬
        X, y, metadata = generator.generate_samples(df)
        
        # åˆ†ææ ·æœ¬
        generator.analyze_samples(X, y, metadata)
        
        # ä¿å­˜ç»“æœ
        results[config['name']] = {
            'generator': generator,
            'X': X,
            'y': y,
            'metadata': metadata
        }
        
        # ç¬¬ä¸€ä¸ªé…ç½®å±•ç¤ºå¯è§†åŒ–
        if i == 0:
            generator.visualize_samples(X, y, metadata)
    
    return results


def practical_examples():
    """
    å±•ç¤ºæœ€ä½³å®è·µå»ºè®®
    """
    print("\n" + "=" * 60)
    print("ğŸ’¡ æ»‘çª—è®¾è®¡æœ€ä½³å®è·µ")
    print("=" * 60)
    
    practices = {
        "çŸ­çº¿äº¤æ˜“ (1-3å¤©)": {
            "çª—å£å¤§å°": "5-20å¤©",
            "é¢„æµ‹æ­¥é•¿": "1-3å¤©",
            "ç‰¹å¾ç±»å‹": "æ‰‹å·¥ç‰¹å¾ + çŸ­æœŸæŠ€æœ¯æŒ‡æ ‡",
            "é€‚ç”¨åœºæ™¯": "æ—¥å†…äº¤æ˜“ã€çŸ­æœŸæ³¢åŠ¨æ•æ‰"
        },
        "ä¸­çº¿äº¤æ˜“ (5-20å¤©)": {
            "çª—å£å¤§å°": "30-60å¤©", 
            "é¢„æµ‹æ­¥é•¿": "5-10å¤©",
            "ç‰¹å¾ç±»å‹": "ç»„åˆç‰¹å¾",
            "é€‚ç”¨åœºæ™¯": "è¶‹åŠ¿è·Ÿè¸ªã€æ³¢æ®µæ“ä½œ"
        },
        "é•¿çº¿äº¤æ˜“ (30å¤©+)": {
            "çª—å£å¤§å°": "60-120å¤©",
            "é¢„æµ‹æ­¥é•¿": "20-30å¤©", 
            "ç‰¹å¾ç±»å‹": "è‡ªåŠ¨ç‰¹å¾ + å®è§‚æŒ‡æ ‡",
            "é€‚ç”¨åœºæ™¯": "ä»·å€¼æŠ•èµ„ã€é•¿æœŸè¶‹åŠ¿"
        }
    }
    
    for strategy, params in practices.items():
        print(f"\nğŸ“ˆ {strategy}:")
        for key, value in params.items():
            print(f"   {key}: {value}")
    
    print(f"\nâš ï¸  å¸¸è§è¸©å‘ç‚¹:")
    print(f"   1. çª—å£è¿‡å°: ä¿¡æ¯ä¸è¶³ï¼Œæ¨¡å¼å­¦ä¹ å›°éš¾")
    print(f"   2. çª—å£è¿‡å¤§: åŒ…å«è¿‡æ—¶ä¿¡æ¯ï¼Œè®¡ç®—é‡å¢åŠ ")
    print(f"   3. strideè¿‡å¤§: æ ·æœ¬æ•°é‡ä¸è¶³ï¼Œæ¨¡å‹éš¾ä»¥è®­ç»ƒ") 
    print(f"   4. æœªæ¥æ•°æ®æ³„éœ²: ç‰¹å¾ä¸­åŒ…å«ç›®æ ‡å€¼ä¿¡æ¯")
    print(f"   5. æ•°æ®ä¸å¹³è¡¡: åˆ†ç±»ä»»åŠ¡ä¸­æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹å¤±è¡¡")
    print(f"   6. è‡ªåŠ¨ç‰¹å¾çˆ†ç‚¸: tsfreshç”Ÿæˆè¿‡å¤šç‰¹å¾ï¼Œéœ€è¦ç­›é€‰")
    
    print(f"\nâœ… é€‰æ‹©å»ºè®®:")
    print(f"   â€¢ æ ¹æ®æŒä»“å‘¨æœŸé€‰æ‹©prediction_steps")
    print(f"   â€¢ è®­ç»ƒæ—¶stride=1ï¼Œé¢„æµ‹æ—¶å¯é€‚å½“å¢åŠ ")
    print(f"   â€¢ æ‰‹å·¥ç‰¹å¾ä¼˜å…ˆï¼Œè‡ªåŠ¨ç‰¹å¾ä½œä¸ºè¡¥å……")
    print(f"   â€¢ ç»„åˆç‰¹å¾æ—¶æ§åˆ¶æ€»ç‰¹å¾æ•°é‡(<100)")


def test_boundary_fix():
    """
    æµ‹è¯•è¾¹ç•Œä¿®å¤æ•ˆæœ - ä½¿ç”¨InfluxDBçœŸå®è‚¡ç¥¨æ•°æ®
    """
    print("ğŸ§ª æµ‹è¯•create_targetè¾¹ç•Œä¿®å¤æ•ˆæœ")
    print("=" * 50)
    
    # ä½¿ç”¨InfluxDBçœŸå®è‚¡ç¥¨æ•°æ®è¿›è¡Œæµ‹è¯•
    print("ğŸ“Š åŠ è½½çœŸå®è‚¡ç¥¨æ•°æ®è¿›è¡Œè¾¹ç•Œæµ‹è¯•...")
    test_data = load_real_stock_data("000001", "2024-01-01", "2024-01-31")
    
    # å¦‚æœInfluxDBä¸å¯ç”¨ï¼Œä½¿ç”¨CSVæ•°æ®ä½œä¸ºå¤‡ç”¨
    if test_data is None or len(test_data) < 10:
        print("âš ï¸ InfluxDBæ•°æ®ä¸å¯ç”¨")
    else:
        # å¦‚æœæ•°æ®å¤ªå¤šï¼Œåªå–å‰15å¤©è¿›è¡Œå¿«é€Ÿæµ‹è¯•
        if len(test_data) > 15:
            test_data = test_data.head(15).copy()
        print(f"âœ… ä½¿ç”¨InfluxDBçœŸå®è‚¡ç¥¨æ•°æ®è¿›è¡Œæµ‹è¯•: {len(test_data)} æ¡è®°å½•")
    
    print(f"ï¿½ æ•°æ®æ—¶é—´èŒƒå›´: {test_data['datetime'].min().date()} åˆ° {test_data['datetime'].max().date()}")
    print("æ”¶ç›˜ä»·å‰5ä¸ª:", test_data['close'].head().tolist())
    
    # æµ‹è¯•ä¸åŒtarget_typeçš„è¾¹ç•Œå¤„ç†
    test_configs = [
        ('price', 3),
        ('return', 2), 
        ('direction', 4)
    ]
    
    for target_type, prediction_steps in test_configs:
        print(f"\nğŸ” æµ‹è¯• {target_type}, prediction_steps={prediction_steps}")
        
        generator = SlidingWindowGenerator(
            target_type=target_type,
            prediction_steps=prediction_steps,
            feature_type='manual'
        )
        
        target = generator.create_target(test_data)
        
        print(f"ç›®æ ‡æ•°ç»„é•¿åº¦: {len(target)}")
        print(f"NaNæ•°é‡: {np.sum(np.isnan(target))}")
        print(f"æœ‰æ•ˆå€¼æ•°é‡: {np.sum(~np.isnan(target))}")
        print(f"ç›®æ ‡å€¼: {target}")
        
        # éªŒè¯æœ«å°¾æ˜¯å¦æ­£ç¡®å¡«å……äº†NaN
        expected_nan_count = prediction_steps
        actual_nan_count = np.sum(np.isnan(target))
        
        if actual_nan_count >= expected_nan_count:
            print("âœ… è¾¹ç•Œå¤„ç†æ­£ç¡®")
        else:
            print("âŒ è¾¹ç•Œå¤„ç†å¯èƒ½æœ‰é—®é¢˜")
    
    print(f"\nâœ… è¾¹ç•Œä¿®å¤æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    # å…ˆæµ‹è¯•è¾¹ç•Œä¿®å¤
    test_boundary_fix()
    
    print("\n" + "="*60)
    
    # è¿è¡Œæ–°ç‰ˆæ¼”ç¤º
    results = demo_new_sliding_window()
    
    # å±•ç¤ºæœ€ä½³å®è·µ
    practical_examples()