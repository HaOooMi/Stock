#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åŠ è½½å™¨ - ç»Ÿä¸€æ•°æ®æ¥å£

åŠŸèƒ½ï¼š
1. ä»ML outputåŠ è½½æ ‡å‡†åŒ–ç‰¹å¾æ•°æ®
2. åŠ è½½ç›®æ ‡å˜é‡æ•°æ®
3. ç»Ÿä¸€è¿”å›MultiIndex [date, ticker]æ ¼å¼
4. æ•°æ®å¯¹é½ä¸æ¸…æ´—
"""

import os
import sys
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
ml_root = os.path.dirname(current_dir)
project_root = os.path.dirname(ml_root)
if project_root not in sys.path:
    sys.path.insert(0, project_root)


class DataLoader:
    """
    æ•°æ®åŠ è½½å™¨ç±»
    
    åŠŸèƒ½ï¼š
    1. åŠ è½½ç‰¹å¾å’Œç›®æ ‡æ•°æ®
    2. æ•°æ®å¯¹é½ä¸æ¸…æ´—
    3. ç»Ÿä¸€æ ¼å¼ä¸ºMultiIndex
    """
    
    def __init__(self, data_root: str = "machine learning/ML output"):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        Parameters:
        -----------
        data_root : str
            æ•°æ®æ ¹ç›®å½•
        """
        if os.path.isabs(data_root):
            self.data_root = data_root
        else:
            self.data_root = os.path.join(project_root, data_root)
        
        print(f"ğŸ“ æ•°æ®åŠ è½½å™¨åˆå§‹åŒ–")
        print(f"   æ•°æ®æ ¹ç›®å½•: {self.data_root}")
    
    def load_features_and_targets(self, 
                                  symbol: str,
                                  target_col: str = 'future_return_5d',
                                  use_scaled: bool = True) -> Tuple[pd.DataFrame, pd.Series]:
        """
        åŠ è½½ç‰¹å¾å’Œç›®æ ‡æ•°æ®ï¼ˆä»ML outputç›®å½•ï¼‰
        
        Parameters:
        -----------
        symbol : str
            è‚¡ç¥¨ä»£ç 
        target_col : str
            ç›®æ ‡åˆ—å
        use_scaled : bool
            æ˜¯å¦ä½¿ç”¨æ ‡å‡†åŒ–åçš„ç‰¹å¾
            
        Returns:
        --------
        Tuple[pd.DataFrame, pd.Series]
            (ç‰¹å¾æ•°æ®, ç›®æ ‡æ•°æ®)ï¼Œç´¢å¼•ä¸ºMultiIndex [date, ticker]
        """
        print(f"ğŸ“Š åŠ è½½æ•°æ®: {symbol}")
        
        # 1. åŠ è½½ç‰¹å¾æ•°æ®
        if use_scaled:
            # ä»scaled_features.csvåŠ è½½
            feature_pattern = f"scaler_{symbol}_scaled_features.csv"
            feature_files = [f for f in os.listdir(self.data_root) if f.startswith(f"scaler_{symbol}")]
            
            if not feature_files:
                raise FileNotFoundError(f"æœªæ‰¾åˆ°æ ‡å‡†åŒ–ç‰¹å¾æ–‡ä»¶: {feature_pattern}")
            
            feature_file = os.path.join(self.data_root, feature_files[0])
            print(f"   ğŸ“ˆ åŠ è½½æ ‡å‡†åŒ–ç‰¹å¾: {feature_files[0]}")
        else:
            # ä»with_targetsæ–‡ä»¶åŠ è½½
            target_pattern = f"with_targets_{symbol}_complete_*.csv"
            target_files = [f for f in os.listdir(self.data_root) if f.startswith(f"with_targets_{symbol}")]
            
            if not target_files:
                raise FileNotFoundError(f"æœªæ‰¾åˆ°ç›®æ ‡æ–‡ä»¶: {target_pattern}")
            
            # ä½¿ç”¨æœ€æ–°çš„æ–‡ä»¶
            target_files.sort(reverse=True)
            feature_file = os.path.join(self.data_root, target_files[0])
            print(f"   ğŸ“ˆ åŠ è½½ç‰¹å¾: {target_files[0]}")
        
        features_df = pd.read_csv(feature_file, index_col=0, parse_dates=True)
        
        # 2. åŠ è½½ç›®æ ‡æ•°æ®ï¼ˆä»with_targetsæ–‡ä»¶ï¼‰
        target_pattern = f"with_targets_{symbol}_complete_*.csv"
        target_files = [f for f in os.listdir(self.data_root) if f.startswith(f"with_targets_{symbol}")]
        
        if not target_files:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°ç›®æ ‡æ–‡ä»¶: {target_pattern}")
        
        # ä½¿ç”¨æœ€æ–°çš„æ–‡ä»¶
        target_files.sort(reverse=True)
        target_file = os.path.join(self.data_root, target_files[0])
        print(f"   ğŸ¯ åŠ è½½ç›®æ ‡: {target_files[0]}")
        
        targets_df = pd.read_csv(target_file, index_col=0, parse_dates=True)
        
        # 3. æ£€æŸ¥ç›®æ ‡åˆ—æ˜¯å¦å­˜åœ¨
        if target_col not in targets_df.columns:
            available_targets = [col for col in targets_df.columns if col.startswith('future_return_')]
            raise ValueError(f"ç›®æ ‡åˆ— '{target_col}' ä¸å­˜åœ¨ã€‚å¯ç”¨ç›®æ ‡: {available_targets}")
        
        # 4. æå–ç‰¹å¾åˆ—ï¼ˆæ’é™¤closeå’Œç›®æ ‡åˆ—ï¼‰
        exclude_cols = ['close'] + [col for col in features_df.columns if col.startswith('future_return_') or col.startswith('label_')]
        feature_cols = [col for col in features_df.columns if col not in exclude_cols]
        
        # 5. å¯¹é½ç´¢å¼•
        common_index = features_df.index.intersection(targets_df.index)
        features_aligned = features_df.loc[common_index, feature_cols]
        targets_aligned = targets_df.loc[common_index, target_col]
        
        # 6. è½¬æ¢ä¸ºMultiIndexæ ¼å¼ [date, ticker]
        # ä¸ºå•ä¸ªè‚¡ç¥¨åˆ›å»ºMultiIndex
        dates = features_aligned.index
        tickers = [symbol] * len(dates)
        multi_index = pd.MultiIndex.from_arrays([dates, tickers], names=['date', 'ticker'])
        
        features_multi = pd.DataFrame(features_aligned.values, index=multi_index, columns=feature_cols)
        targets_multi = pd.Series(targets_aligned.values, index=multi_index, name=target_col)
        
        # 7. æ¸…æ´—æ•°æ®ï¼ˆåˆ é™¤NaNï¼‰
        valid_mask = ~(features_multi.isna().any(axis=1) | targets_multi.isna())
        features_clean = features_multi[valid_mask]
        targets_clean = targets_multi[valid_mask]
        
        print(f"   âœ… æ•°æ®åŠ è½½å®Œæˆ:")
        print(f"      ç‰¹å¾æ•°é‡: {len(feature_cols)}")
        print(f"      æœ‰æ•ˆæ ·æœ¬: {len(features_clean)} / {len(features_aligned)}")
        print(f"      æ—¶é—´èŒƒå›´: {features_clean.index.get_level_values('date').min().date()} ~ "
              f"{features_clean.index.get_level_values('date').max().date()}")
        
        return features_clean, targets_clean
    
    def load_universe(self, 
                     symbol: str,
                     min_volume: Optional[float] = None,
                     min_price: Optional[float] = None) -> pd.DataFrame:
        """
        åŠ è½½å¯äº¤æ˜“æ ‡çš„åˆ—è¡¨ï¼ˆè¿‡æ»¤ä½æµåŠ¨æ€§ï¼‰
        
        Parameters:
        -----------
        symbol : str
            è‚¡ç¥¨ä»£ç 
        min_volume : float, optional
            æœ€å°æˆäº¤é‡
        min_price : float, optional
            æœ€å°ä»·æ ¼
            
        Returns:
        --------
        pd.DataFrame
            å¯äº¤æ˜“æ ‡çš„çš„MultiIndexæ•°æ®
        """
        print(f"ğŸ” åŠ è½½å¯äº¤æ˜“æ ‡çš„åˆ—è¡¨")
        
        # åŠ è½½åŸå§‹æ•°æ®ï¼ˆåŒ…å«volumeå’Œcloseï¼‰
        target_files = [f for f in os.listdir(self.data_root) if f.startswith(f"with_targets_{symbol}")]
        if not target_files:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
        
        target_files.sort(reverse=True)
        data_file = os.path.join(self.data_root, target_files[0])
        df = pd.read_csv(data_file, index_col=0, parse_dates=True)
        
        # åˆå§‹åŒ–è¿‡æ»¤mask
        valid_mask = pd.Series(True, index=df.index)
        
        # åº”ç”¨è¿‡æ»¤æ¡ä»¶
        if min_volume is not None and 'volume' in df.columns:
            volume_mask = df['volume'] >= min_volume
            valid_mask &= volume_mask
            print(f"   ğŸ“Š æˆäº¤é‡è¿‡æ»¤: {volume_mask.sum()} / {len(df)} æ ·æœ¬")
        
        if min_price is not None and 'close' in df.columns:
            price_mask = df['close'] >= min_price
            valid_mask &= price_mask
            print(f"   ğŸ’° ä»·æ ¼è¿‡æ»¤: {price_mask.sum()} / {len(df)} æ ·æœ¬")
        
        # è½¬æ¢ä¸ºMultiIndexæ ¼å¼
        dates = df[valid_mask].index
        tickers = [symbol] * len(dates)
        multi_index = pd.MultiIndex.from_arrays([dates, tickers], names=['date', 'ticker'])
        
        universe_df = pd.DataFrame({
            'tradable': True,
            'close': df.loc[valid_mask, 'close'].values if 'close' in df.columns else np.nan,
            'volume': df.loc[valid_mask, 'volume'].values if 'volume' in df.columns else np.nan
        }, index=multi_index)
        
        print(f"   âœ… å¯äº¤æ˜“æ ‡çš„: {len(universe_df)} ä¸ªæ—¶é—´ç‚¹")
        
        return universe_df
    
    def get_feature_list(self, symbol: str, use_scaled: bool = True) -> List[str]:
        """
        è·å–ç‰¹å¾åˆ—è¡¨
        
        Parameters:
        -----------
        symbol : str
            è‚¡ç¥¨ä»£ç 
        use_scaled : bool
            æ˜¯å¦ä½¿ç”¨æ ‡å‡†åŒ–ç‰¹å¾
            
        Returns:
        --------
        List[str]
            ç‰¹å¾åç§°åˆ—è¡¨
        """
        # ä¼˜å…ˆä»final_feature_list.txtè¯»å–
        feature_list_file = os.path.join(self.data_root, "final_feature_list.txt")
        if os.path.exists(feature_list_file):
            with open(feature_list_file, 'r', encoding='utf-8') as f:
                features = [line.strip() for line in f if line.strip()]
            print(f"   ğŸ“‹ ä»æ–‡ä»¶åŠ è½½ç‰¹å¾åˆ—è¡¨: {len(features)} ä¸ªç‰¹å¾")
            return features
        
        # å¦åˆ™ä»æ•°æ®æ–‡ä»¶ä¸­æå–
        if use_scaled:
            feature_files = [f for f in os.listdir(self.data_root) if f.startswith(f"scaler_{symbol}_scaled_features.csv")]
        else:
            feature_files = [f for f in os.listdir(self.data_root) if f.startswith(f"with_targets_{symbol}_complete_")]
        
        if not feature_files:
            raise FileNotFoundError("æœªæ‰¾åˆ°ç‰¹å¾æ–‡ä»¶")
        
        feature_file = os.path.join(self.data_root, feature_files[0])
        df = pd.read_csv(feature_file, index_col=0, nrows=1)
        
        exclude_cols = ['close'] + [col for col in df.columns if col.startswith('future_return_') or col.startswith('label_')]
        features = [col for col in df.columns if col not in exclude_cols]
        
        print(f"   ğŸ“‹ ä»æ•°æ®æ–‡ä»¶æå–ç‰¹å¾åˆ—è¡¨: {len(features)} ä¸ªç‰¹å¾")
        return features


if __name__ == "__main__":
    """
    ä½¿ç”¨ç¤ºä¾‹
    """
    print("ğŸ“Š æ•°æ®åŠ è½½å™¨æµ‹è¯•")
    print("=" * 50)
    
    try:
        # åˆå§‹åŒ–åŠ è½½å™¨
        loader = DataLoader()
        
        # åŠ è½½ç‰¹å¾å’Œç›®æ ‡
        symbol = "000001"
        features, targets = loader.load_features_and_targets(
            symbol=symbol,
            target_col='future_return_5d',
            use_scaled=True
        )
        
        print(f"\nâœ… æ•°æ®åŠ è½½æˆåŠŸ:")
        print(f"   ç‰¹å¾å½¢çŠ¶: {features.shape}")
        print(f"   ç›®æ ‡å½¢çŠ¶: {targets.shape}")
        print(f"   ç´¢å¼•ç±»å‹: {type(features.index)}")
        print(f"   ç´¢å¼•çº§åˆ«: {features.index.names}")
        
        # åŠ è½½å¯äº¤æ˜“æ ‡çš„
        universe = loader.load_universe(
            symbol=symbol,
            min_volume=1000000,
            min_price=1.0
        )
        
        print(f"\nâœ… å¯äº¤æ˜“æ ‡çš„åŠ è½½æˆåŠŸ:")
        print(f"   å½¢çŠ¶: {universe.shape}")
        
        # è·å–ç‰¹å¾åˆ—è¡¨
        feature_list = loader.get_feature_list(symbol=symbol)
        print(f"\nâœ… ç‰¹å¾åˆ—è¡¨è·å–æˆåŠŸ:")
        print(f"   ç‰¹å¾æ•°é‡: {len(feature_list)}")
        print(f"   å‰10ä¸ªç‰¹å¾: {feature_list[:10]}")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
