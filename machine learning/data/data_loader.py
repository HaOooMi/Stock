#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åŠ è½½å™¨ - ç»Ÿä¸€æ•°æ®æ¥å£

åŠŸèƒ½ï¼š
1. ä»ML outputåŠ è½½æ ‡å‡†åŒ–ç‰¹å¾æ•°æ®
2. åŠ è½½ç›®æ ‡å˜é‡æ•°æ®
3. ç»Ÿä¸€è¿”å›MultiIndex [date, ticker]æ ¼å¼
4. æ•°æ®å¯¹é½ä¸æ¸…æ´—
5. é›†æˆæ•°æ®å¿«ç…§ç®¡ç†
6. é›†æˆäº¤æ˜“å¯è¡Œæ€§è¿‡æ»¤
7. é›†æˆPITæ•°æ®å¯¹é½
"""

import os
import sys
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
ml_root = os.path.dirname(current_dir)
project_root = os.path.dirname(ml_root)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# å¯¼å…¥æ–°æ¨¡å—
try:
    from data.data_snapshot import DataSnapshot
    from data.tradability_filter import TradabilityFilter
    from data.pit_aligner import PITDataAligner
    from data.market_data_loader import MarketDataLoader
except ImportError:
    # å¦‚æœæ¨¡å—æœªæ‰¾åˆ°ï¼Œå°è¯•ç›¸å¯¹å¯¼å…¥
    try:
        from data_snapshot import DataSnapshot
        from tradability_filter import TradabilityFilter
        from pit_aligner import PITDataAligner
        from market_data_loader import MarketDataLoader
    except ImportError:
        print("âš ï¸  è­¦å‘Š: æ— æ³•å¯¼å…¥æ•°æ®æ¸…æ´—æ¨¡å—ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")
        DataSnapshot = None
        TradabilityFilter = None
        PITDataAligner = None
        MarketDataLoader = None


class DataLoader:
    """
    æ•°æ®åŠ è½½å™¨ç±»ï¼ˆå¢å¼ºç‰ˆï¼‰
    
    åŠŸèƒ½ï¼š
    1. åŠ è½½ç‰¹å¾å’Œç›®æ ‡æ•°æ®
    2. æ•°æ®å¯¹é½ä¸æ¸…æ´—
    3. ç»Ÿä¸€æ ¼å¼ä¸ºMultiIndex
    4. æ•°æ®å¿«ç…§ç®¡ç†
    5. äº¤æ˜“å¯è¡Œæ€§è¿‡æ»¤
    6. PITæ•°æ®å¯¹é½
    """
    
    def __init__(self, 
                 data_root: str = "ML output",
                 enable_snapshot: bool = True,
                 enable_filtering: bool = True,
                 enable_pit_alignment: bool = True,
                 enable_influxdb: bool = True,
                 influxdb_config: Optional[Dict[str, str]] = None,
                 filter_config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        Parameters:
        -----------
        data_root : str
            æ•°æ®æ ¹ç›®å½•
        enable_snapshot : bool
            æ˜¯å¦å¯ç”¨å¿«ç…§ç®¡ç†
        enable_filtering : bool
            æ˜¯å¦å¯ç”¨äº¤æ˜“å¯è¡Œæ€§è¿‡æ»¤
        enable_pit_alignment : bool
            æ˜¯å¦å¯ç”¨PITå¯¹é½
        enable_influxdb : bool
            æ˜¯å¦å¯ç”¨InfluxDBæ•°æ®åŠ è½½
        influxdb_config : dict, optional
            InfluxDBé…ç½®
        filter_config : dict, optional
            è¿‡æ»¤å™¨é…ç½®
        """
        if os.path.isabs(data_root):
            self.data_root = data_root
        else:
            self.data_root = os.path.join(ml_root, data_root)
        
        # åŠŸèƒ½å¼€å…³
        self.enable_snapshot = enable_snapshot
        self.enable_filtering = enable_filtering
        self.enable_pit_alignment = enable_pit_alignment
        self.enable_influxdb = enable_influxdb
        
        # åˆå§‹åŒ–å¸‚åœºæ•°æ®åŠ è½½å™¨ï¼ˆInfluxDB + MySQLï¼‰
        if enable_influxdb and MarketDataLoader is not None:
            influxdb_config = influxdb_config or {}
            try:
                self.market_data_loader = MarketDataLoader(**influxdb_config)
            except Exception as e:
                print(f"   âš ï¸  å¸‚åœºæ•°æ®åŠ è½½å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.market_data_loader = None
        else:
            self.market_data_loader = None
        
        # åˆå§‹åŒ–å­æ¨¡å—
        if enable_snapshot and DataSnapshot is not None:
            self.snapshot_mgr = DataSnapshot(output_dir=self.data_root)
        else:
            self.snapshot_mgr = None
        
        if enable_filtering and TradabilityFilter is not None:
            filter_config = filter_config or {}
            self.filter_engine = TradabilityFilter(**filter_config)
        else:
            self.filter_engine = None
        
        if enable_pit_alignment and PITDataAligner is not None:
            self.pit_aligner = PITDataAligner()
        else:
            self.pit_aligner = None
        
        print(f"ğŸ“ æ•°æ®åŠ è½½å™¨åˆå§‹åŒ–ï¼ˆå¢å¼ºç‰ˆï¼‰")
        print(f"   æ•°æ®æ ¹ç›®å½•: {self.data_root}")
        print(f"   å¿«ç…§ç®¡ç†: {'âœ…' if enable_snapshot else 'âŒ'}")
        print(f"   äº¤æ˜“è¿‡æ»¤: {'âœ…' if enable_filtering else 'âŒ'}")
        print(f"   PITå¯¹é½: {'âœ…' if enable_pit_alignment else 'âŒ'}")
        print(f"   å¸‚åœºæ•°æ®: {'âœ…' if self.market_data_loader is not None else 'âŒ'}")
    
    def _load_csv_with_encoding(self, file_path: str) -> pd.DataFrame:
        """
        å°è¯•å¤šç§ç¼–ç è¯»å–CSVæ–‡ä»¶
        
        Parameters:
        -----------
        file_path : str
            æ–‡ä»¶è·¯å¾„
            
        Returns:
        --------
        pd.DataFrame
            è¯»å–çš„æ•°æ®
        """
        encodings = ['utf-8', 'gbk', 'latin1', 'cp1252']
        
        for encoding in encodings:
            try:
                return pd.read_csv(file_path, index_col=0, parse_dates=True, encoding=encoding)
            except UnicodeDecodeError:
                continue
            except Exception as e:
                if encoding == encodings[-1]:
                    raise Exception(f"æ— æ³•è¯»å–CSVæ–‡ä»¶ {file_path}: {str(e)}")
        
        raise Exception(f"æ— æ³•ä»¥ä»»ä½•æ”¯æŒçš„ç¼–ç è¯»å–CSVæ–‡ä»¶: {file_path}")
    
    def load_features_and_targets(self, 
                                  symbol: str,
                                  target_col: str = 'future_return_5d',
                                  use_scaled: bool = True) -> Tuple[pd.DataFrame, pd.Series]:
        """
        åŠ è½½ç‰¹å¾å’Œç›®æ ‡æ•°æ®ï¼ˆä»ML outputç›®å½•ï¼‰
        
        Parameters:
        -----------
        symbol : str
            è‚¡ç¥¨ä»£ç 
        target_col : str
            ç›®æ ‡åˆ—å
        use_scaled : bool
            æ˜¯å¦ä½¿ç”¨æ ‡å‡†åŒ–åçš„ç‰¹å¾
            
        Returns:
        --------
        Tuple[pd.DataFrame, pd.Series]
            (ç‰¹å¾æ•°æ®, ç›®æ ‡æ•°æ®)ï¼Œç´¢å¼•ä¸ºMultiIndex [date, ticker]
        """
        print(f"ğŸ“Š åŠ è½½æ•°æ®: {symbol}")
        
        # ç¡®å®šæ ¹ç›®å½•ï¼ˆML outputï¼‰
        if 'ML output' in self.data_root:
            ml_output_root = self.data_root.split('ML output')[0] + 'ML output'
        else:
            ml_output_root = self.data_root
        
        # 1. åŠ è½½ç‰¹å¾æ•°æ®
        if use_scaled:
            # æ ‡å‡†åŒ–ç‰¹å¾åœ¨ scalers/baseline_v1 ç›®å½•
            scalers_dir = os.path.join(ml_output_root, 'scalers', 'baseline_v1')
            feature_pattern = f"scaler_{symbol}_scaled_features.csv"
            
            if not os.path.exists(scalers_dir):
                raise FileNotFoundError(f"æ ‡å‡†åŒ–å™¨ç›®å½•ä¸å­˜åœ¨: {scalers_dir}")
            
            feature_files = [f for f in os.listdir(scalers_dir) 
                           if f == feature_pattern]
            
            if not feature_files:
                raise FileNotFoundError(f"æœªæ‰¾åˆ°æ ‡å‡†åŒ–ç‰¹å¾æ–‡ä»¶: {feature_pattern} (ç›®å½•: {scalers_dir})")
            
            feature_file = os.path.join(scalers_dir, feature_files[0])
            print(f"   ğŸ“ˆ åŠ è½½æ ‡å‡†åŒ–ç‰¹å¾: {feature_files[0]}")
        else:
            # ä»with_targetsæ–‡ä»¶åŠ è½½
            target_pattern = f"with_targets_{symbol}_complete_*.csv"
            target_files = [f for f in os.listdir(self.data_root) if f.startswith(f"with_targets_{symbol}")]
            
            if not target_files:
                raise FileNotFoundError(f"æœªæ‰¾åˆ°ç›®æ ‡æ–‡ä»¶: {target_pattern}")
            
            # ä½¿ç”¨æœ€æ–°çš„æ–‡ä»¶
            target_files.sort(reverse=True)
            feature_file = os.path.join(self.data_root, target_files[0])
            print(f"   ğŸ“ˆ åŠ è½½ç‰¹å¾: {target_files[0]}")
        
        # åŠ è½½ç‰¹å¾æ•°æ®ï¼ˆå°è¯•å¤šç§ç¼–ç ï¼‰
        features_df = self._load_csv_with_encoding(feature_file)
        
        # 2. åŠ è½½ç›®æ ‡æ•°æ®ï¼ˆä» datasets ç›®å½•ï¼‰
        target_pattern = f"with_targets_{symbol}_complete_*.csv"
        
        target_files = [f for f in os.listdir(self.data_root) 
                       if f.startswith(f"with_targets_{symbol}_complete_") and f.endswith('.csv')]
        
        if not target_files:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°ç›®æ ‡æ–‡ä»¶: {target_pattern} (ç›®å½•: {self.data_root})")
        
        # ä½¿ç”¨æœ€æ–°çš„æ–‡ä»¶
        target_files.sort(reverse=True)
        target_file = os.path.join(self.data_root, target_files[0])
        print(f"   ğŸ¯ åŠ è½½ç›®æ ‡: {target_files[0]}")
        
        # åŠ è½½ç›®æ ‡æ•°æ®ï¼ˆå°è¯•å¤šç§ç¼–ç ï¼‰
        targets_df = self._load_csv_with_encoding(target_file)
        
        # 3. æ£€æŸ¥ç›®æ ‡åˆ—æ˜¯å¦å­˜åœ¨
        if target_col not in targets_df.columns:
            available_targets = [col for col in targets_df.columns if col.startswith('future_return_')]
            raise ValueError(f"ç›®æ ‡åˆ— '{target_col}' ä¸å­˜åœ¨ã€‚å¯ç”¨ç›®æ ‡: {available_targets}")
        
        # 4. æå–ç‰¹å¾åˆ—ï¼ˆæ’é™¤closeå’Œç›®æ ‡åˆ—ï¼‰
        exclude_cols = ['close'] + [col for col in features_df.columns if col.startswith('future_return_') or col.startswith('label_')]
        feature_cols = [col for col in features_df.columns if col not in exclude_cols]
        
        # 5. å¯¹é½ç´¢å¼•
        common_index = features_df.index.intersection(targets_df.index)
        features_aligned = features_df.loc[common_index, feature_cols]
        targets_aligned = targets_df.loc[common_index, target_col]
        
        # 6. è½¬æ¢ä¸ºMultiIndexæ ¼å¼ [date, ticker]
        # ä¸ºå•ä¸ªè‚¡ç¥¨åˆ›å»ºMultiIndex
        dates = features_aligned.index
        tickers = [symbol] * len(dates)
        multi_index = pd.MultiIndex.from_arrays([dates, tickers], names=['date', 'ticker'])
        
        features_multi = pd.DataFrame(features_aligned.values, index=multi_index, columns=feature_cols)
        targets_multi = pd.Series(targets_aligned.values, index=multi_index, name=target_col)
        
        # 7. æ¸…æ´—æ•°æ®ï¼ˆåˆ é™¤NaNï¼‰
        valid_mask = ~(features_multi.isna().any(axis=1) | targets_multi.isna())
        features_clean = features_multi[valid_mask]
        targets_clean = targets_multi[valid_mask]
        
        print(f"   âœ… æ•°æ®åŠ è½½å®Œæˆ:")
        print(f"      ç‰¹å¾æ•°é‡: {len(feature_cols)}")
        print(f"      æœ‰æ•ˆæ ·æœ¬: {len(features_clean)} / {len(features_aligned)}")
        print(f"      æ—¶é—´èŒƒå›´: {features_clean.index.get_level_values('date').min().date()} ~ "
              f"{features_clean.index.get_level_values('date').max().date()}")
        
        return features_clean, targets_clean
    
    def _load_market_data_from_influxdb(self,
                                       symbol: str,
                                       start_date: str,
                                       end_date: str) -> pd.DataFrame:
        """
        ä» InfluxDB åŠ è½½å¸‚åœºæ•°æ®
        
        Parameters:
        -----------
        symbol : str
            è‚¡ç¥¨ä»£ç 
        start_date : str
            å¼€å§‹æ—¥æœŸ
        end_date : str
            ç»“æŸæ—¥æœŸ
            
        Returns:
        --------
        pd.DataFrame
            å¸‚åœºæ•°æ®ï¼Œç´¢å¼•ä¸ºæ—¥æœŸ
        """
        if self.market_data_loader is None:
            print(f"   âš ï¸  å¸‚åœºæ•°æ®åŠ è½½å™¨æœªå¯ç”¨ï¼Œè·³è¿‡å¸‚åœºæ•°æ®åŠ è½½")
            return pd.DataFrame()
        
        try:
            market_df = self.market_data_loader.load_market_data(
                symbol=symbol,
                start_date=start_date,
                end_date=end_date
            )
            return market_df
        except Exception as e:
            print(f"   âš ï¸  ä»å¸‚åœºæ•°æ®æºåŠ è½½æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _merge_market_data(self,
                          features: pd.DataFrame,
                          market_df: pd.DataFrame,
                          symbol: str) -> pd.DataFrame:
        """
        åˆå¹¶å¸‚åœºæ•°æ®åˆ°ç‰¹å¾æ•°æ®
        
        Parameters:
        -----------
        features : pd.DataFrame
            ç‰¹å¾æ•°æ®ï¼ŒMultiIndex [date, ticker]
        market_df : pd.DataFrame
            å¸‚åœºæ•°æ®ï¼Œç´¢å¼•ä¸ºæ—¥æœŸ
        symbol : str
            è‚¡ç¥¨ä»£ç 
            
        Returns:
        --------
        pd.DataFrame
            åˆå¹¶åçš„æ•°æ®
        """
        if market_df.empty:
            return features
        
        # æå–ç‰¹å¾æ•°æ®çš„æ—¥æœŸç´¢å¼•
        dates = features.index.get_level_values('date')
        
        # å¯¹é½å¸‚åœºæ•°æ®åˆ°ç‰¹å¾æ•°æ®çš„æ—¥æœŸ
        market_aligned = market_df.reindex(dates)
        
        # æ·»åŠ éœ€è¦çš„åˆ—åˆ°ç‰¹å¾æ•°æ®
        # æ³¨æ„ï¼šä¸è¦†ç›–å·²å­˜åœ¨çš„åˆ—
        for col in market_aligned.columns:
            if col not in features.columns:
                features[col] = market_aligned[col].values
        
        print(f"   âœ… å¸‚åœºæ•°æ®åˆå¹¶å®Œæˆ: æ·»åŠ  {len(market_aligned.columns)} åˆ—")
        
        return features
    
    def load_universe(self, 
                     symbol: str,
                     min_volume: Optional[float] = None,
                     min_price: Optional[float] = None) -> pd.DataFrame:
        """
        åŠ è½½å¯äº¤æ˜“æ ‡çš„åˆ—è¡¨ï¼ˆè¿‡æ»¤ä½æµåŠ¨æ€§ï¼‰
        
        Parameters:
        -----------
        symbol : str
            è‚¡ç¥¨ä»£ç 
        min_volume : float, optional
            æœ€å°æˆäº¤é‡
        min_price : float, optional
            æœ€å°ä»·æ ¼
            
        Returns:
        --------
        pd.DataFrame
            å¯äº¤æ˜“æ ‡çš„çš„MultiIndexæ•°æ®
        """
        print(f"ğŸ” åŠ è½½å¯äº¤æ˜“æ ‡çš„åˆ—è¡¨")
        
        # åŠ è½½åŸå§‹æ•°æ®ï¼ˆåŒ…å«volumeå’Œcloseï¼‰
        target_files = [f for f in os.listdir(self.data_root) if f.startswith(f"with_targets_{symbol}")]
        if not target_files:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
        
        target_files.sort(reverse=True)
        data_file = os.path.join(self.data_root, target_files[0])
        df = pd.read_csv(data_file, index_col=0, parse_dates=True)
        
        # åˆå§‹åŒ–è¿‡æ»¤mask
        valid_mask = pd.Series(True, index=df.index)
        
        # åº”ç”¨è¿‡æ»¤æ¡ä»¶
        if min_volume is not None and 'volume' in df.columns:
            volume_mask = df['volume'] >= min_volume
            valid_mask &= volume_mask
            print(f"   ğŸ“Š æˆäº¤é‡è¿‡æ»¤: {volume_mask.sum()} / {len(df)} æ ·æœ¬")
        
        if min_price is not None and 'close' in df.columns:
            price_mask = df['close'] >= min_price
            valid_mask &= price_mask
            print(f"   ğŸ’° ä»·æ ¼è¿‡æ»¤: {price_mask.sum()} / {len(df)} æ ·æœ¬")
        
        # è½¬æ¢ä¸ºMultiIndexæ ¼å¼
        dates = df[valid_mask].index
        tickers = [symbol] * len(dates)
        multi_index = pd.MultiIndex.from_arrays([dates, tickers], names=['date', 'ticker'])
        
        universe_df = pd.DataFrame({
            'tradable': True,
            'close': df.loc[valid_mask, 'close'].values if 'close' in df.columns else np.nan,
            'volume': df.loc[valid_mask, 'volume'].values if 'volume' in df.columns else np.nan
        }, index=multi_index)
        
        print(f"   âœ… å¯äº¤æ˜“æ ‡çš„: {len(universe_df)} ä¸ªæ—¶é—´ç‚¹")
        
        return universe_df
    
    def get_feature_list(self, symbol: str, use_scaled: bool = True) -> List[str]:
        """
        è·å–ç‰¹å¾åˆ—è¡¨
        
        Parameters:
        -----------
        symbol : str
            è‚¡ç¥¨ä»£ç 
        use_scaled : bool
            æ˜¯å¦ä½¿ç”¨æ ‡å‡†åŒ–ç‰¹å¾
            
        Returns:
        --------
        List[str]
            ç‰¹å¾åç§°åˆ—è¡¨
        """
        # ä¼˜å…ˆä»final_feature_list.txtè¯»å–
        feature_list_file = os.path.join(self.data_root, "final_feature_list.txt")
        if os.path.exists(feature_list_file):
            with open(feature_list_file, 'r', encoding='utf-8') as f:
                features = [line.strip() for line in f if line.strip()]
            print(f"   ğŸ“‹ ä»æ–‡ä»¶åŠ è½½ç‰¹å¾åˆ—è¡¨: {len(features)} ä¸ªç‰¹å¾")
            return features
        
        # å¦åˆ™ä»æ•°æ®æ–‡ä»¶ä¸­æå–
        if use_scaled:
            feature_files = [f for f in os.listdir(self.data_root) if f.startswith(f"scaler_{symbol}_scaled_features.csv")]
        else:
            feature_files = [f for f in os.listdir(self.data_root) if f.startswith(f"with_targets_{symbol}_complete_")]
        
        if not feature_files:
            raise FileNotFoundError("æœªæ‰¾åˆ°ç‰¹å¾æ–‡ä»¶")
        
        feature_file = os.path.join(self.data_root, feature_files[0])
        df = pd.read_csv(feature_file, index_col=0, nrows=1)
        
        exclude_cols = ['close'] + [col for col in df.columns if col.startswith('future_return_') or col.startswith('label_')]
        features = [col for col in df.columns if col not in exclude_cols]
        
        print(f"   ğŸ“‹ ä»æ•°æ®æ–‡ä»¶æå–ç‰¹å¾åˆ—è¡¨: {len(features)} ä¸ªç‰¹å¾")
        return features
    
    def load_with_snapshot(self,
                          symbol: str,
                          start_date: str,
                          end_date: str,
                          target_col: str = 'future_return_5d',
                          use_scaled: bool = True,
                          filters: Optional[Dict[str, Any]] = None,
                          random_seed: int = 42,
                          save_parquet: bool = True) -> Tuple[pd.DataFrame, pd.Series, str]:
        """
        åŠ è½½æ•°æ®å¹¶åˆ›å»ºå¿«ç…§ï¼ˆæ¨èä½¿ç”¨ï¼‰
        
        Parameters:
        -----------
        symbol : str
            è‚¡ç¥¨ä»£ç 
        start_date : str
            å¼€å§‹æ—¥æœŸ
        end_date : str
            ç»“æŸæ—¥æœŸ
        target_col : str
            ç›®æ ‡åˆ—å
        use_scaled : bool
            æ˜¯å¦ä½¿ç”¨æ ‡å‡†åŒ–ç‰¹å¾
        filters : dict, optional
            è¿‡æ»¤å‚æ•°
        random_seed : int
            éšæœºç§å­
        save_parquet : bool
            æ˜¯å¦ä¿å­˜ä¸ºParquet
            
        Returns:
        --------
        Tuple[pd.DataFrame, pd.Series, str]
            (ç‰¹å¾æ•°æ®, ç›®æ ‡æ•°æ®, å¿«ç…§ID)
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“Š åŠ è½½æ•°æ®å¹¶åˆ›å»ºå¿«ç…§")
        print(f"{'='*60}")
        
        # 1. åŠ è½½åŸå§‹æ•°æ®
        features, targets = self.load_features_and_targets(
            symbol=symbol,
            target_col=target_col,
            use_scaled=use_scaled
        )
        
        # 2. ä»å¸‚åœºæ•°æ®æºåŠ è½½æ•°æ®ï¼ˆInfluxDB + MySQLï¼‰
        if self.enable_influxdb and self.market_data_loader is not None:
            print(f"\n[å¸‚åœºæ•°æ®] åŠ è½½ InfluxDB + MySQL æ•°æ®")
            market_df = self._load_market_data_from_influxdb(
                symbol=symbol,
                start_date=start_date,
                end_date=end_date
            )
            
            if not market_df.empty:
                # åˆå¹¶å¸‚åœºæ•°æ®åˆ°ç‰¹å¾æ•°æ®
                features = self._merge_market_data(features, market_df, symbol)
        
        # 3. åº”ç”¨äº¤æ˜“å¯è¡Œæ€§è¿‡æ»¤
        if self.enable_filtering and self.filter_engine is not None:
            # åˆå¹¶ç‰¹å¾å’Œç›®æ ‡ä»¥ä¾¿è¿‡æ»¤
            combined_data = features.copy()
            combined_data[target_col] = targets
            
            # åº”ç”¨è¿‡æ»¤
            filter_log_path = os.path.join(
                self.data_root, 
                'datasets', 
                'baseline_v1', 
                f'filter_log_{symbol}.csv'
            )
            
            filtered_data, filter_log = self.filter_engine.apply_filters(
                combined_data,
                save_log=True,
                log_path=filter_log_path
            )
            
            # æå–å¯äº¤æ˜“æ ·æœ¬
            tradable_mask = filtered_data['tradable_flag'] == 1
            features = filtered_data[tradable_mask][features.columns]
            targets = filtered_data[tradable_mask][target_col]
            
            print(f"\n   âœ… äº¤æ˜“è¿‡æ»¤å®Œæˆ: {len(features)} ä¸ªå¯äº¤æ˜“æ ·æœ¬")
        
        # 4. PITå¯¹é½éªŒè¯
        if self.enable_pit_alignment and self.pit_aligner is not None:
            combined_data = features.copy()
            combined_data[target_col] = targets
            
            pit_results = self.pit_aligner.validate_pit_alignment(
                combined_data,
                target_col=target_col
            )
            
            if not pit_results.get('overall_pass', False):
                print(f"   âš ï¸  è­¦å‘Š: PITå¯¹é½éªŒè¯æœªé€šè¿‡")
        
        # 5. åˆ›å»ºæ•°æ®å¿«ç…§
        snapshot_id = None
        if self.enable_snapshot and self.snapshot_mgr is not None:
            # å‡†å¤‡å¿«ç…§æ•°æ®
            snapshot_data = features.copy()
            snapshot_data[target_col] = targets
            
            # è¿‡æ»¤å‚æ•°
            filters = filters or {
                'min_volume': getattr(self.filter_engine, 'min_volume', None) if self.filter_engine else None,
                'min_price': getattr(self.filter_engine, 'min_price', None) if self.filter_engine else None,
                'exclude_st': getattr(self.filter_engine, 'exclude_st', None) if self.filter_engine else None
            }
            
            # åˆ›å»ºå¿«ç…§
            snapshot_path = self.snapshot_mgr.create_snapshot(
                data=snapshot_data,
                symbol=symbol,
                start_date=start_date,
                end_date=end_date,
                filters=filters,
                random_seed=random_seed,
                save_parquet=save_parquet
            )
            
            snapshot_id = self.snapshot_mgr.snapshot_id
            print(f"\n   âœ… æ•°æ®å¿«ç…§åˆ›å»ºå®Œæˆ: {snapshot_id}")
        
        print(f"\n{'='*60}")
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
        print(f"{'='*60}")
        print(f"   ç‰¹å¾æ•°é‡: {len(features.columns)}")
        print(f"   æ ·æœ¬æ•°é‡: {len(features)}")
        print(f"   å¿«ç…§ID: {snapshot_id or 'N/A'}")
        print(f"{'='*60}\n")
        
        return features, targets, snapshot_id
    
    def load_from_snapshot(self, snapshot_id: str) -> Tuple[pd.DataFrame, pd.Series]:
        """
        ä»å¿«ç…§åŠ è½½æ•°æ®
        
        Parameters:
        -----------
        snapshot_id : str
            å¿«ç…§ID
            
        Returns:
        --------
        Tuple[pd.DataFrame, pd.Series]
            (ç‰¹å¾æ•°æ®, ç›®æ ‡æ•°æ®)
        """
        if self.snapshot_mgr is None:
            raise RuntimeError("å¿«ç…§ç®¡ç†å™¨æœªå¯ç”¨")
        
        # åŠ è½½å¿«ç…§
        data, metadata = self.snapshot_mgr.load_snapshot(snapshot_id)
        
        # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
        target_col = metadata.get('target_col', 'future_return_5d')
        
        # å¦‚æœç›®æ ‡åˆ—åœ¨æ•°æ®ä¸­
        if target_col in data.columns:
            targets = data[target_col]
            features = data.drop(columns=[target_col])
        else:
            # å¦åˆ™å‡è®¾æ‰€æœ‰åˆ—éƒ½æ˜¯ç‰¹å¾
            features = data
            targets = pd.Series(index=data.index, dtype=float)
        
        print(f"âœ… ä»å¿«ç…§åŠ è½½æ•°æ®: {snapshot_id}")
        print(f"   ç‰¹å¾æ•°é‡: {len(features.columns)}")
        print(f"   æ ·æœ¬æ•°é‡: {len(features)}")
        
        return features, targets


if __name__ == "__main__":
    """
    ä½¿ç”¨ç¤ºä¾‹
    """
    print("ğŸ“Š æ•°æ®åŠ è½½å™¨æµ‹è¯•")
    print("=" * 50)
    
    try:
        # åˆå§‹åŒ–åŠ è½½å™¨
        loader = DataLoader()
        
        # åŠ è½½ç‰¹å¾å’Œç›®æ ‡
        symbol = "000001"
        features, targets = loader.load_features_and_targets(
            symbol=symbol,
            target_col='future_return_5d',
            use_scaled=True
        )
        
        print(f"\nâœ… æ•°æ®åŠ è½½æˆåŠŸ:")
        print(f"   ç‰¹å¾å½¢çŠ¶: {features.shape}")
        print(f"   ç›®æ ‡å½¢çŠ¶: {targets.shape}")
        print(f"   ç´¢å¼•ç±»å‹: {type(features.index)}")
        print(f"   ç´¢å¼•çº§åˆ«: {features.index.names}")
        
        # åŠ è½½å¯äº¤æ˜“æ ‡çš„
        universe = loader.load_universe(
            symbol=symbol,
            min_volume=1000000,
            min_price=1.0
        )
        
        print(f"\nâœ… å¯äº¤æ˜“æ ‡çš„åŠ è½½æˆåŠŸ:")
        print(f"   å½¢çŠ¶: {universe.shape}")
        
        # è·å–ç‰¹å¾åˆ—è¡¨
        feature_list = loader.get_feature_list(symbol=symbol)
        print(f"\nâœ… ç‰¹å¾åˆ—è¡¨è·å–æˆåŠŸ:")
        print(f"   ç‰¹å¾æ•°é‡: {len(feature_list)}")
        print(f"   å‰10ä¸ªç‰¹å¾: {feature_list[:10]}")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
