#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®å¿«ç…§ç®¡ç†å™¨ - Data Snapshot Manager

åŠŸèƒ½ï¼š
1. æ•°æ®ç‰ˆæœ¬åŒ–ä¸å¿«ç…§ç®¡ç†
2. Point-in-Time (PIT) æ•°æ®ç¡®ä¿æ— å‰è§†åå·®
3. æ•°æ®è´¨é‡æ£€æŸ¥ä¸æŠ¥å‘Š
4. å®éªŒå…ƒæ•°æ®è®°å½•

ç¬¦åˆç ”ç©¶å®ªç« è¦æ±‚ï¼š
- å†å²æˆåˆ†ç®¡ç†ï¼ˆé¿å…å¹¸å­˜è€…åå·®ï¼‰
- è´¢åŠ¡æ•°æ®PITå¯¹é½
- äº¤æ˜“å¯è¡Œæ€§è¿‡æ»¤
- æ•°æ®è´¨é‡æŠ¥è¡¨
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
import hashlib
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
ml_root = os.path.dirname(current_dir)
project_root = os.path.dirname(ml_root)
if project_root not in sys.path:
    sys.path.insert(0, project_root)


class DataSnapshot:
    """
    æ•°æ®å¿«ç…§ç®¡ç†å™¨
    
    èŒè´£ï¼š
    1. åˆ›å»ºæ•°æ®å¿«ç…§å¹¶åˆ†é…å”¯ä¸€ID
    2. è®°å½•å¿«ç…§å…ƒæ•°æ®ï¼ˆæ ·æœ¬æœŸã€è‚¡ç¥¨æ± ã€è¿‡æ»¤é˜ˆå€¼ç­‰ï¼‰
    3. éªŒè¯æ•°æ®è´¨é‡
    4. ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š
    """
    
    def __init__(self, 
                 output_dir: str = "ML output",
                 snapshot_id: Optional[str] = None):
        """
        åˆå§‹åŒ–æ•°æ®å¿«ç…§ç®¡ç†å™¨
        
        Parameters:
        -----------
        output_dir : str
            è¾“å‡ºç›®å½•
        snapshot_id : str, optional
            å¿«ç…§IDï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
        """
        if os.path.isabs(output_dir):
            self.output_dir = output_dir
        else:
            self.output_dir = os.path.join(ml_root, output_dir)
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        self.snapshots_dir = os.path.join(self.output_dir, 'snapshots')
        self.quality_reports_dir = os.path.join(self.output_dir, 'reports', 'data_quality')
        os.makedirs(self.snapshots_dir, exist_ok=True)
        os.makedirs(self.quality_reports_dir, exist_ok=True)
        
        # ç”Ÿæˆæˆ–ä½¿ç”¨æŒ‡å®šçš„å¿«ç…§ID
        if snapshot_id is None:
            self.snapshot_id = self._generate_snapshot_id()
        else:
            self.snapshot_id = snapshot_id
        
        # å¿«ç…§å…ƒæ•°æ®
        self.metadata = {
            'snapshot_id': self.snapshot_id,
            'created_at': datetime.now().isoformat(),
            'version': '1.0',
            'data_sources': [],
            'filters': {},
            'quality_checks': {},
            'statistics': {}
        }
        
        print(f"ğŸ“¸ æ•°æ®å¿«ç…§ç®¡ç†å™¨åˆå§‹åŒ–")
        print(f"   å¿«ç…§ID: {self.snapshot_id}")
        print(f"   å¿«ç…§ç›®å½•: {self.snapshots_dir}")
    
    def _generate_snapshot_id(self) -> str:
        """
        ç”Ÿæˆå¿«ç…§ID
        
        æ ¼å¼: ds_YYYYQQ_vN (å¦‚ ds_2025Q4_v1)
        
        Returns:
        --------
        str
            å¿«ç…§ID
        """
        now = datetime.now()
        year = now.year
        quarter = (now.month - 1) // 3 + 1
        
        # æŸ¥æ‰¾å½“å‰å­£åº¦çš„æœ€å¤§ç‰ˆæœ¬å·
        prefix = f"ds_{year}Q{quarter}"
        existing_snapshots = [d for d in os.listdir(self.snapshots_dir) 
                            if d.startswith(prefix)]
        
        if existing_snapshots:
            versions = [int(s.split('_v')[-1]) for s in existing_snapshots 
                       if '_v' in s]
            next_version = max(versions) + 1 if versions else 1
        else:
            next_version = 1
        
        return f"{prefix}_v{next_version}"
    
    def create_snapshot(self,
                       data: pd.DataFrame,
                       symbol: str,
                       start_date: str,
                       end_date: str,
                       filters: Dict[str, Any],
                       random_seed: int = 42,
                       save_parquet: bool = True) -> str:
        """
        åˆ›å»ºæ•°æ®å¿«ç…§
        
        Parameters:
        -----------
        data : pd.DataFrame
            åŸå§‹æ•°æ®ï¼ˆMultiIndex [date, ticker]ï¼‰
        symbol : str
            è‚¡ç¥¨ä»£ç 
        start_date : str
            å¼€å§‹æ—¥æœŸ
        end_date : str
            ç»“æŸæ—¥æœŸ
        filters : dict
            è¿‡æ»¤å‚æ•°
        random_seed : int
            éšæœºç§å­
        save_parquet : bool
            æ˜¯å¦ä¿å­˜ä¸ºParquetæ ¼å¼
            
        Returns:
        --------
        str
            å¿«ç…§è·¯å¾„
        """
        print(f"\nğŸ“¸ åˆ›å»ºæ•°æ®å¿«ç…§: {self.snapshot_id}")
        
        # æ›´æ–°å…ƒæ•°æ®
        self.metadata['symbol'] = symbol
        self.metadata['start_date'] = start_date
        self.metadata['end_date'] = end_date
        self.metadata['filters'] = filters
        self.metadata['random_seed'] = random_seed
        self.metadata['n_samples'] = len(data)
        self.metadata['n_features'] = len(data.columns)
        
        # æ•°æ®è´¨é‡æ£€æŸ¥
        quality_report = self.check_data_quality(data)
        self.metadata['quality_checks'] = quality_report
        
        # è®¡ç®—æ•°æ®å“ˆå¸Œï¼ˆç”¨äºéªŒè¯æ•°æ®å®Œæ•´æ€§ï¼‰
        data_hash = self._calculate_data_hash(data)
        self.metadata['data_hash'] = data_hash
        
        # åˆ›å»ºå¿«ç…§ç›®å½•
        snapshot_path = os.path.join(self.snapshots_dir, self.snapshot_id)
        os.makedirs(snapshot_path, exist_ok=True)
        
        # ä¿å­˜æ•°æ®
        if save_parquet:
            # Parquetæ ¼å¼ï¼ˆæ¨èï¼Œå¿«é€Ÿä¸”åˆ—å­˜ï¼‰
            data_file = os.path.join(snapshot_path, f'{symbol}_data.parquet')
            data.to_parquet(data_file, engine='pyarrow', compression='snappy')
            self.metadata['data_file'] = data_file
            self.metadata['data_format'] = 'parquet'
            print(f"   ğŸ’¾ æ•°æ®å·²ä¿å­˜: {data_file}")
        else:
            # CSVæ ¼å¼ï¼ˆå¤‡é€‰ï¼‰
            data_file = os.path.join(snapshot_path, f'{symbol}_data.csv')
            data.to_csv(data_file)
            self.metadata['data_file'] = data_file
            self.metadata['data_format'] = 'csv'
            print(f"   ğŸ’¾ æ•°æ®å·²ä¿å­˜: {data_file}")
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata_file = os.path.join(snapshot_path, 'metadata.json')
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, indent=2, ensure_ascii=False, default=str)
        print(f"   ğŸ“ å…ƒæ•°æ®å·²ä¿å­˜: {metadata_file}")
        
        # ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š
        self.save_quality_report(quality_report)
        
        print(f"   âœ… å¿«ç…§åˆ›å»ºå®Œæˆ: {snapshot_path}")
        
        return snapshot_path
    
    def check_data_quality(self, data: pd.DataFrame) -> Dict[str, Any]:
        """
        æ•°æ®è´¨é‡æ£€æŸ¥
        
        æ£€æŸ¥é¡¹ï¼š
        1. ç¼ºå¤±ç‡
        2. æå€¼æ¯”ä¾‹
        3. é‡å¤æ•°æ®
        4. åœç‰Œ/æ¶¨è·Œåœæ¯”ä¾‹
        5. å¯äº¤æ˜“æ ·æœ¬æ•°
        
        Parameters:
        -----------
        data : pd.DataFrame
            æ•°æ®
            
        Returns:
        --------
        dict
            è´¨é‡æŠ¥å‘Š
        """
        print(f"\nğŸ” æ‰§è¡Œæ•°æ®è´¨é‡æ£€æŸ¥...")
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'total_samples': len(data),
            'checks': {}
        }
        
        # 1. ç¼ºå¤±ç‡æ£€æŸ¥
        missing_ratio = data.isna().sum() / len(data)
        report['checks']['missing_ratio'] = {
            'max': float(missing_ratio.max()),
            'mean': float(missing_ratio.mean()),
            'columns_with_missing': missing_ratio[missing_ratio > 0].to_dict()
        }
        
        # çº¢ç¯ï¼šä»»ä½•åˆ—ç¼ºå¤±ç‡ > 20%
        red_flag_missing = missing_ratio.max() > 0.20
        report['checks']['missing_ratio']['red_flag'] = bool(red_flag_missing)
        
        print(f"   âœ“ ç¼ºå¤±ç‡: æœ€å¤§ {missing_ratio.max():.2%}, å¹³å‡ {missing_ratio.mean():.2%}")
        if red_flag_missing:
            print(f"     âš ï¸  çº¢ç¯: å­˜åœ¨åˆ—ç¼ºå¤±ç‡ > 20%")
        
        # 2. æå€¼æ£€æŸ¥ï¼ˆä»…æ•°å€¼åˆ—ï¼‰
        numeric_cols = data.select_dtypes(include=[np.number]).columns
        extreme_ratios = {}
        for col in numeric_cols:
            q1, q99 = data[col].quantile([0.01, 0.99])
            extreme_count = ((data[col] < q1) | (data[col] > q99)).sum()
            extreme_ratios[col] = float(extreme_count / len(data))
        
        report['checks']['extreme_values'] = {
            'max': max(extreme_ratios.values()) if extreme_ratios else 0,
            'mean': np.mean(list(extreme_ratios.values())) if extreme_ratios else 0
        }
        
        print(f"   âœ“ æå€¼æ¯”ä¾‹: æœ€å¤§ {report['checks']['extreme_values']['max']:.2%}")
        
        # 3. é‡å¤æ•°æ®æ£€æŸ¥
        if isinstance(data.index, pd.MultiIndex):
            duplicates = data.index.duplicated().sum()
        else:
            duplicates = data.index.duplicated().sum()
        
        report['checks']['duplicates'] = {
            'count': int(duplicates),
            'ratio': float(duplicates / len(data))
        }
        
        # çº¢ç¯ï¼šé‡å¤ç‡ > 1%
        red_flag_duplicates = (duplicates / len(data)) > 0.01
        report['checks']['duplicates']['red_flag'] = bool(red_flag_duplicates)
        
        print(f"   âœ“ é‡å¤æ•°æ®: {duplicates} ({duplicates/len(data):.2%})")
        if red_flag_duplicates:
            print(f"     âš ï¸  çº¢ç¯: é‡å¤ç‡ > 1%")
        
        # 4. å¯äº¤æ˜“æ ·æœ¬æ£€æŸ¥
        if 'tradable_flag' in data.columns:
            tradable_count = data['tradable_flag'].sum()
            tradable_ratio = tradable_count / len(data)
            report['checks']['tradable_samples'] = {
                'count': int(tradable_count),
                'ratio': float(tradable_ratio)
            }
            
            # çº¢ç¯ï¼šå¯äº¤æ˜“æ ·æœ¬ < 70%
            red_flag_tradable = tradable_ratio < 0.70
            report['checks']['tradable_samples']['red_flag'] = bool(red_flag_tradable)
            
            print(f"   âœ“ å¯äº¤æ˜“æ ·æœ¬: {tradable_count} ({tradable_ratio:.2%})")
            if red_flag_tradable:
                print(f"     âš ï¸  çº¢ç¯: å¯äº¤æ˜“æ ·æœ¬ < 70%")
        
        # 5. åœç‰Œ/æ¶¨è·Œåœæ£€æŸ¥
        if 'volume' in data.columns:
            suspended = (data['volume'] == 0).sum()
            suspended_ratio = suspended / len(data)
            report['checks']['suspended'] = {
                'count': int(suspended),
                'ratio': float(suspended_ratio)
            }
            print(f"   âœ“ åœç‰Œ: {suspended} ({suspended_ratio:.2%})")
        
        if 'pct_change' in data.columns or 'close' in data.columns:
            # è®¡ç®—æ¶¨è·Œå¹…
            if 'pct_change' in data.columns:
                pct_change = data['pct_change']
            else:
                pct_change = data['close'].pct_change()
            
            limit_up = (pct_change > 0.095).sum()
            limit_down = (pct_change < -0.095).sum()
            limit_ratio = (limit_up + limit_down) / len(data)
            
            report['checks']['limit_moves'] = {
                'limit_up': int(limit_up),
                'limit_down': int(limit_down),
                'total': int(limit_up + limit_down),
                'ratio': float(limit_ratio)
            }
            print(f"   âœ“ æ¶¨è·Œåœ: ä¸Š {limit_up}, ä¸‹ {limit_down} (æ€»è®¡ {limit_ratio:.2%})")
        
        # 6. æ—¶é—´è¿ç»­æ€§æ£€æŸ¥
        if isinstance(data.index, pd.MultiIndex):
            dates = data.index.get_level_values('date').unique()
        else:
            dates = pd.to_datetime(data.index).unique()
        
        dates = pd.Series(dates).sort_values()
        date_gaps = dates.diff().dt.days
        max_gap = date_gaps.max()
        
        report['checks']['time_continuity'] = {
            'n_dates': int(len(dates)),
            'max_gap_days': int(max_gap) if not pd.isna(max_gap) else 0,
            'date_range': f"{dates.min().date()} ~ {dates.max().date()}"
        }
        
        # çº¢ç¯ï¼šæœ€å¤§é—´éš” > 10å¤©ï¼ˆå¯èƒ½å­˜åœ¨æ•°æ®ç¼ºå¤±ï¼‰
        red_flag_gap = max_gap > 10 if not pd.isna(max_gap) else False
        report['checks']['time_continuity']['red_flag'] = bool(red_flag_gap)
        
        print(f"   âœ“ æ—¶é—´è¿ç»­æ€§: {len(dates)} ä¸ªäº¤æ˜“æ—¥, æœ€å¤§é—´éš” {max_gap} å¤©")
        if red_flag_gap:
            print(f"     âš ï¸  çº¢ç¯: æœ€å¤§é—´éš” > 10å¤©")
        
        # æ€»ä½“è¯„åˆ†
        red_flags = sum([
            report['checks'].get('missing_ratio', {}).get('red_flag', False),
            report['checks'].get('duplicates', {}).get('red_flag', False),
            report['checks'].get('tradable_samples', {}).get('red_flag', False),
            report['checks'].get('time_continuity', {}).get('red_flag', False)
        ])
        
        report['overall_quality'] = 'PASS' if red_flags == 0 else 'WARNING'
        report['red_flags_count'] = int(red_flags)
        
        print(f"\n   {'âœ…' if red_flags == 0 else 'âš ï¸ '} æ€»ä½“è¯„åˆ†: {report['overall_quality']} ({red_flags} ä¸ªçº¢ç¯)")
        
        return report
    
    def save_quality_report(self, report: Dict[str, Any]):
        """
        ä¿å­˜æ•°æ®è´¨é‡æŠ¥å‘Š
        
        Parameters:
        -----------
        report : dict
            è´¨é‡æŠ¥å‘Š
        """
        # æŒ‰æ—¥æœŸå‘½åæŠ¥å‘Šæ–‡ä»¶
        report_file = os.path.join(
            self.quality_reports_dir,
            f"{self.snapshot_id}.json"
        )
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"   ğŸ“Š è´¨é‡æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    def load_snapshot(self, snapshot_id: Optional[str] = None) -> Tuple[pd.DataFrame, Dict]:
        """
        åŠ è½½å¿«ç…§æ•°æ®
        
        Parameters:
        -----------
        snapshot_id : str, optional
            å¿«ç…§IDï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰å¿«ç…§ID
            
        Returns:
        --------
        Tuple[pd.DataFrame, dict]
            (æ•°æ®, å…ƒæ•°æ®)
        """
        if snapshot_id is None:
            snapshot_id = self.snapshot_id
        
        snapshot_path = os.path.join(self.snapshots_dir, snapshot_id)
        
        if not os.path.exists(snapshot_path):
            raise FileNotFoundError(f"å¿«ç…§ä¸å­˜åœ¨: {snapshot_path}")
        
        # åŠ è½½å…ƒæ•°æ®
        metadata_file = os.path.join(snapshot_path, 'metadata.json')
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # åŠ è½½æ•°æ®
        data_format = metadata.get('data_format', 'parquet')
        symbol = metadata.get('symbol', '000001')
        
        if data_format == 'parquet':
            data_file = os.path.join(snapshot_path, f'{symbol}_data.parquet')
            data = pd.read_parquet(data_file)
        else:
            data_file = os.path.join(snapshot_path, f'{symbol}_data.csv')
            data = pd.read_csv(data_file, index_col=[0, 1], parse_dates=[0])
        
        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        data_hash = self._calculate_data_hash(data)
        if data_hash != metadata.get('data_hash'):
            print(f"âš ï¸  è­¦å‘Š: æ•°æ®å“ˆå¸Œä¸åŒ¹é…ï¼Œæ•°æ®å¯èƒ½å·²è¢«ä¿®æ”¹")
        
        print(f"âœ… å¿«ç…§åŠ è½½æˆåŠŸ: {snapshot_id}")
        print(f"   æ ·æœ¬æ•°: {len(data)}")
        print(f"   ç‰¹å¾æ•°: {len(data.columns)}")
        
        return data, metadata
    
    def _calculate_data_hash(self, data: pd.DataFrame) -> str:
        """
        è®¡ç®—æ•°æ®å“ˆå¸Œå€¼
        
        Parameters:
        -----------
        data : pd.DataFrame
            æ•°æ®
            
        Returns:
        --------
        str
            MD5å“ˆå¸Œå€¼
        """
        # ä½¿ç”¨æ•°æ®çš„å½¢çŠ¶å’Œå‰10è¡Œä½œä¸ºå“ˆå¸Œè¾“å…¥
        hash_input = f"{data.shape}_{data.head(10).to_json()}"
        return hashlib.md5(hash_input.encode()).hexdigest()
    
    def list_snapshots(self) -> pd.DataFrame:
        """
        åˆ—å‡ºæ‰€æœ‰å¿«ç…§
        
        Returns:
        --------
        pd.DataFrame
            å¿«ç…§åˆ—è¡¨
        """
        snapshots = []
        
        for snapshot_id in os.listdir(self.snapshots_dir):
            snapshot_path = os.path.join(self.snapshots_dir, snapshot_id)
            metadata_file = os.path.join(snapshot_path, 'metadata.json')
            
            if os.path.exists(metadata_file):
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                snapshots.append({
                    'snapshot_id': snapshot_id,
                    'created_at': metadata.get('created_at'),
                    'symbol': metadata.get('symbol'),
                    'start_date': metadata.get('start_date'),
                    'end_date': metadata.get('end_date'),
                    'n_samples': metadata.get('n_samples'),
                    'quality': metadata.get('quality_checks', {}).get('overall_quality', 'UNKNOWN')
                })
        
        return pd.DataFrame(snapshots)


if __name__ == "__main__":
    """
    ä½¿ç”¨ç¤ºä¾‹
    """
    print("ğŸ“¸ æ•°æ®å¿«ç…§ç®¡ç†å™¨æµ‹è¯•")
    print("=" * 50)
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    dates = pd.date_range('2022-01-01', '2024-12-31', freq='D')
    tickers = ['000001'] * len(dates)
    
    data = pd.DataFrame({
        'close': np.random.randn(len(dates)) * 10 + 100,
        'volume': np.random.randint(1000000, 10000000, len(dates)),
        'feature_1': np.random.randn(len(dates)),
        'feature_2': np.random.randn(len(dates)),
        'tradable_flag': np.random.choice([0, 1], len(dates), p=[0.1, 0.9])
    }, index=pd.MultiIndex.from_arrays([dates, tickers], names=['date', 'ticker']))
    
    # åˆ›å»ºå¿«ç…§ç®¡ç†å™¨
    snapshot_mgr = DataSnapshot()
    
    # åˆ›å»ºå¿«ç…§
    filters = {
        'min_volume': 1000000,
        'min_price': 1.0,
        'exclude_st': True
    }
    
    snapshot_path = snapshot_mgr.create_snapshot(
        data=data,
        symbol='000001',
        start_date='2022-01-01',
        end_date='2024-12-31',
        filters=filters,
        random_seed=42
    )
    
    print(f"\nâœ… å¿«ç…§åˆ›å»ºæˆåŠŸ: {snapshot_path}")
    
    # åˆ—å‡ºæ‰€æœ‰å¿«ç…§
    print("\nğŸ“‹ ç°æœ‰å¿«ç…§:")
    snapshots_df = snapshot_mgr.list_snapshots()
    print(snapshots_df)
