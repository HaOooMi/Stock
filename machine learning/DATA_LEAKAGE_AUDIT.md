# æ•°æ®æ³„æ¼å½»åº•å®¡æŸ¥æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: 2025-10-14
**å®¡æŸ¥äºº**: Assistant (GPT-4)
**ç›®æ ‡**: å½»åº•æ£€æŸ¥"ä¿¡å·ä¸æœªæ¥æ”¶ç›Šç›¸å…³æ€§è¿‡é«˜(0.3369), å¯èƒ½å­˜åœ¨æ•°æ®æ³„éœ²"é—®é¢˜

---

## å¿…æŸ¥é¡¹A1: æ ‡ç­¾æ˜¯å¦ç»Ÿä¸€æ¥è‡ªåŒä¸€å¤„å‡½æ•°

### âœ… æ£€æŸ¥ç»“æœ: é€šè¿‡

**è¯æ®**: `target_engineering.py` Line 80-96
```python
def generate_future_returns(self, data: pd.DataFrame, 
                           periods: List[int] = [1, 5, 10],
                           price_col: str = 'close') -> pd.DataFrame:
    for period in periods:
        target_col = f'future_return_{period}d'
        
        # ã€å…³é”®ã€‘æ­£ç¡®ä½¿ç”¨shift(-period)å‘æœªæ¥ç§»åŠ¨
        future_prices = result_df[price_col].shift(-period)
        current_prices = result_df[price_col]
        
        # è®¡ç®—æ”¶ç›Šç‡
        result_df[target_col] = (future_prices - current_prices) / current_prices
```

**éªŒè¯**:
- âœ… æ‰€æœ‰æ ‡ç­¾éƒ½æ¥è‡ª`generate_future_returns()`å‡½æ•°
- âœ… ä½¿ç”¨`shift(-period)`æ­£ç¡®è®¡ç®—æœªæ¥æ”¶ç›Š
- âœ… å…¬å¼æ­£ç¡®: `(future_price - current_price) / current_price`

**å°¾éƒ¨NaNä¿ç•™éªŒè¯** (`target_engineering.py` Line 138-164):
```python
def _verify_future_returns(self, data: pd.DataFrame, periods: List[int]):
    for period in periods:
        target_col = f'future_return_{period}d'
        
        # å°¾éƒ¨åº”è¯¥æœ‰periodè¡ŒNaN
        tail_nans = data[target_col].tail(period).isna().sum()
        
        if tail_nans != period:
            print(f"      âš ï¸ è­¦å‘Š: å°¾éƒ¨NaNæ•°é‡ä¸åŒ¹é…é¢„æœŸ")
```

**ç»“è®º**: âœ… **æ ‡ç­¾ç”Ÿæˆæ­£ç¡®ï¼Œå°¾éƒ¨NaNæ­£ç¡®ä¿ç•™**

---

## å¿…æŸ¥é¡¹A2: è®­ç»ƒé›†å°¾éƒ¨æ˜¯å¦purgeï¼ˆåˆ å»max_hè¡Œï¼‰

### âš ï¸ æ£€æŸ¥ç»“æœ: **æœ‰é—®é¢˜ï¼purge_periods=10ï¼Œä½†ç¼ºå°‘éªŒè¯**

**è¯æ®**: `pca_state.py` Line 135-173

#### å½“å‰å®ç°
```python
def fit_pca_with_time_split(self, features_df: pd.DataFrame,
                           n_components: float = 0.9,
                           train_ratio: float = 0.8,
                           purge_periods: int = 10) -> Dict:  # ç¡¬ç¼–ç 10å¤©
    
    # æ—¶é—´åˆ‡åˆ†
    n_samples = len(features_df)
    split_idx = int(n_samples * train_ratio)
    
    # Purgeè®­ç»ƒé›†å°¾éƒ¨(é˜²æ­¢æ ‡ç­¾æ³„æ¼)
    split_idx_purged = split_idx - purge_periods  # åˆ å»10è¡Œ
    
    if split_idx_purged < 50:
        raise ValueError(f"è®­ç»ƒæ ·æœ¬è¿‡å°‘({split_idx_purged}),è¯·å‡å°purge_periodsæˆ–å¢åŠ train_ratio")
    
    train_index = features_df.index[:split_idx_purged]  # è®­ç»ƒé›†[0:70)
    test_index = features_df.index[split_idx:]          # æµ‹è¯•é›†[80:88)
    
    # ã€å…³é”®ã€‘ä¸­é—´gap [70:80) è¢«purgeæ‰
```

#### ğŸš¨ å‘ç°çš„é—®é¢˜

**é—®é¢˜1**: `purge_periods=10` **ç¡¬ç¼–ç ï¼Œæœªæ ¹æ®max_target_periodåŠ¨æ€è°ƒæ•´**

**è¯æ®**: `target_engineering.py` Line 12å®šä¹‰äº†`periods=[1, 5, 10]`
- æœ€å¤§ç›®æ ‡çª—å£ `max_h = 10å¤©`
- Purgeåº”è¯¥ â‰¥ 10å¤©ï¼Œä½†ä»£ç ä¸­æ˜¯ç¡¬ç¼–ç `purge_periods=10`

**é—®é¢˜2**: **ç¼ºå°‘æ˜¾å¼éªŒè¯purge gapæ˜¯å¦å……åˆ†**

**æ­£ç¡®åšæ³•**:
```python
# åº”è¯¥åŠ¨æ€è®¡ç®—purge_periods
max_target_period = max(target_periods)  # 10
purge_periods = max_target_period  # è‡³å°‘10å¤©

# åº”è¯¥éªŒè¯gapå†…çš„æ•°æ®ä¸åŒ…å«è®­ç»ƒç›®æ ‡
gap_data = features_df.iloc[split_idx_purged:split_idx]
assert len(gap_data) == purge_periods, "Purge gapé•¿åº¦é”™è¯¯"
```

**å½“å‰é£é™©**:
- âœ… Purgeé€»è¾‘å­˜åœ¨ä¸”æ­£ç¡®æ‰§è¡Œ
- âš ï¸ **ä½†purge_periodsæ˜¯ç¡¬ç¼–ç ï¼Œæœªä¸target_periodså…³è”**
- âš ï¸ **å¦‚æœæœªæ¥å¢åŠ æ›´å¤§çš„target_period (å¦‚20å¤©)ï¼Œpurgeä¼šå¤±æ•ˆ**

**ç»“è®º**: âš ï¸ **éƒ¨åˆ†é€šè¿‡ï¼Œä½†å­˜åœ¨ç¡¬ç¼–ç é£é™©**

---

## å¿…æŸ¥é¡¹A3: IC/Spreadçš„predä¸yæ˜¯å¦ä¸¥æ ¼å¯¹é½ï¼ˆT+1æ‰§è¡Œï¼‰

### âš ï¸ æ£€æŸ¥ç»“æœ: **æœ‰ä¸¥é‡é—®é¢˜ï¼**

#### é—®é¢˜1: `strategy_backtest.py` ä¸­T+1å¯¹é½**æ­£ç¡®å®ç°**

**è¯æ®**: `strategy_backtest.py` Line 408-420
```python
def calculate_strategy_performance(self, signal_data: pd.DataFrame) -> Dict:
    returns = signal_data['future_return_5d'].fillna(0).values
    signal = signal_data['signal_combined'].values
    
    # ã€å…³é”®ä¿®å¤ã€‘ä¿¡å·å¯¹é½: T+1æ‰§è¡Œ
    # ä»Šå¤©çš„ä¿¡å·å†³å®šæ˜å¤©çš„ä»“ä½,é¿å…look-ahead bias
    signal_t_plus_1 = np.roll(signal, 1)  # ä¿¡å·åç§»1å¤©
    signal_t_plus_1[0] = 0  # ç¬¬ä¸€å¤©æ— ä¿¡å·
    
    # ç­–ç•¥æ”¶ç›Šï¼šä½¿ç”¨å¯¹é½åçš„ä¿¡å·
    strategy_returns = signal_t_plus_1 * returns
```

âœ… **è¿™éƒ¨åˆ†æ­£ç¡®**: ä½¿ç”¨`np.roll(signal, 1)`ç¡®ä¿T+1æ‰§è¡Œ

#### ğŸš¨ é—®é¢˜2: `quick_triage.py` ä¸­**ICè®¡ç®—ç¼ºå°‘T+1å¯¹é½**

**æ£€æŸ¥**: `quick_triage.py`ï¼ˆå‡è®¾å­˜åœ¨ICè®¡ç®—ï¼‰

**é¢„æœŸä»£ç åº”è¯¥æ˜¯**:
```python
# é”™è¯¯åšæ³•ï¼ˆä¼šå¯¼è‡´0.3369é«˜ç›¸å…³æ€§ï¼‰
correlation = signal_data['signal'].corr(signal_data['future_return_5d'])

# æ­£ç¡®åšæ³•
signal_t_plus_1 = signal_data['signal'].shift(1)  # T+1å¯¹é½
correlation = signal_t_plus_1.corr(signal_data['future_return_5d'])
```

**éœ€è¦ç¡®è®¤**: 
- â“ `quick_triage.py` æ˜¯å¦å­˜åœ¨ï¼Ÿ
- â“ ICè®¡ç®—æ˜¯å¦ä½¿ç”¨äº†T+1å¯¹é½ï¼Ÿ

**å½“å‰è¯æ®ä¸è¶³**: éœ€è¦æ£€æŸ¥`quick_triage.py`çš„å®é™…ä»£ç 

**ç»“è®º**: âš ï¸ **ç­–ç•¥å›æµ‹å·²æ­£ç¡®å®ç°T+1ï¼Œä½†ICè®¡ç®—éœ€è¦éªŒè¯**

---

## å¿…æŸ¥é¡¹A4: scaler/PCA/KMeansæ˜¯å¦åªåœ¨è®­ç»ƒæ®µfit

### âœ… æ£€æŸ¥ç»“æœ: é€šè¿‡

#### ScaleréªŒè¯ (`feature_engineering.py` Line 668-710)

```python
def scale_features(self, features_df: pd.DataFrame, 
                   scaler_type: str = 'robust',
                   train_ratio: float = 0.8) -> Dict:
    
    # æ—¶é—´åˆ‡åˆ†
    n_samples = len(df)
    split_idx = int(n_samples * train_ratio)
    train_index = df.index[:split_idx]
    valid_index = df.index[split_idx:]
    
    train_X = df.loc[train_index, feature_cols]
    valid_X = df.loc[valid_index, feature_cols]
    
    # ã€å…³é”®ã€‘åªåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆ
    scaler.fit(train_X.fillna(0))
    scaled_train = scaler.transform(train_X.fillna(0))
    
    # æµ‹è¯•é›†åªtransform
    scaled_valid = scaler.transform(valid_X.fillna(0))
```

âœ… **æ­£ç¡®**: Scaleråªåœ¨è®­ç»ƒé›†fitï¼Œæµ‹è¯•é›†åªtransform

#### PCAéªŒè¯ (`pca_state.py` Line 135-235)

```python
def fit_pca_with_time_split(self, features_df: pd.DataFrame,
                           n_components: float = 0.9,
                           train_ratio: float = 0.8,
                           purge_periods: int = 10) -> Dict:
    
    # æ—¶é—´åˆ‡åˆ† + Purge
    split_idx = int(n_samples * train_ratio)
    split_idx_purged = split_idx - purge_periods
    
    X_train = features_df.iloc[:split_idx_purged].fillna(0)  # è®­ç»ƒé›†
    X_test = features_df.iloc[split_idx:].fillna(0)          # æµ‹è¯•é›†
    
    # ã€å…³é”®ã€‘åªåœ¨è®­ç»ƒé›†ä¸Šæ‹ŸåˆPCA
    pca_final = PCA(n_components=n_components_needed)
    pca_final.fit(X_train)
    
    # ç”Ÿæˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„PCAçŠ¶æ€
    states_train = pca_final.transform(X_train)
    states_test = pca_final.transform(X_test)   # æµ‹è¯•é›†åªtransform
```

âœ… **æ­£ç¡®**: PCAåªåœ¨è®­ç»ƒé›†fitï¼Œæµ‹è¯•é›†åªtransform

#### KMeanséªŒè¯ (`cluster_evaluate.py` Line 68-86)

```python
def perform_kmeans_clustering(self, states_train: np.ndarray, k: int) -> KMeans:
    kmeans = KMeans(
        n_clusters=k,
        random_state=self.random_state,
        n_init=20,
        max_iter=500
    )
    
    # ã€å…³é”®ã€‘åªåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆ
    kmeans.fit(states_train)
    
    return kmeans
```

```python
def evaluate_cluster_returns(self, states: np.ndarray, targets_df: pd.DataFrame,
                            kmeans: KMeans, phase: str = "train") -> pd.DataFrame:
    
    # ã€å…³é”®ã€‘æµ‹è¯•é›†åªpredictï¼Œä¸é‡æ–°fit
    cluster_labels = kmeans.predict(states)
```

âœ… **æ­£ç¡®**: KMeansåªåœ¨è®­ç»ƒé›†fitï¼Œæµ‹è¯•é›†åªpredict

**ç»“è®º**: âœ… **æ‰€æœ‰é¢„å¤„ç†å™¨éƒ½æ­£ç¡®éµå¾ªfit-transformåˆ†ç¦»åŸåˆ™**

---

## å¿…æŸ¥é¡¹A5: ç ´åæ€§å¯¹ç…§å®éªŒ

### âŒ æ£€æŸ¥ç»“æœ: **æœªå®ç°ï¼**

**è¦æ±‚**: æ•…æ„ä½¿ç”¨é”™è¯¯æ ‡ç­¾ï¼ˆå¦‚`close.pct_change(5)`æˆ–`future_return_-5`ï¼‰çœ‹æŒ‡æ ‡æ˜¯å¦"æ›´å¥½"

**å½“å‰çŠ¶æ€**: 
- âŒ æ²¡æœ‰æ‰¾åˆ°ç ´åæ€§å¯¹ç…§å®éªŒä»£ç 
- âŒ `quick_triage.py` ä¸­åº”è¯¥æœ‰`check_leakage_with_wrong_labels()`å‡½æ•°ï¼Œä½†**æ–‡ä»¶ä¸å­˜åœ¨æˆ–æœªè¿è¡Œ**

**åº”å®ç°çš„æµ‹è¯•**:
```python
def destructive_test():
    """ç ´åæ€§å¯¹ç…§: ä½¿ç”¨é”™è¯¯æ ‡ç­¾æ£€æµ‹æ³„æ¼"""
    
    # é”™è¯¯æ ‡ç­¾1: è¿‡å»5å¤©æ”¶ç›Šï¼ˆç»å¯¹é”™è¯¯ï¼‰
    wrong_label_past = data['close'].pct_change(5)
    
    # é”™è¯¯æ ‡ç­¾2: è´Ÿçš„æœªæ¥æ”¶ç›Šï¼ˆæ—¶é—´åè½¬ï¼‰
    wrong_label_negated = -data['close'].pct_change(5).shift(-5)
    
    # é”™è¯¯æ ‡ç­¾3: éšæœºæ‰“ä¹±çš„æœªæ¥æ”¶ç›Š
    wrong_label_shuffled = data['future_return_5d'].sample(frac=1).values
    
    # è®¡ç®—ç›¸å…³æ€§
    corr_correct = signal.corr(data['future_return_5d'])
    corr_past = signal.corr(wrong_label_past)
    corr_negated = signal.corr(wrong_label_negated)
    corr_shuffled = pd.Series(signal).corr(pd.Series(wrong_label_shuffled))
    
    # ã€éªŒæ”¶æ ‡å‡†ã€‘
    # 1. æ­£ç¡®æ ‡ç­¾ç›¸å…³æ€§åº”è¯¥æœ€é«˜
    assert corr_correct > corr_past, "ä¿¡å·å¯¹è¿‡å»æ”¶ç›Šç›¸å…³æ€§è¿‡é«˜ï¼Œå­˜åœ¨æ³„æ¼ï¼"
    assert corr_correct > corr_negated, "ä¿¡å·å¯¹è´Ÿæ”¶ç›Šç›¸å…³æ€§è¿‡é«˜ï¼Œå­˜åœ¨æ³„æ¼ï¼"
    assert corr_correct > corr_shuffled, "ä¿¡å·å¯¹éšæœºæ”¶ç›Šç›¸å…³æ€§è¿‡é«˜ï¼Œå­˜åœ¨æ³„æ¼ï¼"
    
    # 2. é”™è¯¯æ ‡ç­¾ç›¸å…³æ€§åº”è¯¥æ¥è¿‘0
    assert abs(corr_past) < 0.1, f"è¿‡å»æ”¶ç›Šç›¸å…³æ€§ {corr_past:.4f} è¿‡é«˜"
    assert abs(corr_shuffled) < 0.1, f"éšæœºæ”¶ç›Šç›¸å…³æ€§ {corr_shuffled:.4f} è¿‡é«˜"
```

**ç»“è®º**: âŒ **æœªå®ç°ç ´åæ€§å¯¹ç…§ï¼Œè¿™æ˜¯é‡å¤§é—æ¼ï¼**

---

## æ•°æ®æ³„æ¼æ ¹å› åˆ†æ

### ğŸ” ä¸ºä»€ä¹ˆ"ä¿¡å·ä¸æœªæ¥æ”¶ç›Šç›¸å…³æ€§0.3369"ï¼Ÿ

#### å‡è®¾1: ICè®¡ç®—æ—¶æœªT+1å¯¹é½ âš ï¸ **é«˜åº¦æ€€ç–‘**

**åœºæ™¯**: å¦‚æœ`quick_triage.py`ä¸­ç›´æ¥è®¡ç®—:
```python
# é”™è¯¯åšæ³•ï¼ˆå¯¼è‡´0.3369é«˜ç›¸å…³æ€§ï¼‰
signal_today = model.predict(features_today)
return_today = future_return_5d[today]  # è¿™ä¸ªå€¼åŒ…å«äº†æœªæ¥5å¤©çš„ä¿¡æ¯

correlation = signal_today.corr(return_today)  # 0.3369
```

**é—®é¢˜**: 
- `signal_today`ä½¿ç”¨äº†`features_today`ï¼ˆæˆªè‡³ä»Šå¤©çš„ç‰¹å¾ï¼‰
- ä½†`future_return_5d[today]`æ˜¯**æœªæ¥5å¤©çš„æ”¶ç›Š**ï¼Œåœ¨ä»Šå¤©ç”Ÿæˆä¿¡å·æ—¶**ä¸åº”è¯¥çŸ¥é“**
- æ­£ç¡®åšæ³•åº”è¯¥æ˜¯`signal_today.shift(1).corr(return_today)`

#### å‡è®¾2: ç‰¹å¾å·¥ç¨‹ä¸­åŒ…å«äº†æœªæ¥ä¿¡æ¯ âš ï¸ **éœ€è¦éªŒè¯**

**æ£€æŸ¥**: `feature_engineering.py`ä¸­æ˜¯å¦æœ‰ç‰¹å¾ä½¿ç”¨äº†`shift(-n)`ï¼ˆè´Ÿå€¼è¡¨ç¤ºæœªæ¥ï¼‰

**æ‰«æç»“æœ**:
```python
# Line 239: æ”¶ç›Šç‡ç‰¹å¾
data['return_1d'] = data['close'].pct_change()      # âœ… æ­£ç¡®
data['return_5d'] = data['close'].pct_change(5)     # âœ… æ­£ç¡®
data['return_10d'] = data['close'].pct_change(10)   # âœ… æ­£ç¡®

# Line 246: æ»šåŠ¨ç»Ÿè®¡
data[f'rolling_mean_{window}d'] = data['close'].rolling(window).mean()  # âœ… æ­£ç¡®

# Line 256: åŠ¨é‡ç‰¹å¾
data['momentum_3d'] = data['close'] / data['close'].shift(3) - 1  # âœ… æ­£ç¡®ï¼ˆä½¿ç”¨è¿‡å»ï¼‰
```

âœ… **ç‰¹å¾å·¥ç¨‹ä¸­æ²¡æœ‰ä½¿ç”¨æœªæ¥ä¿¡æ¯**

#### å‡è®¾3: ç°‡å æ¯”è¿‡å°å¯¼è‡´è¿‡æ‹Ÿåˆå™ªå£° âœ… **å·²ä¿®å¤**

**è¯æ®**: ä¹‹å‰å‘ç°"æœ€ä½³ç°‡å æ¯”ä»…1.6%"

**å·²ä¿®å¤**: `strategy_backtest.py` Line 131-189æ·»åŠ äº†`min_cluster_pct=0.05`è¿‡æ»¤

```python
def select_best_clusters(self, comparison_df: pd.DataFrame, top_n: int =3, 
                       min_cluster_pct: float = 0.05) -> Dict:
    
    # è¿‡æ»¤æ‰å æ¯”è¿‡å°çš„ç°‡
    valid_clusters = valid_clusters[valid_clusters['cluster_pct'] >= min_cluster_pct].copy()
```

âœ… **å·²ä¿®å¤å æ¯”è¿‡å°é—®é¢˜**

#### å‡è®¾4: Purge gapä¸è¶³å¯¼è‡´è®­ç»ƒé›†å°¾éƒ¨æ ‡ç­¾æ³„æ¼ âš ï¸ **ä½é£é™©ä½†éœ€ä¼˜åŒ–**

**å½“å‰**: `purge_periods=10`ï¼ˆç¡¬ç¼–ç ï¼‰
**æœ€å¤§ç›®æ ‡çª—å£**: `max(target_periods) = 10å¤©`

**é£é™©**: 
- å¦‚æœæœªæ¥å¢åŠ `target_periods=[1,5,10,20]`ï¼Œpurge=10ä¼šå¤±æ•ˆ
- åº”è¯¥åŠ¨æ€è®¾ç½®`purge_periods = max(target_periods)`

âš ï¸ **å»ºè®®ä¼˜åŒ–ä½†éç´§æ€¥**

---

## ç»¼åˆè¯Šæ–­ç»“è®º

### ğŸ”´ é«˜åº¦æ€€ç–‘çš„æ³„æ¼æº

#### âš ï¸ **æœ€å¯èƒ½åŸå› **: `quick_triage.py`ä¸­ICè®¡ç®—ç¼ºå°‘T+1å¯¹é½

**æ¨ç†é“¾**:
1. æ‚¨æŠ¥å‘Š"ä¿¡å·ä¸æœªæ¥æ”¶ç›Šç›¸å…³æ€§0.3369" â†’ è¿™æ˜¯ICè®¡ç®—çš„ç»“æœ
2. ICåº”è¯¥åœ¨`quick_triage.py`çš„`check_ranking_power()`ä¸­è®¡ç®—
3. è¯¥æ–‡ä»¶**ä¸åœ¨workspaceä¸­**æˆ–**æœªè¢«æ£€æŸ¥**
4. å¦‚æœICè®¡ç®—æ—¶ä½¿ç”¨:
   ```python
   correlation = signal_data['signal'].corr(signal_data['future_return_5d'])
   ```
   è€Œä¸æ˜¯:
   ```python
   signal_t_plus_1 = signal_data['signal'].shift(1)
   correlation = signal_t_plus_1.corr(signal_data['future_return_5d'])
   ```
   å°±ä¼šå¯¼è‡´0.3369çš„è™šé«˜ç›¸å…³æ€§ï¼

### ğŸŸ¡ æ¬¡è¦å¯èƒ½åŸå› 

1. **Purgeç¡¬ç¼–ç **: å­˜åœ¨ä½†é£é™©è¾ƒä½
2. **ç¼ºå°‘ç ´åæ€§å¯¹ç…§**: æ— æ³•éªŒè¯æ³„æ¼æ˜¯å¦å­˜åœ¨
3. **ç°‡å æ¯”è¿‡æ»¤**: å·²ä¿®å¤

---

## ç´§æ€¥ä¿®å¤å»ºè®®

### ğŸš¨ ç«‹å³æ‰§è¡Œï¼ˆPriority 1ï¼‰

#### ä¿®å¤1: æ£€æŸ¥å¹¶ä¿®æ­£`quick_triage.py`çš„ICè®¡ç®—

```python
# é”™è¯¯åšæ³•ï¼ˆéœ€è¦ä¿®æ­£ï¼‰
def check_ranking_power_WRONG(self, signal_data, target_col='future_return_5d'):
    correlation = signal_data['signal'].corr(signal_data[target_col])
    return correlation  # ä¼šå¾—åˆ°0.3369

# æ­£ç¡®åšæ³•
def check_ranking_power_CORRECT(self, signal_data, target_col='future_return_5d'):
    # T+1å¯¹é½
    signal_t_plus_1 = signal_data['signal'].shift(1).dropna()
    target_aligned = signal_data[target_col].iloc[1:]
    
    correlation = signal_t_plus_1.corr(target_aligned)
    return correlation  # åº”è¯¥æ˜¾è‘—é™ä½
```

#### ä¿®å¤2: å®ç°ç ´åæ€§å¯¹ç…§å®éªŒ

```python
def check_leakage_with_wrong_labels(self, signal_data, target_col='future_return_5d'):
    """ç ´åæ€§å¯¹ç…§: ä½¿ç”¨é”™è¯¯æ ‡ç­¾æ£€æµ‹æ³„æ¼"""
    
    # å‡†å¤‡ä¿¡å·ï¼ˆT+1å¯¹é½ï¼‰
    signal = signal_data['signal'].shift(1).dropna()
    
    # æ­£ç¡®æ ‡ç­¾
    correct_target = signal_data[target_col].iloc[1:]
    
    # é”™è¯¯æ ‡ç­¾1: è¿‡å»æ”¶ç›Š
    wrong_past = signal_data['close'].pct_change(5).iloc[1:]
    
    # é”™è¯¯æ ‡ç­¾2: éšæœºæ‰“ä¹±
    wrong_random = correct_target.sample(frac=1, random_state=42).values
    
    # è®¡ç®—ç›¸å…³æ€§
    corr_correct = signal.corr(correct_target)
    corr_past = signal.corr(wrong_past)
    corr_random = pd.Series(signal.values).corr(pd.Series(wrong_random))
    
    print(f"ç›¸å…³æ€§ - æ­£ç¡®æ ‡ç­¾: {corr_correct:.4f}")
    print(f"ç›¸å…³æ€§ - è¿‡å»æ”¶ç›Š: {corr_past:.4f}")
    print(f"ç›¸å…³æ€§ - éšæœºæ‰“ä¹±: {corr_random:.4f}")
    
    # éªŒæ”¶
    if corr_correct < 0.02:
        print("âŒ ICè¿‡ä½ï¼Œä¿¡å·æ— é¢„æµ‹åŠ›")
        return False
    
    if abs(corr_past) > 0.1 or abs(corr_random) > 0.1:
        print("ğŸš¨ è­¦å‘Šï¼šé”™è¯¯æ ‡ç­¾ç›¸å…³æ€§è¿‡é«˜ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®æ³„æ¼ï¼")
        return False
    
    if corr_correct <= max(abs(corr_past), abs(corr_random)):
        print("ğŸš¨ ä¸¥é‡è­¦å‘Šï¼šæ­£ç¡®æ ‡ç­¾ç›¸å…³æ€§ä¸é«˜äºé”™è¯¯æ ‡ç­¾ï¼Œç¡®è®¤å­˜åœ¨æ³„æ¼ï¼")
        return False
    
    print("âœ… é€šè¿‡ç ´åæ€§å¯¹ç…§éªŒè¯")
    return True
```

#### ä¿®å¤3: åŠ¨æ€Purgeå‚æ•°

```python
def fit_pca_with_time_split(self, features_df: pd.DataFrame,
                           n_components: float = 0.9,
                           train_ratio: float = 0.8,
                           target_periods: List[int] = [1, 5, 10]) -> Dict:  # æ–°å¢å‚æ•°
    
    # ã€ä¿®å¤ã€‘åŠ¨æ€è®¡ç®—purge_periods
    purge_periods = max(target_periods)  # ç¡®ä¿â‰¥æœ€å¤§ç›®æ ‡çª—å£
    
    print(f"   ğŸš« è®­ç»ƒé›†å°¾éƒ¨purge: {purge_periods}å¤© (åŸºäºmax_target_period={max(target_periods)})")
    
    # æ—¶é—´åˆ‡åˆ†
    split_idx = int(n_samples * train_ratio)
    split_idx_purged = split_idx - purge_periods
    
    # éªŒè¯gap
    if split_idx_purged < 50:
        raise ValueError(f"è®­ç»ƒæ ·æœ¬è¿‡å°‘({split_idx_purged}),è¯·å‡å°target_periodsæˆ–å¢åŠ train_ratio")
```

### ğŸŸ¡ æ¨èæ‰§è¡Œï¼ˆPriority 2ï¼‰

1. **æ·»åŠ æ˜¾å¼çš„è®­ç»ƒ-æµ‹è¯•æ—¶é—´gapéªŒè¯**
```python
def validate_time_gap(train_index, test_index, min_gap_days=10):
    train_end = train_index.max()
    test_start = test_index.min()
    gap_days = (test_start - train_end).days
    
    assert gap_days >= min_gap_days, f"æ—¶é—´gapä¸è¶³: {gap_days} < {min_gap_days}å¤©"
    print(f"âœ… æ—¶é—´gapéªŒè¯é€šè¿‡: {gap_days}å¤©")
```

2. **æ·»åŠ ICåˆ†å¸ƒç›‘æ§**
```python
def monitor_ic_distribution(ic_values, threshold=0.15):
    """ç›‘æ§ICå¼‚å¸¸å€¼"""
    mean_ic = np.mean(ic_values)
    std_ic = np.std(ic_values)
    
    if mean_ic > threshold:
        print(f"âš ï¸ è­¦å‘Š: å¹³å‡ICè¿‡é«˜ {mean_ic:.4f} > {threshold}")
        print(f"   å¯èƒ½å­˜åœ¨æ•°æ®æ³„æ¼ï¼Œå»ºè®®æ£€æŸ¥ä¿¡å·å¯¹é½")
```

---

## æœ€ç»ˆéªŒæ”¶æ¸…å•

### âœ… å·²é€šè¿‡
- [x] A1: æ ‡ç­¾æ¥è‡ªç»Ÿä¸€å‡½æ•° âœ…
- [x] A1: å°¾éƒ¨NaNæ­£ç¡®ä¿ç•™ âœ…
- [x] A3: ç­–ç•¥å›æµ‹T+1å¯¹é½ âœ…
- [x] A4: Scaleråªåœ¨è®­ç»ƒé›†fit âœ…
- [x] A4: PCAåªåœ¨è®­ç»ƒé›†fit âœ…
- [x] A4: KMeansåªåœ¨è®­ç»ƒé›†fit âœ…
- [x] B1: ç°‡å æ¯”è¿‡æ»¤ âœ…

### âš ï¸ éœ€è¦ä¿®å¤
- [ ] A2: Purgeå‚æ•°ç¡¬ç¼–ç  â†’ **å»ºè®®åŠ¨æ€åŒ–**
- [ ] A3: ICè®¡ç®—T+1å¯¹é½ â†’ **ç´§æ€¥æ£€æŸ¥`quick_triage.py`**
- [ ] A5: ç ´åæ€§å¯¹ç…§å®éªŒ â†’ **ç´§æ€¥å®ç°**
- [ ] æ—¶é—´gapæ˜¾å¼éªŒè¯ â†’ **å»ºè®®æ·»åŠ **

### âŒ æœªå®ç°
- [ ] ç ´åæ€§å¯¹ç…§å®éªŒä»£ç 
- [ ] ICåˆ†å¸ƒç›‘æ§ä»£ç 

---

## è¡ŒåŠ¨è®¡åˆ’

### ç¬¬ä¸€æ­¥ï¼ˆç´§æ€¥ï¼‰: å®šä½ICè®¡ç®—ä»£ç 
```bash
# æŸ¥æ‰¾quick_triage.pyæˆ–ICè®¡ç®—ç›¸å…³ä»£ç 
grep -r "future_return" machine\ learning/*.py
grep -r "\.corr(" machine\ learning/*.py
grep -r "IC\|information_coefficient" machine\ learning/*.py
```

### ç¬¬äºŒæ­¥: ä¿®æ­£ICè®¡ç®—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
å‚è€ƒä¸Šé¢çš„`check_ranking_power_CORRECT()`å®ç°

### ç¬¬ä¸‰æ­¥: å®ç°ç ´åæ€§å¯¹ç…§
åˆ›å»ºæ–°å‡½æ•°æˆ–åœ¨`quick_triage.py`ä¸­æ·»åŠ 

### ç¬¬å››æ­¥: åŠ¨æ€Purgeå‚æ•°
ä¿®æ”¹`pca_state.py`çš„`fit_pca_with_time_split()`å‡½æ•°

### ç¬¬äº”æ­¥: é‡æ–°è¿è¡Œæµ‹è¯•
```bash
python machine\ learning/quick_triage.py
python machine\ learning/strategy_backtest.py
```

### ç¬¬å…­æ­¥: éªŒæ”¶æ–°ICå€¼
- æœŸæœ›ICåº”è¯¥åœ¨ **0.02 ~ 0.08** ä¹‹é—´ï¼ˆåˆç†èŒƒå›´ï¼‰
- å¦‚æœä»ç„¶>0.15ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥ç‰¹å¾å·¥ç¨‹

---

## æŠ¥å‘Šæ€»ç»“

### ğŸ¯ æ ¸å¿ƒå‘ç°
1. **æœ€å¯èƒ½çš„æ³„æ¼æº**: ICè®¡ç®—ç¼ºå°‘T+1å¯¹é½ï¼ˆéœ€éªŒè¯ï¼‰
2. **æ¬¡è¦é—®é¢˜**: Purgeå‚æ•°ç¡¬ç¼–ç ã€ç¼ºå°‘ç ´åæ€§å¯¹ç…§
3. **å·²ä¿®å¤**: ç°‡å æ¯”è¿‡æ»¤ã€ç­–ç•¥å›æµ‹T+1å¯¹é½

### ğŸ“‹ æ‰§è¡Œä¼˜å…ˆçº§
1. **P0-ç´§æ€¥**: æ£€æŸ¥ICè®¡ç®—æ˜¯å¦T+1å¯¹é½
2. **P1-é‡è¦**: å®ç°ç ´åæ€§å¯¹ç…§å®éªŒ
3. **P2-æ¨è**: åŠ¨æ€Purgeå‚æ•°ã€æ—¶é—´gapéªŒè¯

### âœ… éªŒæ”¶æ ‡å‡†
ä¿®å¤ååº”æ»¡è¶³:
- ICç›¸å…³æ€§: **0.02 ~ 0.08**ï¼ˆä¸åº”>0.15ï¼‰
- ç ´åæ€§å¯¹ç…§: æ­£ç¡®æ ‡ç­¾ > é”™è¯¯æ ‡ç­¾
- æ ·æœ¬å¤–IC: **> 0** ä¸”ç¨³å®š
- è®­ç»ƒ-æµ‹è¯•gap: **â‰¥ max_target_period**

